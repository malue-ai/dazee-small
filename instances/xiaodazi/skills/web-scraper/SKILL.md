---
name: web-scraper
description: High-performance web scraping powered by Crawl4AI (59k+ GitHub Stars). Playwright-based anti-detection, async concurrent crawling, LLM-optimized Markdown output. Completely free & open-source.
quickstart: |
  # å®‰è£…: pip install crawl4ai && crawl4ai-setup
  # å•ä¸ªURL:
  from crawl4ai import AsyncWebCrawler
  async with AsyncWebCrawler() as crawler:
      result = await crawler.arun("https://example.com")
      print(result.markdown.raw_markdown)
  # æ‰¹é‡å¹¶å‘:
  from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
  config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
  async with AsyncWebCrawler() as crawler:
      results = await crawler.arun_many(urls, config=config)
      for r in results:
          if r.success: print(r.url, len(r.markdown.raw_markdown))
metadata:
  xiaodazi:
    dependency_level: lightweight
    os: [common]
    backend_type: local
    user_facing: true
---

# Web Scraper (Crawl4AI)

åŸºäº **Crawl4AI** çš„é«˜æ€§èƒ½ç½‘é¡µçˆ¬è™«ã€‚Crawl4AI æ˜¯ 2025 å¹´ GitHub #1 Trending é¡¹ç›®ï¼ˆ59k+ Starsï¼‰ï¼Œä¸“ä¸º AI Agent å’Œ LLM è®¾è®¡ã€‚

## æ ¸å¿ƒä¼˜åŠ¿

| ç‰¹æ€§ | è¯´æ˜ |
|---|---|
| ğŸ† **ä¸šç•Œæ ‡æ†** | 59k+ GitHub Starsï¼ŒApache-2.0 å¼€æº |
| ğŸ’° **å®Œå…¨å…è´¹** | æ— éœ€ä»»ä½• API Key |
| ğŸ›¡ï¸ **å¼ºåçˆ¬** | Playwright æµè§ˆå™¨å¼•æ“ + stealth æ¨¡å¼ + éšæœº UA |
| ğŸš€ **æé€Ÿ** | å¼‚æ­¥å¹¶å‘ + å†…å­˜è‡ªé€‚åº”è°ƒåº¦ï¼Œæ¯”æ›¿ä»£æ–¹æ¡ˆå¿« 6x |
| ğŸ¯ **LLM ä¼˜åŒ–** | è‡ªåŠ¨è¾“å‡ºå¹²å‡€ Markdownï¼Œé€‚åˆ RAG å’Œ LLM æ¶ˆè´¹ |
| ğŸ“„ **åŠ¨æ€é¡µé¢** | æ”¯æŒ JavaScript æ¸²æŸ“ã€æ— é™æ»šåŠ¨ã€SPA |
| ğŸ”§ **æ™ºèƒ½æå–** | PruningContentFilter è‡ªåŠ¨å»é™¤å™ªå£°å†…å®¹ |

## ä½¿ç”¨åœºæ™¯

- ç”¨æˆ·è¯´ã€Œå¸®æˆ‘è¯»å–è¿™ç¯‡æ–‡ç« ã€ã€ŒæŠ“å–è¿™ä¸ªç½‘é¡µã€
- ç”¨æˆ·è¯´ã€Œæ”¶é›†æœ€è¿‘çš„æ–°é—»ã€ã€Œæ•´ç†è¡Œä¸šèµ„è®¯ã€
- æ‰¹é‡æŠ“å–æœç´¢ç»“æœä¸­çš„ç½‘é¡µå®Œæ•´å†…å®¹
- ä½œä¸º `deep-research` çš„å†…å®¹è·å–å±‚

## æŠ€æœ¯æ ˆ

```
Crawl4AI v0.8.x
â”œâ”€â”€ Playwright (Chromium æµè§ˆå™¨å¼•æ“)
â”œâ”€â”€ AsyncWebCrawler (å¼‚æ­¥å¹¶å‘)
â”œâ”€â”€ PruningContentFilter (æ™ºèƒ½å†…å®¹è¿‡æ»¤)
â”œâ”€â”€ DefaultMarkdownGenerator (Markdown ç”Ÿæˆ)
â””â”€â”€ MemoryAdaptiveDispatcher (å†…å­˜è‡ªé€‚åº”è°ƒåº¦)
```

## å¿«é€Ÿå¼€å§‹

### å•ä¸ª URL

```python
from crawl4ai import AsyncWebCrawler

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun("https://example.com")
    print(result.markdown)  # å¹²å‡€çš„ Markdown
```

### æ‰¹é‡å¹¶å‘

```python
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

urls = [
    "https://36kr.com/article1",
    "https://techcrunch.com/article2",
    "https://theverge.com/article3",
]

config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

async with AsyncWebCrawler() as crawler:
    results = await crawler.arun_many(urls, config=config)
    for result in results:
        if result.success:
            print(f"âœ… {result.url}: {len(result.markdown.raw_markdown)} chars")
```

### æ™ºèƒ½å†…å®¹è¿‡æ»¤

```python
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    markdown_generator=DefaultMarkdownGenerator(
        content_filter=PruningContentFilter(threshold=0.4, threshold_type="fixed")
    ),
)

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun("https://news.ycombinator.com", config=config)
    print(result.markdown.fit_markdown)  # è¿‡æ»¤åçš„é«˜è´¨é‡å†…å®¹
```

## åçˆ¬èƒ½åŠ›

Crawl4AI å†…ç½®å¤šå±‚åçˆ¬æœºåˆ¶:

| å±‚çº§ | æœºåˆ¶ | è¯´æ˜ |
|---|---|---|
| æµè§ˆå™¨ | Playwright Chromium | çœŸå®æµè§ˆå™¨æ¸²æŸ“ï¼Œä¸æ˜¯ HTTP è¯·æ±‚ |
| User-Agent | `user_agent_mode="random"` | æ¯æ¬¡éšæœº UA |
| Stealth | å†…ç½® stealth æ¨¡å¼ | éšè—è‡ªåŠ¨åŒ–ç‰¹å¾ |
| ä»£ç† | `proxy` å‚æ•° | æ”¯æŒä»£ç†æ±  |
| JavaScript | å®Œæ•´ JS æ¸²æŸ“ | æ”¯æŒåŠ¨æ€åŠ è½½é¡µé¢ |
| Cookies | Session ç®¡ç† | æ”¯æŒç™»å½•æ€ä¿æŒ |

### é…ç½®åçˆ¬

```python
from crawl4ai import BrowserConfig

browser_config = BrowserConfig(
    headless=True,
    user_agent_mode="random",      # éšæœº UA
    ignore_https_errors=True,
)
```

## å¹¶å‘æ€§èƒ½

Crawl4AI ä½¿ç”¨ `MemoryAdaptiveDispatcher` æ ¹æ®ç³»ç»Ÿèµ„æºè‡ªåŠ¨è°ƒæ•´å¹¶å‘:

```python
# æ‰¹é‡æŠ“å– - è‡ªåŠ¨è°ƒæ•´å¹¶å‘åº¦
results = await crawler.arun_many(urls, config=config)

# æµå¼å¤„ç† - è¾¹çˆ¬è¾¹å¤„ç†
config = CrawlerRunConfig(stream=True, cache_mode=CacheMode.BYPASS)
async for result in await crawler.arun_many(urls, config=config):
    if result.success:
        process(result)
```

## è¾“å‡ºæ ¼å¼

### Markdown (é»˜è®¤)

```markdown
# Article Title

Published on 2024-01-15 by John Doe

Main content paragraph 1...

## Section Heading

Content with **bold** and [links](https://example.com).

| Column 1 | Column 2 |
|---|---|
| Data | Data |
```

### è¾“å‡ºå­—æ®µ

```python
result.markdown.raw_markdown    # åŸå§‹ Markdown
result.markdown.fit_markdown    # è¿‡æ»¤åçš„ Markdown (éœ€é…ç½® content_filter)
result.extracted_content        # ç»“æ„åŒ–æå–ç»“æœ (éœ€é…ç½® extraction_strategy)
result.success                  # æ˜¯å¦æˆåŠŸ
result.url                      # åŸå§‹ URL
result.error_message            # é”™è¯¯ä¿¡æ¯
```

## å…¸å‹åœºæ™¯

### åœºæ™¯ 1: æ”¶é›†ä¸€å‘¨æ–°é—»

```python
# Step 1: é€šè¿‡ api_calling è°ƒç”¨ Jina Search è·å– URLï¼ˆå‚è€ƒ ddg-search skillï¼‰
results = await api_calling(url="https://s.jina.ai/AIè¡Œä¸šæ–°é—» æœ€è¿‘ä¸€å‘¨", method="GET", headers={"Accept": "application/json"})
urls = [r["url"] for r in results]

# Step 2: Crawl4AI å¹¶å‘æŠ“å– (10ä¸ªURLçº¦5-10ç§’)
config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    markdown_generator=DefaultMarkdownGenerator(
        content_filter=PruningContentFilter(threshold=0.4)
    ),
)
async with AsyncWebCrawler() as crawler:
    results = await crawler.arun_many(urls, config=config)

# Step 3: æå–æˆåŠŸçš„æ–‡ç« 
articles = [r for r in results if r.success]
for article in articles:
    content = article.markdown.fit_markdown  # è¿‡æ»¤åçš„å¹²å‡€å†…å®¹
```

### åœºæ™¯ 2: å¤„ç†åŠ¨æ€é¡µé¢ (SPA)

```python
config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    wait_until="networkidle",         # ç­‰å¾…ç½‘ç»œç©ºé—²
    delay_before_return_html=1.0,     # é¢å¤–ç­‰å¾… JS æ¸²æŸ“
    scan_full_page=True,              # æ»šåŠ¨åˆ°åº•éƒ¨
)
```

## CLI ç”¨æ³•

Crawl4AI è¿˜æä¾› CLI å·¥å…· `crwl`:

```bash
# åŸºæœ¬çˆ¬å–
crwl https://example.com -o markdown

# å¸¦å†…å®¹è¿‡æ»¤
crwl https://example.com -o markdown-fit

# JSON è¾“å‡º + è¯¦ç»†æ—¥å¿—
crwl https://example.com -o json -v --bypass-cache
```

## å®‰è£…

```bash
pip install crawl4ai
crawl4ai-setup  # å®‰è£… Playwright æµè§ˆå™¨ä¾èµ–
```

é…ç½®ä¸º `auto_install: true`ï¼Œé¦–æ¬¡ä½¿ç”¨è‡ªåŠ¨å®‰è£… Python åŒ…ã€‚`crawl4ai-setup` éœ€æ‰‹åŠ¨è¿è¡Œä¸€æ¬¡å®‰è£…æµè§ˆå™¨ã€‚

## å¯¹æ¯”å…¶ä»–æ–¹æ¡ˆ

| æ–¹æ¡ˆ | Stars | åçˆ¬ | åŠ¨æ€é¡µé¢ | é€Ÿåº¦ | æˆæœ¬ |
|---|---|---|---|---|---|
| **Crawl4AI** | **59k** | **Playwright+Stealth** | **âœ…** | **6x æœ€å¿«** | **å…è´¹** |
| Firecrawl | 77k | äº‘ç«¯æµè§ˆå™¨ | âœ… | å¿« | ä»˜è´¹ ($83/æœˆ) |
| httpx+BS4 | - | âŒ æ—  | âŒ | ä¸­ç­‰ | å…è´¹ |
| Jina Reader | - | äº‘ç«¯ | âœ… | å¿« | å…è´¹é¢åº¦ |
| Scrapy | 54k | éœ€æ’ä»¶ | âŒ | å¿« | å…è´¹ |

## æœ€ä½³å®è·µ

### âœ… æ¨è

- æ–°é—»ç½‘ç«™ã€åšå®¢ã€æŠ€æœ¯æ–‡ç« ã€æ–‡æ¡£
- åŠ¨æ€åŠ è½½é¡µé¢ (React/Vue/SPA)
- åçˆ¬ä¸¥æ ¼çš„ç½‘ç«™
- æ‰¹é‡æŠ“å–æœç´¢ç»“æœ
- éœ€è¦å®Œæ•´ JavaScript æ¸²æŸ“çš„é¡µé¢

### é…ç½®å»ºè®®

```python
# æ–°é—»/åšå®¢ (è½»é‡)
config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

# é‡åº¦åçˆ¬ç½‘ç«™
browser_config = BrowserConfig(
    headless=True,
    user_agent_mode="random",
)

# SPA / åŠ¨æ€é¡µé¢
config = CrawlerRunConfig(
    wait_until="networkidle",
    scan_full_page=True,
    delay_before_return_html=1.0,
)
```

## å‚è€ƒèµ„æº

- GitHub: https://github.com/unclecode/crawl4ai (59k+ Stars)
- æ–‡æ¡£: https://docs.crawl4ai.com/
- è®¸å¯è¯: Apache-2.0 (å®Œå…¨å…è´¹å•†ç”¨)
