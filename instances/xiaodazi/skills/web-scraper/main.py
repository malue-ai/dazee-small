"""
Web Scraper Skill - Powered by Crawl4AI

åŸºäº Crawl4AI (59k+ GitHub Stars) çš„é«˜æ€§èƒ½ç½‘é¡µçˆ¬è™«ã€‚
Playwright æµè§ˆå™¨å¼•æ“ + stealth åçˆ¬ + å¼‚æ­¥å¹¶å‘ + LLM ä¼˜åŒ– Markdownã€‚
å®Œå…¨å…è´¹å¼€æº (Apache-2.0)ã€‚
"""

import asyncio
from typing import Any, Dict, List, Optional

from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CacheMode,
    CrawlerRunConfig,
)
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

from logger import get_logger

logger = get_logger("web-scraper")

# ============================================================
# é»˜è®¤é…ç½®
# ============================================================

# æµè§ˆå™¨é…ç½® (åçˆ¬)
DEFAULT_BROWSER_CONFIG = BrowserConfig(
    headless=True,
    user_agent_mode="random",       # éšæœº User-Agent
    ignore_https_errors=True,
    verbose=False,
)

# çˆ¬å–é…ç½® (å†…å®¹æå–)
DEFAULT_CRAWLER_CONFIG = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,    # ä¸ç¼“å­˜ï¼Œè·å–æœ€æ–°å†…å®¹
    page_timeout=30000,             # é¡µé¢è¶…æ—¶ 30 ç§’
    markdown_generator=DefaultMarkdownGenerator(
        content_filter=PruningContentFilter(
            threshold=0.4,
            threshold_type="fixed",
        )
    ),
)


class WebScraperSkill:
    """
    åŸºäº Crawl4AI çš„é«˜æ€§èƒ½ç½‘é¡µçˆ¬è™«

    ç‰¹æ€§:
    - Playwright æµè§ˆå™¨å¼•æ“ (çœŸå®æ¸²æŸ“, æ”¯æŒ JS)
    - stealth æ¨¡å¼ + éšæœº UA (åçˆ¬)
    - å¼‚æ­¥å¹¶å‘ + å†…å­˜è‡ªé€‚åº”è°ƒåº¦ (é«˜æ€§èƒ½)
    - PruningContentFilter æ™ºèƒ½è¿‡æ»¤ (é«˜è´¨é‡)
    - è‡ªåŠ¨ Markdown è¾“å‡º (LLM å‹å¥½)
    """

    def __init__(
        self,
        browser_config: Optional[BrowserConfig] = None,
        crawler_config: Optional[CrawlerRunConfig] = None,
    ):
        self.browser_config = browser_config or DEFAULT_BROWSER_CONFIG
        self.crawler_config = crawler_config or DEFAULT_CRAWLER_CONFIG

    async def fetch_url(
        self,
        url: str,
        use_fit_markdown: bool = True,
    ) -> Dict[str, Any]:
        """
        æŠ“å–å•ä¸ª URL

        Args:
            url: ç›®æ ‡ URL
            use_fit_markdown: æ˜¯å¦ä½¿ç”¨è¿‡æ»¤åçš„ Markdown (æ¨è True)

        Returns:
            {
                "success": bool,
                "url": str,
                "content": str,       # Markdown å†…å®¹
                "raw_content": str,   # åŸå§‹ Markdown (æœªè¿‡æ»¤)
                "text_length": int,
                "error": str          # ä»…å¤±è´¥æ—¶
            }
        """
        try:
            async with AsyncWebCrawler(config=self.browser_config) as crawler:
                result = await crawler.arun(url=url, config=self.crawler_config)

                if not result.success:
                    logger.warning(f"âš ï¸ æŠ“å–å¤±è´¥: {url} - {result.error_message}")
                    return {
                        "success": False,
                        "url": url,
                        "error": result.error_message or "æŠ“å–å¤±è´¥",
                    }

                # æå– Markdown
                raw_md = result.markdown.raw_markdown if result.markdown else ""
                fit_md = result.markdown.fit_markdown if result.markdown else ""
                content = fit_md if (use_fit_markdown and fit_md) else raw_md

                if not content or len(content) < 50:
                    return {
                        "success": False,
                        "url": url,
                        "error": "æ— æ³•æå–æœ‰æ•ˆå†…å®¹(å†…å®¹è¿‡çŸ­)",
                    }

                logger.info(f"âœ… æˆåŠŸ: {url} ({len(content)} å­—ç¬¦)")
                return {
                    "success": True,
                    "url": url,
                    "content": content,
                    "raw_content": raw_md,
                    "text_length": len(content),
                }

        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸: {url} - {e}", exc_info=True)
            return {"success": False, "url": url, "error": str(e)}

    async def fetch_batch(
        self,
        urls: List[str],
        use_fit_markdown: bool = True,
        stream: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        å¹¶å‘æŠ“å–å¤šä¸ª URL (ä½¿ç”¨ Crawl4AI å†…ç½®çš„ MemoryAdaptiveDispatcher)

        Args:
            urls: URL åˆ—è¡¨
            use_fit_markdown: æ˜¯å¦ä½¿ç”¨è¿‡æ»¤åçš„ Markdown
            stream: æ˜¯å¦ä½¿ç”¨æµå¼æ¨¡å¼ (è¾¹çˆ¬è¾¹å¤„ç†)

        Returns:
            ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ“¡ æ‰¹é‡æŠ“å–: {len(urls)} ä¸ª URL")

        output: List[Dict[str, Any]] = []

        try:
            async with AsyncWebCrawler(config=self.browser_config) as crawler:
                if stream:
                    # æµå¼æ¨¡å¼: è¾¹çˆ¬è¾¹æ”¶é›†
                    stream_config = self.crawler_config.clone(stream=True)
                    async for result in await crawler.arun_many(
                        urls, config=stream_config
                    ):
                        output.append(self._parse_result(result, use_fit_markdown))
                else:
                    # æ‰¹é‡æ¨¡å¼: ç­‰å¾…å…¨éƒ¨å®Œæˆ
                    results = await crawler.arun_many(
                        urls, config=self.crawler_config
                    )
                    for result in results:
                        output.append(self._parse_result(result, use_fit_markdown))

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æŠ“å–å¼‚å¸¸: {e}", exc_info=True)
            # å¯¹äºæœªå¤„ç†çš„ URLï¼Œæ ‡è®°ä¸ºå¤±è´¥
            processed_urls = {r["url"] for r in output}
            for url in urls:
                if url not in processed_urls:
                    output.append({"success": False, "url": url, "error": str(e)})

        success_count = sum(1 for r in output if r.get("success"))
        logger.info(f"âœ… å®Œæˆ: {success_count}/{len(urls)} æˆåŠŸ")
        return output

    def _parse_result(self, result, use_fit_markdown: bool) -> Dict[str, Any]:
        """è§£æå•ä¸ª CrawlResult"""
        if not result.success:
            return {
                "success": False,
                "url": result.url,
                "error": result.error_message or "æŠ“å–å¤±è´¥",
            }

        raw_md = result.markdown.raw_markdown if result.markdown else ""
        fit_md = result.markdown.fit_markdown if result.markdown else ""
        content = fit_md if (use_fit_markdown and fit_md) else raw_md

        if not content or len(content) < 50:
            return {
                "success": False,
                "url": result.url,
                "error": "å†…å®¹è¿‡çŸ­æˆ–æ— æ³•æå–",
            }

        return {
            "success": True,
            "url": result.url,
            "content": content,
            "raw_content": raw_md,
            "text_length": len(content),
        }


# ============================================================
# Skill æ‰§è¡Œå…¥å£
# ============================================================

async def execute(params: Dict[str, Any], context: Any) -> str:
    """
    Skill æ‰§è¡Œå…¥å£

    å‚æ•°:
        url: å•ä¸ª URL
        urls: URL åˆ—è¡¨ (æ‰¹é‡æ¨¡å¼)
        use_fit_markdown: æ˜¯å¦ä½¿ç”¨è¿‡æ»¤åçš„ Markdown (é»˜è®¤ true)

    è¿”å›:
        Markdown æ ¼å¼çš„æŠ“å–ç»“æœ
    """
    skill = WebScraperSkill()

    url = params.get("url")
    urls = params.get("urls")
    use_fit = params.get("use_fit_markdown", True)

    # å•ä¸ª URL
    if url and not urls:
        result = await skill.fetch_url(url, use_fit_markdown=use_fit)

        if not result.get("success"):
            return f"âŒ æŠ“å–å¤±è´¥: {result.get('error')}\n\nURL: {url}"

        return (
            f"**æ¥æº**: {url}\n"
            f"**é•¿åº¦**: {result['text_length']} å­—ç¬¦\n\n"
            f"---\n\n"
            f"{result['content']}"
        )

    # æ‰¹é‡ URL
    elif urls and isinstance(urls, list):
        results = await skill.fetch_batch(urls, use_fit_markdown=use_fit)

        success = [r for r in results if r.get("success")]
        failed = [r for r in results if not r.get("success")]

        report = f"# æ‰¹é‡æŠ“å–å®Œæˆ\n\n"
        report += f"- **æˆåŠŸ**: {len(success)}/{len(urls)}\n"

        if failed:
            report += f"- **å¤±è´¥**: {len(failed)}\n\n"
            report += "## âŒ å¤±è´¥\n\n"
            for r in failed[:5]:
                report += f"- {r['url']}: {r.get('error')}\n"
            report += "\n"

        report += "## âœ… æŠ“å–å†…å®¹\n\n"
        for i, r in enumerate(success, 1):
            excerpt = r["content"][:300].replace("\n", " ")
            report += (
                f"### {i}. {r['url']}\n\n"
                f"*({r['text_length']} å­—ç¬¦)*\n\n"
                f"{excerpt}...\n\n---\n\n"
            )

        return report

    else:
        return "âŒ å‚æ•°é”™è¯¯: å¿…é¡»æä¾› url æˆ– urls å‚æ•°"
