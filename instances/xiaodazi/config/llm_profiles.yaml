# ============================================================
# 框架内部 LLM 配置
# ============================================================
#
# 通过 config.yaml 的 agent.provider 一键切换所有模型。
# 普通用户通常不需要修改此文件。
#
# 模型分配：
#
#   ┌──────────┬───────────────────────────┬──────────────────────────┐
#   │ 角色     │ qwen                      │ claude                   │
#   ├──────────┼───────────────────────────┼──────────────────────────┤
#   │ 主 Agent │ qwen3-max                 │ claude-opus-4-6          │
#   │ heavy    │ qwen3-max                 │ claude-sonnet-4-5        │
#   │ light    │ qwen-plus                 │ claude-haiku-4-5         │
#   └──────────┴───────────────────────────┴──────────────────────────┘
#

# ==================== Provider 模板 ====================
#
# 一键切换的核心。agent.provider 决定使用哪套模板。
# 未来可扩展 deepseek / openai 等。
#

provider_templates:
  qwen:
    agent_model: "qwen3-max"           # 默认主 Agent
    agent_llm:
      max_tokens: 65536
      enable_thinking: true
      thinking_budget: 10000
      enable_caching: false
      # temperature: thinking 开启时由框架自动设置
    heavy:
      provider: "qwen"
      model: "qwen3-max"
      api_key_env: "DASHSCOPE_API_KEY"
      region: "singapore"
    light:
      provider: "qwen"
      model: "qwen-plus"
      api_key_env: "DASHSCOPE_API_KEY"
      region: "singapore"

  claude:
    agent_model: "claude-sonnet-4-5-20250929"  # 默认主 Agent（可改为 claude-opus-4-6）
    agent_llm:
      max_tokens: 128000
      enable_thinking: true
      thinking_budget: 10000
      enable_caching: true
      # temperature: thinking 开启时由框架自动设置（Claude 强制 1.0）
    heavy:
      provider: "claude"
      model: "claude-sonnet-4-5-20250929"
      api_key_env: "ANTHROPIC_API_KEY"
    light:
      provider: "claude"
      model: "claude-haiku-4-5-20251001"
      api_key_env: "ANTHROPIC_API_KEY"

# ==================== 各调用点配置 ====================
#
# tier: heavy / light（从上面的 provider_templates 解析）
# 其余为该调用点的专属参数。
#
# temperature 规范（只允许以下值）：
#   0   - 精准模式（默认）
#   0.8 - 生成/创意模式
#   不设 - enable_thinking=true 时由框架自动设置（Claude 强制 1.0）
#
# 如需覆盖某个调用点的模型，直接写 provider/model/api_key_env，
# 会跳过 tier 模板解析。
#

llm_profiles:
  # ---------- 运行时（每次请求都可能调用） ----------

  intent_analyzer:
    tier: "light"
    max_tokens: 512         # Output is ~50 token JSON, 512 is generous
    temperature: 0
    enable_thinking: false
    timeout: 30.0
    max_retries: 2

  semantic_inference:
    tier: "light"
    max_tokens: 2048
    temperature: 0
    enable_thinking: false
    timeout: 30.0
    max_retries: 2

  tool_capability_inference:
    tier: "light"
    max_tokens: 100
    temperature: 0
    enable_thinking: false
    timeout: 30.0
    max_retries: 2

  background_task:
    tier: "light"
    max_tokens: 100
    temperature: 0.8
    enable_thinking: false
    timeout: 30.0
    max_retries: 1

  # ---------- 规划系统 ----------

  plan_generator:
    tier: "light"
    max_tokens: 4096
    temperature: 0
    enable_thinking: false
    timeout: 60.0
    max_retries: 2

  plan_manager:
    tier: "heavy"
    max_tokens: 65536
    enable_thinking: true
    thinking_budget: 5000
    timeout: 120.0
    max_retries: 2
    # temperature: thinking 开启，由框架自动设置

  # ---------- 记忆系统 ----------

  fragment_extractor:
    tier: "light"
    max_tokens: 1024
    temperature: 0
    enable_thinking: false
    timeout: 30.0
    max_retries: 2

  behavior_analyzer:
    tier: "light"
    max_tokens: 2048
    temperature: 0
    enable_thinking: false
    timeout: 60.0
    max_retries: 2

  memory_update:
    tier: "light"
    max_tokens: 2048
    temperature: 0
    enable_thinking: false
    timeout: 60.0
    max_retries: 2

  # ---------- 启动时（提示词生成，仅首次或缓存失效时） ----------

  llm_analyzer:
    tier: "heavy"
    max_tokens: 32000
    temperature: 0
    enable_thinking: false
    timeout: 600.0
    max_retries: 3

  schema_generator:
    tier: "heavy"
    max_tokens: 4096
    temperature: 0
    enable_thinking: false
    timeout: 90.0
    max_retries: 2

  prompt_merger:
    tier: "heavy"
    max_tokens: 32000
    temperature: 0
    enable_thinking: false
    timeout: 600.0
    max_retries: 3

  prompt_decomposer:
    tier: "heavy"
    max_tokens: 32000
    temperature: 0
    enable_thinking: false
    timeout: 300.0
    max_retries: 2
