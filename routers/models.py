"""
Models è·¯ç”±å±‚ - LLM æ¨¡å‹ç®¡ç†

ä¸¤å±‚æ¨¡å‹ï¼š
- æ”¯æŒç›®å½•ï¼ˆSupportedï¼‰ï¼šç³»ç»ŸçŸ¥é“çš„æ‰€æœ‰æ¨¡å‹ï¼ˆpreset + customï¼‰
- æ¿€æ´»åˆ—è¡¨ï¼ˆActivatedï¼‰ï¼šç”¨æˆ·é…ç½®äº† API Key çš„æ¨¡å‹ï¼Œå®é™…å¯ç”¨

ç«¯ç‚¹æ¦‚è§ˆï¼š
- GET  /supported           â†’ æ”¯æŒçš„æ¨¡å‹ç›®å½•
- POST /supported           â†’ å‘ç›®å½•æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹
- GET  /                    â†’ ç”¨æˆ·å·²æ¿€æ´»çš„æ¨¡å‹åˆ—è¡¨
- POST /                    â†’ æ¿€æ´»æ¨¡å‹ï¼ˆå¡« API Keyï¼‰
- GET  /{model_name}        â†’ æ¨¡å‹è¯¦æƒ…ï¼ˆå«æ¿€æ´»çŠ¶æ€ï¼‰
- DELETE /{model_name}      â†’ åœç”¨æ¨¡å‹
- GET  /providers           â†’ å·²æ³¨å†Œ Provider åˆ—è¡¨
- GET  /providers/supported â†’ Provider å…ƒä¿¡æ¯ï¼ˆé€‰æ‹©å™¨ç”¨ï¼‰
- POST /providers/validate-key â†’ éªŒè¯ API Key
"""

import os
from typing import List, Optional

import httpx
from fastapi import APIRouter, HTTPException, Query, status

from logger import get_logger
from core.llm.model_registry import (
    AdapterType,
    ModelCapabilities,
    ModelConfig,
    ModelPricing,
    ModelRegistry,
    ModelType,
)
from core.llm.registry import LLMRegistry
from models.llm import (
    ActivatedModelResponse,
    ModelActivateRequest,
    ModelCapabilitiesResponse,
    ModelDetailResponse,
    ModelPricingResponse,
    ModelRegisterRequest,
    ProviderActivateRequest,
    ProviderDetailResponse,
    ProviderInfoResponse,
    ProviderModelResponse,
    ProviderValidateKeyRequest,
    ProviderValidateKeyResponse,
    SupportedModelResponse,
    ValidatedModelInfo,
)

logger = get_logger(__name__)

router = APIRouter(prefix="/api/v1/models", tags=["Models"])


# ============================================================
# Helper: build response objects from ModelConfig
# ============================================================


def _caps_response(c: ModelCapabilities) -> ModelCapabilitiesResponse:
    return ModelCapabilitiesResponse(
        supports_tools=c.supports_tools,
        supports_vision=c.supports_vision,
        supports_thinking=c.supports_thinking,
        supports_audio=c.supports_audio,
        supports_streaming=c.supports_streaming,
        max_tokens=c.max_tokens,
        max_input_tokens=c.max_input_tokens,
    )


def _pricing_response(p: ModelPricing) -> ModelPricingResponse:
    return ModelPricingResponse(
        input_per_million=p.input_per_million,
        output_per_million=p.output_per_million,
        cache_read_per_million=p.cache_read_per_million,
        cache_write_per_million=p.cache_write_per_million,
        is_free=p.is_free,
    )


# ============================================================
# æ”¯æŒç›®å½•ï¼ˆSupported Catalogï¼‰
# ============================================================


@router.get(
    "/supported",
    response_model=List[SupportedModelResponse],
    summary="è·å–æ”¯æŒçš„æ¨¡å‹ç›®å½•",
    description="åˆ—å‡ºç³»ç»Ÿæ”¯æŒçš„æ‰€æœ‰æ¨¡å‹ï¼ˆpreset + è‡ªå®šä¹‰æ³¨å†Œï¼‰ï¼Œå«æ˜¯å¦å·²æ¿€æ´»æ ‡è®°",
)
async def list_supported_models(
    type: Optional[str] = Query(None, description="æ¨¡å‹ç±»å‹è¿‡æ»¤"),
    provider: Optional[str] = Query(None, description="æä¾›å•†è¿‡æ»¤"),
):
    """
    æ”¯æŒçš„æ¨¡å‹ç›®å½•

    è¿”å›æ‰€æœ‰ç³»ç»ŸçŸ¥é“çš„æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹æ ‡æ³¨ is_activatedï¼ˆç”¨æˆ·æ˜¯å¦å·²é…ç½® API Keyï¼‰ã€‚
    å‰ç«¯ç”¨æ­¤æ¥å£æ¸²æŸ“ã€Œå¯æ·»åŠ çš„æ¨¡å‹åˆ—è¡¨ã€ã€‚
    """
    model_type_enum = None
    if type:
        try:
            model_type_enum = ModelType(type)
        except ValueError:
            pass

    models = ModelRegistry.list_models(model_type=model_type_enum, provider=provider)

    return [
        SupportedModelResponse(
            model_name=m.model_name,
            display_name=m.display_name or m.model_name,
            provider=m.provider,
            model_type=m.model_type.value,
            adapter=m.adapter.value,
            base_url=m.base_url,
            api_key_env=m.api_key_env,
            description=m.description,
            is_activated=ModelRegistry.is_activated(m.model_name),
            capabilities=_caps_response(m.capabilities),
            pricing=_pricing_response(m.pricing),
        )
        for m in models
    ]


@router.post(
    "/supported",
    response_model=dict,
    status_code=status.HTTP_201_CREATED,
    summary="å‘ç›®å½•æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹",
    description="å‘æ”¯æŒç›®å½•æ·»åŠ ä¸€ä¸ªæ–°çš„æ¨¡å‹å®šä¹‰ï¼ˆä¸å« API Keyï¼Œä»…æ‰©å±•ç›®å½•ï¼‰",
)
async def register_supported_model(request: ModelRegisterRequest):
    """
    å‘æ”¯æŒç›®å½•æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹

    ä»…æ‰©å±•ç›®å½•ï¼Œä¸æ¿€æ´»ã€‚æ¿€æ´»éœ€è°ƒç”¨ POST /api/v1/modelsã€‚
    """
    # æ ¡éªŒ model_type
    try:
        model_type = ModelType(request.model_type)
    except ValueError:
        valid_types = [t.value for t in ModelType]
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "code": "INVALID_MODEL_TYPE",
                "message": f"æ— æ•ˆçš„æ¨¡å‹ç±»å‹ '{request.model_type}'ï¼Œ"
                           f"å¯é€‰å€¼: {', '.join(valid_types)}",
            },
        )

    # æ ¡éªŒ adapter
    try:
        adapter = AdapterType(request.adapter)
    except ValueError:
        valid_adapters = [a.value for a in AdapterType]
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "code": "INVALID_ADAPTER",
                "message": f"æ— æ•ˆçš„é€‚é…å™¨ç±»å‹ '{request.adapter}'ï¼Œ"
                           f"å¯é€‰å€¼: {', '.join(valid_adapters)}",
            },
        )

    # æ ¡éªŒ provider æ˜¯å¦å·²æ³¨å†Œ
    if not LLMRegistry.is_registered(request.provider):
        available = LLMRegistry.list_providers()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "code": "UNKNOWN_PROVIDER",
                "message": f"æœªçŸ¥çš„ Provider '{request.provider}'ï¼Œ"
                           f"å¯ç”¨çš„ Provider: {', '.join(available)}",
            },
        )

    is_overwrite = ModelRegistry.is_registered(request.model_name)

    model_config = ModelConfig(
        model_name=request.model_name,
        model_type=model_type,
        adapter=adapter,
        base_url=request.base_url,
        api_key_env=request.api_key_env,
        provider=request.provider,
        display_name=request.display_name,
        description=request.description,
        capabilities=ModelCapabilities(
            supports_tools=request.capabilities.supports_tools,
            supports_vision=request.capabilities.supports_vision,
            supports_thinking=request.capabilities.supports_thinking,
            supports_audio=request.capabilities.supports_audio,
            supports_streaming=request.capabilities.supports_streaming,
            max_tokens=request.capabilities.max_tokens,
            max_input_tokens=request.capabilities.max_input_tokens,
        ),
        pricing=ModelPricing(
            input_per_million=request.pricing.input_per_million,
            output_per_million=request.pricing.output_per_million,
            cache_read_per_million=request.pricing.cache_read_per_million,
            cache_write_per_million=request.pricing.cache_write_per_million,
        ),
        extra_config=request.extra_config,
        is_custom=True,
    )

    ModelRegistry.register(model_config)
    await ModelRegistry.save_custom_models()

    action = "è¦†ç›–" if is_overwrite else "æ³¨å†Œ"
    logger.info(
        f"âœ… {action}ç›®å½•æ¨¡å‹: {request.model_name} "
        f"(provider={request.provider}, type={model_type.value})"
    )

    return {
        "success": True,
        "model_name": request.model_name,
        "action": "overwritten" if is_overwrite else "created",
        "message": f"æ¨¡å‹ '{request.model_name}' å·²æ·»åŠ åˆ°æ”¯æŒç›®å½•",
    }


# ============================================================
# å·²æ¿€æ´»æ¨¡å‹ï¼ˆActivated Modelsï¼‰
# ============================================================


@router.get(
    "",
    response_model=List[ActivatedModelResponse],
    summary="è·å–å·²æ¿€æ´»æ¨¡å‹åˆ—è¡¨",
    description="åˆ—å‡ºç”¨æˆ·å·²é…ç½® API Key çš„æ¨¡å‹ï¼ˆå®é™…å¯ç”¨çš„æ¨¡å‹ï¼‰",
)
async def list_activated_models(
    provider: Optional[str] = Query(None, description="æä¾›å•†è¿‡æ»¤"),
):
    """
    å·²æ¿€æ´»æ¨¡å‹åˆ—è¡¨

    è¿”å›ç”¨æˆ·å®é™…é…ç½®äº† API Key çš„æ¨¡å‹ã€‚
    å‰ç«¯ç”¨æ­¤æ¥å£æ¸²æŸ“ã€Œæˆ‘çš„æ¨¡å‹ã€åˆ—è¡¨ã€‚
    """
    models = ModelRegistry.list_activated(provider=provider)

    results = []
    for m in models:
        entry = ModelRegistry.get_activated_entry(m.model_name)
        results.append(
            ActivatedModelResponse(
                model_name=m.model_name,
                display_name=m.display_name or m.model_name,
                provider=m.provider,
                model_type=m.model_type.value,
                adapter=m.adapter.value,
                base_url=entry.base_url or m.base_url if entry else m.base_url,
                api_key_configured=True,
                description=m.description,
                capabilities=_caps_response(m.capabilities),
                pricing=_pricing_response(m.pricing),
                activated_at=entry.activated_at if entry else None,
            )
        )
    return results


@router.post(
    "",
    response_model=dict,
    status_code=status.HTTP_201_CREATED,
    summary="æ¿€æ´»æ¨¡å‹",
    description="é€šè¿‡æä¾› API Key æ¿€æ´»ä¸€ä¸ªæ¨¡å‹ï¼Œä½¿å…¶å¯ç”¨äº Agent",
)
async def activate_model(request: ModelActivateRequest):
    """
    æ¿€æ´»æ¨¡å‹

    æä¾› API Key æ¥æ¿€æ´»ä¸€ä¸ªæ¨¡å‹ã€‚

    - å¦‚æœæ¨¡å‹åœ¨æ”¯æŒç›®å½•ä¸­ï¼šåªéœ€ model_name + api_keyï¼ˆbase_url å¯é€‰è¦†ç›–ï¼‰
    - å¦‚æœæ¨¡å‹ä¸åœ¨ç›®å½•ä¸­ï¼šè¿˜éœ€æä¾› providerï¼ˆè‡ªåŠ¨æ³¨å†Œåˆ°ç›®å½•å¹¶æ¿€æ´»ï¼‰
    """
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨ç›®å½•ä¸­
    in_catalog = ModelRegistry.is_registered(request.model_name)

    # å¦‚æœä¸åœ¨ç›®å½•ä¸­ï¼Œå¿…é¡»æä¾› provider
    if not in_catalog and not request.provider:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "code": "PROVIDER_REQUIRED",
                "message": f"æ¨¡å‹ '{request.model_name}' ä¸åœ¨æ”¯æŒç›®å½•ä¸­ï¼Œ"
                           f"å¿…é¡»æä¾› provider å­—æ®µ",
            },
        )

    # å¦‚æœæä¾›äº† providerï¼Œæ ¡éªŒ
    if request.provider and not LLMRegistry.is_registered(request.provider):
        available = LLMRegistry.list_providers()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "code": "UNKNOWN_PROVIDER",
                "message": f"æœªçŸ¥çš„ Provider '{request.provider}'ï¼Œ"
                           f"å¯ç”¨çš„ Provider: {', '.join(available)}",
            },
        )

    was_activated = ModelRegistry.is_activated(request.model_name)

    try:
        # Build capabilities/pricing dicts for custom models
        caps_dict = None
        pricing_dict = None
        if not in_catalog:
            caps_dict = {
                "supports_tools": request.capabilities.supports_tools,
                "supports_vision": request.capabilities.supports_vision,
                "supports_thinking": request.capabilities.supports_thinking,
                "supports_audio": request.capabilities.supports_audio,
                "supports_streaming": request.capabilities.supports_streaming,
                "max_tokens": request.capabilities.max_tokens,
                "max_input_tokens": request.capabilities.max_input_tokens,
            }
            pricing_dict = {
                "input_per_million": request.pricing.input_per_million,
                "output_per_million": request.pricing.output_per_million,
                "cache_read_per_million": request.pricing.cache_read_per_million,
                "cache_write_per_million": request.pricing.cache_write_per_million,
            }

        ModelRegistry.activate_model(
            model_name=request.model_name,
            api_key=request.api_key,
            base_url=request.base_url,
            provider=request.provider,
            model_type=request.model_type,
            adapter=request.adapter,
            display_name=request.display_name,
            description=request.description,
            capabilities=caps_dict,
            pricing=pricing_dict,
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"code": "ACTIVATION_ERROR", "message": str(e)},
        )

    # æŒä¹…åŒ–
    await ModelRegistry.save_activated_models()
    # å¦‚æœæ˜¯è‡ªå®šä¹‰æ¨¡å‹ï¼Œä¹ŸæŒä¹…åŒ–åˆ°ç›®å½•
    if not in_catalog:
        await ModelRegistry.save_custom_models()

    action = "æ›´æ–°" if was_activated else "æ¿€æ´»"
    logger.info(f"âœ… æ¨¡å‹{action}: {request.model_name}")

    # æ¨¡å‹æ¿€æ´»åçƒ­é‡è½½ Agent + æ¸…é™¤ ChatService ç¼“å­˜
    # ç¡®ä¿åç»­è¯·æ±‚ä½¿ç”¨æ–°æ¿€æ´»çš„æ¨¡å‹ï¼ˆè€Œéå¯åŠ¨æ—¶ç¼“å­˜çš„æ—§æ¨¡å‹ï¼‰
    try:
        from services.agent_registry import get_agent_registry
        registry = get_agent_registry()
        reload_result = await registry.reload_agent()
        logger.info(f"ğŸ”„ æ¨¡å‹ '{request.model_name}' æ¿€æ´»åçƒ­é‡è½½ Agent: {reload_result}")
    except Exception as e:
        logger.warning(f"âš ï¸ æ¨¡å‹æ¿€æ´»å Agent çƒ­é‡è½½å¤±è´¥ï¼ˆä¸å½±å“æ¿€æ´»ï¼‰: {e}")

    try:
        from services.chat_service import get_chat_service
        get_chat_service().invalidate_llm_caches()
    except Exception as e:
        logger.warning(f"âš ï¸ æ¸…é™¤ ChatService ç¼“å­˜å¤±è´¥ï¼ˆä¸å½±å“æ¿€æ´»ï¼‰: {e}")

    return {
        "success": True,
        "model_name": request.model_name,
        "action": "updated" if was_activated else "activated",
        "message": f"æ¨¡å‹ '{request.model_name}' {action}æˆåŠŸ",
    }


# ============================================================
# Provider æŸ¥è¯¢ï¼ˆé™æ€è·¯ç”±ï¼Œå¿…é¡»åœ¨åŠ¨æ€è·¯ç”±ä¹‹å‰ï¼‰
# ============================================================


@router.get(
    "/providers",
    response_model=List[ProviderInfoResponse],
    summary="åˆ—å‡ºæ‰€æœ‰ LLM Provider",
    description="è·å–æ‰€æœ‰å·²æ³¨å†Œçš„ LLM Provider åˆ—è¡¨",
)
async def list_providers():
    """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„ LLM Provider"""
    provider_names = LLMRegistry.list_providers()

    results = []
    for name in provider_names:
        try:
            info = LLMRegistry.get_provider_info(name)
            results.append(
                ProviderInfoResponse(
                    name=info["name"],
                    display_name=info.get("display_name"),
                    default_model=info["default_model"],
                    api_key_env=info["api_key_env"],
                    description=info.get("description"),
                    supported_features=info.get("supported_features", []),
                )
            )
        except Exception as e:
            logger.warning(f"è·å– Provider '{name}' ä¿¡æ¯å¤±è´¥: {e}")

    return results


# ============================================================
# Provider å…ƒä¿¡æ¯ï¼ˆé€‰æ‹©å™¨ç”¨ï¼Œä¸å«æ¨¡å‹åˆ—è¡¨ï¼‰
# ============================================================


SUPPORTED_PROVIDERS = {
    "claude": {
        "display_name": "Claude (Anthropic)",
        "icon": "ğŸŸ ",
        "base_url": "https://api.anthropic.com",
        "api_key_env": "ANTHROPIC_API_KEY",
        "description": "Anthropic Claude ç³»åˆ—ï¼Œæ”¯æŒ Extended Thinking å’Œ Prompt Caching",
        "adapter": "claude",
        "validate_method": "anthropic",
    },
    "openai": {
        "display_name": "OpenAI",
        "icon": "ğŸŸ¢",
        "base_url": "https://api.openai.com/v1",
        "api_key_env": "OPENAI_API_KEY",
        "description": "OpenAI GPT ç³»åˆ—ï¼Œæ”¯æŒè§†è§‰å’Œå·¥å…·è°ƒç”¨",
        "adapter": "openai",
        "validate_method": "openai",
    },
    "qwen": {
        "display_name": "é€šä¹‰åƒé—® (Qwen)",
        "icon": "ğŸ”µ",
        "base_url": "https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
        "api_key_env": "DASHSCOPE_API_KEY",
        "description": "é˜¿é‡Œäº‘é€šä¹‰åƒé—®ç³»åˆ—ï¼Œæ”¯æŒ Thinking å’Œå¤šæ¨¡æ€",
        "adapter": "openai",
        "validate_method": "openai",
    },
    "deepseek": {
        "display_name": "DeepSeek",
        "icon": "ğŸ‹",
        "base_url": "https://api.deepseek.com/v1",
        "api_key_env": "DEEPSEEK_API_KEY",
        "description": "DeepSeek ç³»åˆ—ï¼Œé«˜æ€§ä»·æ¯”æ¨ç†æ¨¡å‹",
        "adapter": "openai",
        "validate_method": "openai",
    },
    "kimi": {
        "display_name": "Kimi (Moonshot)",
        "icon": "ğŸŒ™",
        "base_url": "https://api.moonshot.cn/v1",
        "api_key_env": "MOONSHOT_API_KEY",
        "description": "Moonshot AI Kimi ç³»åˆ—ï¼Œæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡",
        "adapter": "openai",
        "validate_method": "openai",
    },
    "minimax": {
        "display_name": "MiniMax",
        "icon": "ğŸ”¶",
        "base_url": "https://api.minimax.chat/v1",
        "api_key_env": "MINIMAX_API_KEY",
        "description": "MiniMax ç³»åˆ—ï¼Œæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡å’Œè¯­éŸ³",
        "adapter": "openai",
        "validate_method": "openai",
    },
    "glm": {
        "display_name": "æ™ºè°± GLM (Zhipu AI)",
        "icon": "ğŸ”®",
        "base_url": "https://open.bigmodel.cn/api/paas/v4",
        "api_key_env": "ZHIPUAI_API_KEY",
        "description": "æ™ºè°± GLM ç³»åˆ—ï¼Œæ”¯æŒ Thinkingã€Function Calling å’Œè§†è§‰",
        "adapter": "openai",
        "validate_method": "openai",
    },
}


@router.get(
    "/providers/supported",
    response_model=List[ProviderDetailResponse],
    summary="è·å–æ”¯æŒçš„ Provider å…ƒä¿¡æ¯",
    description="è¿”å›æ‰€æœ‰æ”¯æŒçš„ Provider å…ƒä¿¡æ¯ï¼ˆåç§°ã€å›¾æ ‡ã€é»˜è®¤ URLã€é€‚é…å™¨ï¼‰ã€‚ç”¨äºå‰ç«¯è¡¨å•é€‰æ‹©å™¨ã€‚",
)
async def list_supported_providers():
    """
    è·å–æ”¯æŒçš„ Provider å…ƒä¿¡æ¯

    çº¯å…ƒä¿¡æ¯ï¼Œä¸å«æ¨¡å‹åˆ—è¡¨ã€‚å‰ç«¯ç”¨æ­¤æ¸²æŸ“ Provider é€‰æ‹©ä¸‹æ‹‰æ¡†ã€‚
    æ¨¡å‹åˆ—è¡¨è¯·ä½¿ç”¨ GET /api/v1/models/supported?provider=xxx æŸ¥è¯¢ã€‚
    """
    results = []

    for provider_name, meta in SUPPORTED_PROVIDERS.items():
        api_key_env = meta["api_key_env"]
        api_key_configured = bool(os.getenv(api_key_env))

        # Get models from catalog for this provider
        provider_models = ModelRegistry.list_models(provider=provider_name)
        default_model = provider_models[0].model_name if provider_models else ""

        # Build model summaries
        model_summaries = [
            ProviderModelResponse(
                model_name=m.model_name,
                display_name=m.display_name or m.model_name,
                description=m.description,
                supports_thinking=m.capabilities.supports_thinking,
                supports_vision=m.capabilities.supports_vision,
                max_tokens=m.capabilities.max_tokens,
            )
            for m in provider_models
        ]

        results.append(
            ProviderDetailResponse(
                name=provider_name,
                display_name=meta["display_name"],
                icon=meta["icon"],
                base_url=meta["base_url"],
                api_key_env=api_key_env,
                api_key_configured=api_key_configured,
                default_model=default_model,
                description=meta["description"],
                adapter=meta["adapter"],
                models=model_summaries,
            )
        )

    return results


# ============================================================
# API Key éªŒè¯
# ============================================================


async def _validate_openai_compatible(base_url: str, api_key: str) -> tuple[bool, str, list[str]]:
    """Validate API key for OpenAI-compatible providers."""
    url = f"{base_url.rstrip('/')}/models"
    headers = {"Authorization": f"Bearer {api_key}"}

    async with httpx.AsyncClient(timeout=10.0) as client:
        resp = await client.get(url, headers=headers)

    if resp.status_code == 200:
        data = resp.json()
        models = [m.get("id", "") for m in data.get("data", [])]
        return True, "API Key éªŒè¯é€šè¿‡", models
    elif resp.status_code == 401:
        return False, "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ", []
    elif resp.status_code == 403:
        return False, "API Key æƒé™ä¸è¶³", []
    else:
        return False, f"éªŒè¯å¤±è´¥ (HTTP {resp.status_code})", []


async def _validate_anthropic(base_url: str, api_key: str) -> tuple[bool, str, list[str]]:
    """Validate API key for Anthropic Claude."""
    base = base_url.rstrip("/")
    headers = {
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json",
    }

    async with httpx.AsyncClient(timeout=10.0) as client:
        # Step 1: validate key via messages endpoint (empty body â†’ expect 400)
        url = f"{base}/v1/messages"
        body = {"model": "claude-3-5-haiku-20241022", "max_tokens": 1, "messages": []}
        resp = await client.post(url, headers=headers, json=body)

        if resp.status_code not in (200, 400):
            # Log the raw Anthropic error for debugging
            try:
                err_body = resp.json()
                err_msg = err_body.get("error", {}).get("message", resp.text[:200])
            except Exception:
                err_msg = resp.text[:200]
            logger.warning(
                f"Anthropic éªŒè¯å¤±è´¥: HTTP {resp.status_code}, body={err_msg}"
            )

            if resp.status_code == 401:
                return False, "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ", []
            elif resp.status_code == 403:
                return False, f"API Key æƒé™ä¸è¶³ ({err_msg})", []
            else:
                return False, f"éªŒè¯å¤±è´¥ (HTTP {resp.status_code}: {err_msg})", []

        # Step 2: fetch available models so the frontend can proceed
        models: list[str] = []
        try:
            models_resp = await client.get(
                f"{base}/v1/models", headers=headers,
            )
            if models_resp.status_code == 200:
                data = models_resp.json().get("data", [])
                models = [m.get("id", "") for m in data if m.get("id")]
        except Exception:
            # Models listing failed; fall back to catalog models for this provider
            catalog = ModelRegistry.list_models(provider="claude")
            models = [m.model_name for m in catalog]

        # If models API returned nothing, still fall back to catalog
        if not models:
            catalog = ModelRegistry.list_models(provider="claude")
            models = [m.model_name for m in catalog]

        return True, "API Key éªŒè¯é€šè¿‡", models


def _build_model_details(
    api_model_names: list[str], provider: str
) -> list[ValidatedModelInfo]:
    """Match API-returned model names against catalog, return enriched details.

    For models in our catalog: return full capabilities from preset data.
    For models NOT in catalog: return basic info with defaults.
    """
    catalog_models = {
        m.model_name.lower(): m
        for m in ModelRegistry.list_models(provider=provider)
    }

    details: list[ValidatedModelInfo] = []
    seen: set[str] = set()

    for name in api_model_names:
        key = name.lower()
        if key in seen:
            continue
        seen.add(key)

        catalog_entry = catalog_models.get(key)
        if catalog_entry:
            details.append(
                ValidatedModelInfo(
                    model_name=catalog_entry.model_name,
                    display_name=catalog_entry.display_name or catalog_entry.model_name,
                    provider=provider,
                    model_type=catalog_entry.model_type.value,
                    context_window=catalog_entry.capabilities.max_input_tokens,
                    max_output_tokens=catalog_entry.capabilities.max_tokens,
                    supports_tools=catalog_entry.capabilities.supports_tools,
                    supports_vision=catalog_entry.capabilities.supports_vision,
                    supports_thinking=catalog_entry.capabilities.supports_thinking,
                    in_catalog=True,
                )
            )
        else:
            details.append(
                ValidatedModelInfo(  # type: ignore[call-arg]
                    model_name=name,
                    display_name=name,
                    provider=provider,
                    model_type="llm",
                    in_catalog=False,
                )
            )

    # Catalog models first, then non-catalog; within each group, keep original order
    details.sort(key=lambda d: (not d.in_catalog, 0))
    return details


@router.post(
    "/providers/validate-key",
    response_model=ProviderValidateKeyResponse,
    summary="éªŒè¯ API Key",
    description="éªŒè¯æŒ‡å®š Provider çš„ API Key æ˜¯å¦æœ‰æ•ˆ",
)
async def validate_api_key(request: ProviderValidateKeyRequest):
    """éªŒè¯ API Key"""
    provider = request.provider.lower()

    if provider not in SUPPORTED_PROVIDERS:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "code": "UNSUPPORTED_PROVIDER",
                "message": f"ä¸æ”¯æŒçš„ Provider '{request.provider}'",
                "supported": list(SUPPORTED_PROVIDERS.keys()),
            },
        )

    meta = SUPPORTED_PROVIDERS[provider]
    base_url = request.base_url or meta["base_url"]
    validate_method = meta["validate_method"]

    try:
        if validate_method == "anthropic":
            valid, message, models = await _validate_anthropic(base_url, request.api_key)
        else:
            valid, message, models = await _validate_openai_compatible(base_url, request.api_key)

        logger.info(
            f"API Key éªŒè¯ç»“æœ: provider={provider}, valid={valid}, "
            f"message={message}, models_count={len(models)}"
        )

        # Match validated model names against catalog for rich details
        model_details = _build_model_details(models, provider) if valid else []

        return ProviderValidateKeyResponse(
            valid=valid,
            provider=provider,
            message=message,
            models=models,
            model_details=model_details,
        )

    except httpx.TimeoutException:
        return ProviderValidateKeyResponse(
            valid=False, provider=provider,
            message=f"è¿æ¥è¶…æ—¶ï¼Œæ— æ³•è®¿é—® {base_url}", models=[],
        )
    except httpx.ConnectError:
        return ProviderValidateKeyResponse(
            valid=False, provider=provider,
            message=f"æ— æ³•è¿æ¥åˆ° {base_url}ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– Base URL", models=[],
        )
    except Exception as e:
        logger.error(f"éªŒè¯ API Key å¤±è´¥: {e}", exc_info=True)
        return ProviderValidateKeyResponse(
            valid=False, provider=provider,
            message=f"éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {str(e)}", models=[],
        )


# ============================================================
# Provider æ‰¹é‡æ¿€æ´»
# ============================================================


@router.post(
    "/providers/activate",
    response_model=dict,
    status_code=status.HTTP_200_OK,
    summary="æŒ‰ Provider æ‰¹é‡æ¿€æ´»æ¨¡å‹",
    description="æä¾› API Key åï¼Œä¸€æ¬¡æ€§æ¿€æ´»è¯¥ Provider ç›®å½•ä¸­çš„æ‰€æœ‰æ¨¡å‹",
)
async def activate_provider(request: ProviderActivateRequest):
    """
    æŒ‰ Provider æ‰¹é‡æ¿€æ´»æ¨¡å‹

    éªŒè¯ Key é€šè¿‡åè°ƒç”¨æ­¤æ¥å£ï¼Œå°†è¯¥ Provider çš„æ‰€æœ‰ç›®å½•æ¨¡å‹ä¸€æ¬¡æ€§æ¿€æ´»ã€‚
    """
    provider = request.provider.lower()

    if provider not in SUPPORTED_PROVIDERS:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "code": "UNSUPPORTED_PROVIDER",
                "message": f"ä¸æ”¯æŒçš„ Provider '{request.provider}'",
                "supported": list(SUPPORTED_PROVIDERS.keys()),
            },
        )

    base_url = request.base_url or SUPPORTED_PROVIDERS[provider]["base_url"]

    activated = ModelRegistry.activate_provider_models(
        provider=provider,
        api_key=request.api_key,
        base_url=base_url,
    )

    # Persist to YAML
    await ModelRegistry.save_activated_models()

    # æ¨¡å‹æ¿€æ´»åçƒ­é‡è½½ Agentï¼Œä½¿ xiaodazi ç­‰ agent:{} å®ä¾‹
    # èƒ½é€šè¿‡ ModelRegistry.list_activated() fallback è·å–åˆ°æ–°æ¿€æ´»çš„æ¨¡å‹
    #
    # æ³¨æ„ï¼šé¦–æ¬¡å¯åŠ¨æ—¶å¦‚æœæ²¡æœ‰ API Keyï¼Œpreload_instance ä¼šå¤±è´¥ï¼Œ
    # å¯¼è‡´ registry.is_loaded=Falseã€‚æ‰€ä»¥è¿™é‡Œä¸æ£€æŸ¥ is_loadedï¼Œ
    # æ— è®ºä¹‹å‰æ˜¯å¦åŠ è½½æˆåŠŸéƒ½å°è¯•é‡æ–°åŠ è½½ã€‚
    try:
        from services.agent_registry import get_agent_registry
        registry = get_agent_registry()
        reload_result = await registry.reload_agent()
        logger.info(
            f"ğŸ”„ Provider '{provider}' æ¿€æ´»åçƒ­é‡è½½ Agent: {reload_result}"
        )
    except Exception as e:
        logger.warning(f"âš ï¸ Provider æ¿€æ´»å Agent çƒ­é‡è½½å¤±è´¥ï¼ˆä¸å½±å“æ¿€æ´»ï¼‰: {e}")

    # æ¸…é™¤ ChatService ç¼“å­˜ï¼ˆintent_llm / routers ç­‰ï¼‰
    # ç¡®ä¿åç»­è¯·æ±‚ä½¿ç”¨æ–°æ¿€æ´»çš„æ¨¡å‹
    try:
        from services.chat_service import get_chat_service
        get_chat_service().invalidate_llm_caches()
    except Exception as e:
        logger.warning(f"âš ï¸ æ¸…é™¤ ChatService ç¼“å­˜å¤±è´¥ï¼ˆä¸å½±å“æ¿€æ´»ï¼‰: {e}")

    return {
        "success": True,
        "provider": provider,
        "activated_count": len(activated),
        "models": [e.model_name for e in activated],
        "message": f"å·²æ¿€æ´» {len(activated)} ä¸ª {provider} æ¨¡å‹",
    }


# ============================================================
# å•ä¸ªæ¨¡å‹æŸ¥è¯¢å’Œåœç”¨ï¼ˆåŠ¨æ€è·¯ç”±ï¼Œå¿…é¡»åœ¨é™æ€è·¯ç”±ä¹‹åï¼‰
# ============================================================


@router.get(
    "/{model_name}",
    response_model=ModelDetailResponse,
    summary="è·å–æ¨¡å‹è¯¦æƒ…",
    description="è·å–æŒ‡å®šæ¨¡å‹çš„å®Œæ•´é…ç½®ï¼ˆå«æ¿€æ´»çŠ¶æ€ï¼‰",
)
async def get_model_detail(model_name: str):
    """
    è·å–æ¨¡å‹è¯¦æƒ…

    è¿”å›æ¨¡å‹çš„å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…æ‹¬èƒ½åŠ›ã€å®šä»·ã€é€‚é…å™¨ç±»å‹å’Œæ˜¯å¦å·²æ¿€æ´»ã€‚
    """
    config = ModelRegistry.get(model_name)
    if not config:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={
                "code": "MODEL_NOT_FOUND",
                "message": f"æ¨¡å‹ '{model_name}' æœªæ³¨å†Œ",
                "available_models": ModelRegistry.list_model_names(),
            },
        )

    return ModelDetailResponse(
        model_name=config.model_name,
        display_name=config.display_name or config.model_name,
        provider=config.provider,
        model_type=config.model_type.value,
        adapter=config.adapter.value,
        base_url=config.base_url,
        api_key_env=config.api_key_env,
        description=config.description,
        is_activated=ModelRegistry.is_activated(config.model_name),
        capabilities=_caps_response(config.capabilities),
        pricing=_pricing_response(config.pricing),
    )


@router.delete(
    "/{model_name}",
    response_model=dict,
    summary="åœç”¨æ¨¡å‹",
    description="åœç”¨æŒ‡å®šæ¨¡å‹ï¼ˆç§»é™¤ API Key é…ç½®ï¼‰",
)
async def deactivate_model(model_name: str):
    """
    åœç”¨æ¨¡å‹

    ä»æ¿€æ´»åˆ—è¡¨ä¸­ç§»é™¤ï¼Œæ¸…é™¤ API Key ç¯å¢ƒå˜é‡ã€‚
    æ¨¡å‹ä»ä¿ç•™åœ¨æ”¯æŒç›®å½•ä¸­ï¼Œå¯éšæ—¶é‡æ–°æ¿€æ´»ã€‚
    """
    if not ModelRegistry.is_activated(model_name):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={
                "code": "MODEL_NOT_ACTIVATED",
                "message": f"æ¨¡å‹ '{model_name}' æœªæ¿€æ´»",
            },
        )

    ModelRegistry.deactivate_model(model_name)
    await ModelRegistry.save_activated_models()

    logger.info(f"ğŸ—‘ï¸ æ¨¡å‹å·²åœç”¨: {model_name}")

    # æ¸…é™¤ ChatService ç¼“å­˜ï¼ˆåœç”¨åæ¡†æ¶ LLM éœ€åˆ‡æ¢åˆ°å…¶ä»–å¯ç”¨æ¨¡å‹ï¼‰
    try:
        from services.chat_service import get_chat_service
        get_chat_service().invalidate_llm_caches()
    except Exception as e:
        logger.warning(f"âš ï¸ æ¸…é™¤ ChatService ç¼“å­˜å¤±è´¥ï¼ˆä¸å½±å“åœç”¨ï¼‰: {e}")

    return {
        "success": True,
        "model_name": model_name,
        "message": f"æ¨¡å‹ '{model_name}' å·²åœç”¨ï¼ˆAPI Key å·²æ¸…é™¤ï¼Œå¯éšæ—¶é‡æ–°æ¿€æ´»ï¼‰",
    }
