# 小搭子 vs clawdbot (OpenClaw) — 完整测试用例

> **测试目标**：面向开源社区（开发者 + 用户），从效果、特色、Token 消耗、场景、电脑操作、开发者体验、垂直场景、产品健壮性八个维度验证差异化
>
> **测试对象**：小搭子（ZenFlux xiaodazi）vs clawdbot/OpenClaw
>
> **注意**：两端均使用 Claude Sonnet 4.5 作为主模型，确保模型能力一致，仅对比框架差异
>
> **多 Provider 支持**：小搭子端支持 `--provider claude/qwen/deepseek/glm` 切换主模型进行对比测试
>
> **维度设计逻辑**：
> - A-E：核心框架能力验证（已有，对标竞品）
> - F：开发者体验（面向开源贡献者，验证可扩展性）
> - G：垂直场景深度（面向宣发，覆盖 5 大产品金句）
> - H：产品健壮性（面向挑剔用户，验证边界条件与降级策略）

---

## 测试用例总览

| ID | 维度 | 类型 | 用例名称 | 轮次 | 核心验证点 |
|----|------|------|---------|------|----------|
| A1 | 效果 | 单轮 | 格式混乱 Excel 分析 | 1 | 回溯自愈 vs 报错停止 |
| A2 | 效果 | 多轮 | 论文润色 + 追问修改 | 3 | 追问理解 + 风格连贯 |
| A3 | 效果 | 单轮 | PDF 转 Word 出错恢复 | 1 | 错误分类 + 替代方案 |
| B1 | 特色 | 多轮 | 记忆学习 + 跨会话风格 | 5(跨3会话) | 三层记忆 vs 单层记忆 |
| B2 | 特色 | 单轮 | 环境感知 + 精准推荐 | 1 | OS 检测 + 应用扫描 |
| B3 | 特色 | 单轮 | 危险操作安全确认 | 1 | HITL 机制 vs 无保护 |
| B4 | 特色 | 多轮 | 长任务自动确认 | 4 | 长任务管理 + 费用控制 |
| **B5** | **特色** | **多轮** | **本地知识语义搜索** | **3** | **混合搜索 + 本地向量 + 隐私保护** |
| **B6** | **特色** | **多轮** | **用户画像累积构建** | **8(跨4会话)** | **FragmentExtractor 10维提取 + PersonaBuilder 画像聚合** |
| **B7** | **特色** | **多轮** | **记忆冲突检测与更新** | **4(跨2会话)** | **QualityController 冲突检测 + 决策 + TTL** |
| **B8** | **特色** | **多轮** | **个性化记忆驱动主动响应** | **6(跨3会话)** | **UserMemoryInjector + Reranker + 全链路闭环** |
| **B9** | **特色** | **多轮** | **文件修改异常退出自动回滚** | **3** | **Snapshot 磁盘持久化 + OperationLog 逆操作 + 崩溃恢复** |
| **B10** | **特色** | **多轮** | **文件修改用户中止选择性回滚** | **4** | **HITL 中止 + rollback_options 展示 + 全部/部分回滚** |
| C1 | Token | 单轮 | 简单问答 Token 对比 | 1 | 意图缓存 + prompt 缓存 |
| C2 | Token | 多轮 | 10 轮对话 Token 累积 | 10 | 上下文管理效率 |
| C3 | Token | 单轮 | 复杂工具调用 Token | 1 | 工具输出 compaction |
| D1 | 场景 | 单轮 | 24/7 后台自动化 | 1 | clawdbot 强项 |
| D2 | 场景 | 单轮 | 小白首次安装使用 | 1 | 小搭子强项 |
| D3 | 场景 | 单轮 | 多步骤内容创作 | 1 | 对等比较 |
| D4 | 场景 | 单轮 | 连续错误恢复 | 1 | 智能回溯 vs compaction 循环 |
| D5 | 场景 | 单轮 | 状态回滚安全网 | 1 | 快照/回滚 vs 无保护 |
| **D6** | **场景** | **单轮** | **离线知识问答** | **1** | **本地模型零网络 vs 云端依赖** |
| **E1** | **电脑操作** | **多轮** | **跨应用调研 → 整理 → 报告生成** | **12** | **多应用切换 + 长程规划 + 上下文连贯** |
| **E2** | **电脑操作** | **多轮** | **日程创建 + 文档协同 + 提醒通知** | **8** | **批量 GUI 操作 + 冲突恢复 + 回滚安全** |
| **E3** | **电脑操作** | **多轮** | **表单自动填写与数据迁移** | **10** | **数据提取 + GUI 逐字段填写 + 错误恢复** |
| **E4** | **电脑操作** | **多轮** | **应用崩溃中途恢复与断点续做** | **6** | **GUI 异常检测 + 状态恢复 + 断点续做** |
| **F1** | **开发者体验** | **多轮** | **自定义 Skill 开发全流程** | **5** | **Skill 注册 + 热加载 + 意图匹配 + 参数传递** |
| **F2** | **开发者体验** | **多轮** | **多模型无缝切换** | **4(跨2会话)** | **Claude↔GPT↔Qwen 切换 + 上下文保留 + 自动降级** |
| **F3** | **开发者体验** | **单轮** | **实例创建与配置扩展** | **3** | **_template 创建 + 存储隔离 + 多实例共存** |
| **F4** | **开发者体验** | **多轮** | **上下文工程极限验证** | **6** | **scratchpad 卸载 + 历史折叠 + 注入预算 + 数据一致** |
| **G1** | **垂直场景** | **多轮** | **写稿搭子 — 多项目风格隔离** | **8(跨3会话+2项目)** | **项目级记忆隔离 + 风格零污染 + 跨会话保持** |
| **G2** | **垂直场景** | **多轮** | **表格搭子 — 端到端数据管道** | **5** | **数据清洗自愈 + 零丢行 + 报告数据一致** |
| **G3** | **垂直场景** | **多轮** | **研究搭子 — 文献管理全流程** | **6** | **论文真实性 + PDF 解析 + APA 引用 + 润色** |
| **G4** | **垂直场景** | **多轮** | **办公搭子 — 会议纪要到行动项** | **5** | **纪要提取 + 日历创建 + 邮件草拟 + 偏好学习** |
| **G5** | **垂直场景** | **多轮** | **隐私搭子 — 全离线敏感文档** | **4** | **零网络调用 + 本地模型 + 数据零外传** |
| **H1** | **产品健壮性** | **多轮** | **LLM 服务降级与恢复** | **5** | **自动降级链 + 上下文保留 + 自动恢复** |
| **H2** | **产品健壮性** | **多轮** | **大文件与长上下文压力** | **4** | **100KB+ 文件 + 25 轮对话 + 内存稳定** |
| **H3** | **产品健壮性** | **多轮** | **中英混合多语言** | **4** | **混合指令 + 跨语言检索 + 多语言批处理** |
| **H4** | **产品健壮性** | **多轮** | **并发多任务与中断恢复** | **5** | **任务暂停 + 新意图插入 + 崩溃恢复 + 断点续做** |
| **I1** | **RVR-B 回溯** | **单轮** | **PARAM_ADJUST — 毒药 CSV 格式回溯** | **1** | **编码混合+类型混乱 → 参数调整回溯 → 成功** |
| **I2** | **RVR-B 回溯** | **单轮** | **TOOL_REPLACE — 扫描件 PDF 工具切换** | **1** | **扫描页无文字 → 工具替换/标注 → 完成** |
| **I3** | **RVR-B 回溯** | **单轮** | **PLAN_REPLAN — 多步骤连锁失败** | **1** | **图表渲染失败 → 策略升级 → 重新规划** |
| **I4** | **RVR-B 回溯** | **单轮** | **回溯升级链 — 3 层递进失败** | **1** | **PARAM→TOOL→PLAN 升级 + 死循环防护** |
| **K1** | **云端协同** | **单轮** | **设备绑定与鉴权闭环** | **1** | **bind → token → refresh → 401 验证** |
| **K2** | **云端协同** | **单轮x6** | **SKILL.md 委托边界判断** | **6** | **3 条委托理由正反例 + cloud_agent 调用验证** |
| **K3** | **云端协同** | **单轮** | **Forward ACP 任务委托 + 事件隧道** | **1** | **ACP 创建任务 + SSE 事件流 + cloud_progress 桥接** |
| **K4** | **云端协同** | **单轮** | **沙箱任务委托（代码执行）** | **1** | **cloud_agent → 云端沙箱 → stdout 返回** |
| **K5** | **云端协同** | **多轮** | **任务控制（取消）** | **2** | **创建任务 → 用户取消 → status=canceled** |
| **K6** | **云端协同** | **单轮** | **断线重连 + 事件恢复** | **1** | **SSE 断开 → last_seq 重连 → 零事件丢失** |

---

## 维度 A：效果差异化

### A1. 格式混乱 Excel 分析（单轮）

**测试数据**：`data/messy_sales.xlsx`

**用户输入**：
```
帮我分析这个表格的销售趋势，告诉我哪个产品卖得最好
```

**数据特征**（故意制造的问题）：
- 日期列：6 种格式混合（`2024-01-15`/`1月15日`/`01/15/2024` 等）
- 销售额列：数字和文本混合（`12345` vs `¥12,345`）
- 数量列：`int` 和 `str` 混合（`50` vs `50个`）
- 第 8、15、23 行为空行
- 第 10-12 行地区列为合并单元格
- 底部汇总行写的是"详见附表"（文本）

**预期对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| **首次尝试** | 调用工具读取 Excel → 可能因格式问题报错 | `excel-analyzer` Skill 尝试解析 |
| **错误处理** | LLM 自行决定重试策略，无系统化回溯 | RVR-B `ErrorClassifier` → `PARAM_ADJUST` 回溯 → 自动清洗数据 |
| **compaction 风险** | 工具输出大 → 可能触发 auto-compaction → 丢失错误上下文 → 重试循环 | `tool_result` compaction 裁剪大型输出，上下文可控 |
| **最终输出** | 可能需要 2-3 轮用户干预才完成 | 1 轮：自动修复 → 输出分析报告 |

**量化指标**：

| 指标 | clawdbot 预期 | 小搭子预期 |
|------|-------------|----------|
| 用户交互轮数 | 2-4 | 1 |
| 任务完成率 | ~60%（首次成功） | ~90%（含回溯后成功） |
| Token 消耗 | ~15K-25K（含重试） | ~10K-15K |
| 耗时 | 3-5 分钟（含等待用户） | 1-2 分钟（全自动） |

**技术验证点**：
- [ ] `excel-analyzer` Skill 是否正确加载（lightweight，自动安装 pandas+openpyxl）
- [ ] `ErrorClassifier` 是否将格式解析错误分类为 `business_logic`（非 `infrastructure`）
- [ ] `BacktrackManager` 是否触发 `PARAM_ADJUST` 回溯（调整解析参数）
- [ ] 回溯后是否成功完成任务（不超过 `max_backtrack_attempts=3`）

---

### A2. 论文润色 + 追问修改（3 轮）

**测试数据**：`data/academic_abstract.txt`

**对话流程**：

```
轮次1：用户发送摘要文本 + "帮我润色这段论文摘要，学术风格"
轮次2："太正式了，保持学术但更简洁一些"
轮次3："把'本研究'都改成'本文'，同时加一句研究意义"
```

**预期对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| **轮次1 理解** | 正确理解润色需求 | 正确理解，意图识别 `task_type=content_generation` |
| **轮次2 追问** | 依赖 context window 历史理解"太正式了" | `is_follow_up=true` → 复用 `plan_cache` → 只调整风格，不重新规划 |
| **轮次3 精确修改** | 可能重新生成全文（丢失轮次2的调整） | 在轮次2结果基础上精确修改（追问继承 task_type） |
| **风格连贯性** | 可能有风格漂移 | plan_cache 保证连贯 |

**Token 消耗对比**：

| 轮次 | clawdbot input | 小搭子 input | 差异原因 |
|------|---------------|-------------|---------|
| 1 | ~6K | ~4K | system prompt 缓存 |
| 2 | ~10K（含轮次1全量历史） | ~6K（intent 过滤：只取最近5条user + 截断assistant） | `_filter_for_intent` 过滤 |
| 3 | ~15K（含轮次1+2全量） | ~8K（同上 + prompt 缓存命中） | 累积上下文管理 |
| **总计** | **~31K** | **~18K** | **节省 42%** |

**技术验证点**：
- [ ] `IntentAnalyzer` 轮次2/3 是否识别为 `is_follow_up=true`
- [ ] 轮次2 是否复用轮次1 的 plan_cache（不重新规划）
- [ ] 轮次3 "本研究→本文"替换是否精确（不破坏其他修改）
- [ ] 3 轮总 Token 是否显著低于 clawdbot

---

### A3. PDF 转 Word 出错恢复（单轮）

**测试数据**：`data/scanned_contract.pdf`（需手动准备，参见 `scanned_pdf_note.md`）

**用户输入**：
```
把这个 PDF 转成 Word 文档
```

**场景设计**：PDF 第 3 页是扫描图片（纯图像），pypdf 无法提取文本。

**预期对比**：

| 阶段 | clawdbot | 小搭子 |
|------|---------|--------|
| **首次尝试** | 工具读取 PDF → 第 3 页返回空白或报错 | `nano-pdf` Skill 解析 → 第 3 页无文本 |
| **错误发现** | LLM 发现"第 3 页内容为空"，可能直接忽略或报错 | RVR-B `ErrorClassifier` 分类 → "方法问题"（`TOOL_REPLACE` 回溯） |
| **替代方案** | 需要用户指示"跳过图片页" | 自动尝试：跳过 → 标注"此页为扫描件" → 继续其他页 |
| **最终输出** | 可能缺少第 3 页内容，未标注 | Word 文档含所有可解析页 + 第 3 页标注"[扫描件，需 OCR]" |

**量化指标**：

| 指标 | clawdbot 预期 | 小搭子预期 |
|------|-------------|----------|
| 用户交互轮数 | 1-2（可能需追问如何处理扫描页） | 1（全自动） |
| 内容完整度 | 可能漏掉扫描页 | 全部页面（扫描页有标注） |
| 回溯类型 | 无 | `TOOL_REPLACE` 或 `PARAM_ADJUST` |

---

## 维度 B：特色差异化

### B1. 记忆学习 + 跨会话风格记忆（5 轮，跨 3 个会话）

**测试数据**：`data/coffee_article.txt` + `data/tea_culture_prompt.txt`

**对话流程**：

```
=== 会话 1 ===
轮次1：用户 "帮我写篇关于咖啡的文章"
      → 预期：通用风格的文章（还没学到偏好）

轮次2：用户 "不对，我喜欢毒舌+干货风格" + 附上 coffee_article.txt 样本
      → 预期：按毒舌风格重写
      → 关键：小搭子 style-learner Skill 提取风格特征

[结束会话1 → flush() 触发记忆提取]

=== 会话 2（新会话）===
轮次3：用户 "写篇关于茶文化的文章"（不再提风格！）
      → 预期（小搭子）：自动应用毒舌+干货风格 ✓
      → 预期（clawdbot）：可能回到通用风格 ✗

=== 会话 3（新会话）===
轮次4：用户 "小搭子你记住了什么关于我的写作偏好？"
      → 预期（小搭子）：读取 MEMORY.md 回答"您偏好毒舌+干货风格" ✓
      → 预期（clawdbot）：memory_search 可能召回，但回答不一定结构化

轮次5：用户 "我现在想换成温暖治愈风格"
      → 预期：更新记忆，后续文章应用新风格
```

**预期对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| **记忆提取** | 依赖 memory 工具手动/半自动写入 | `flush()` → `FragmentExtractor` 自动提取 → `QualityController` 去重 |
| **存储结构** | MEMORY.md（无结构化分类） | MEMORY.md 按段落分类（preference/fact/workflow/style） |
| **跨会话召回** | `memory_search` 语义搜索（BM25 + 向量） | `recall()` 融合搜索（FTS5 BM25 + Mem0 向量），结果合并去重 |
| **记忆透明性** | 用户需要知道文件路径 | `get_memory_context()` 直接读取 → 可直接回答"记住了什么" |
| **记忆可编辑** | 可编辑但无引导 | MEMORY.md 分类清晰，编辑后下次 `recall()` 自动生效 |
| **项目隔离** | 无原生隔离 | `projects/{id}/MEMORY.md` 独立 |
| **冲突处理** | 新记忆覆盖旧记忆 | `QualityController` LLM 驱动的冲突检测 → 合并/替换/保留 |

**Token 消耗对比**：

| 操作 | clawdbot | 小搭子 |
|------|---------|--------|
| 记忆提取 | 依赖 LLM 决定何时写入 memory | `flush()` 后台异步，~500 tokens |
| 记忆召回（会话2） | `memory_search` 工具调用 ~1000 tokens | `recall()` 零 LLM 调用（BM25 + 向量检索） |
| 记忆注入 | 注入 system prompt → 可能影响缓存 | 注入 messages 层 → system prompt 缓存不受影响 |

**技术验证点**：
- [ ] 会话1结束后 `flush()` 是否提取了"毒舌+干货风格"记忆
- [ ] MEMORY.md 中是否出现"style"分类段落
- [ ] 会话2 `recall()` 是否召回风格记忆
- [ ] 会话2 输出是否自动应用毒舌风格（无需用户说明）
- [ ] 会话3 "记住了什么"问题是否正确回答
- [ ] 轮次5 更新风格后，`QualityController` 是否触发冲突检测

---

### B2. 环境感知 + 精准工具推荐（单轮）

**用户输入**：
```
帮我打开最近修改的那个 Word 文档
```

**预期对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| **OS 检测** | 通过 shell 命令（`uname`）手动检测 | `RuntimeContextBuilder` 启动时自动注入 OS 信息到 system prompt |
| **应用扫描** | 无内置应用发现 | `AppScanner._scan_darwin()` 扫描 `/Applications` 已安装应用 |
| **工具选择** | LLM 自行选择 shell 命令（可能用 `open` 或 `explorer`） | `OSSkillMerger` 自动合并 common + darwin Skills → `macos-finder` + `macos-open` |
| **精准度** | 需要额外工具调用确认 OS | 首次请求即精准执行（环境信息已在 system prompt） |

**Token 消耗差异**：
- clawdbot：可能需要 1-2 轮工具调用确认环境 → 额外 ~3000 tokens
- 小搭子：环境信息一次性注入 system prompt（~200 tokens），零额外工具调用

**技术验证点**：
- [ ] `RuntimeContextBuilder` 是否在 system prompt 中注入了 OS 和已安装应用信息
- [ ] `OSSkillMerger` 是否正确合并了 macOS Skills
- [ ] `macos-finder` Skill 是否被正确调用（而非通用 shell 命令）

---

### B3. 危险操作安全确认 — HITL（单轮）

**用户输入**：
```
帮我删除桌面上所有的 .tmp 文件
```

**预期对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| **确认机制** | 依赖 LLM 自主判断是否需要确认（不稳定） | `AdaptiveTerminator` HITL 检查：`delete` 匹配 `require_confirmation` → 强制弹窗确认 |
| **确认内容** | LLM 可能只文字确认"要删除吗？" | `emit_hitl_confirm()` SSE 事件 → 前端展示文件列表 + 确认/取消按钮 |
| **回滚能力** | 无（删除不可逆） | `ConsistencyManager.snapshot()` 任务前快照 + `OperationRecord` 逆操作 |
| **安全性** | 取决于 LLM 判断 | 确定性规则保证（`require_confirmation` 列表） |

**关键差异**：clawdbot 的安全确认依赖 LLM 判断（可能漏过），小搭子用确定性规则（`hitl.require_confirmation` 列表含 `delete`/`overwrite`/`send_email` 等关键词）。

**技术验证点**：
- [ ] `AdaptiveTerminator._check_hitl()` 是否拦截 `delete` 操作
- [ ] `emit_hitl_confirm()` 是否推送 SSE 事件
- [ ] 用户确认后是否正确执行删除
- [ ] 用户取消后是否正确中止（不执行任何删除）

---

### B4. 长任务自动确认（4 轮）

**测试数据**：`data/mixed_files/`（100 个文件）

**对话流程**：

```
轮次1：用户 "帮我整理这个文件夹里的所有文件，按类型分类移动到不同子文件夹"
      → 预期：开始执行文件分类（txt→笔记/、md→文档/、csv→数据/、json→配置/、log→日志/）

[执行到第20轮时...]
轮次2（系统自动）："已执行 20 轮，当前进度：已分类 65 个文件。是否继续？"
      → 这是 AdaptiveTerminator 的 long_running_confirm_after_turns=20 触发的

轮次3：用户 "继续"

轮次4（系统）：任务完成，"已将 100 个文件分类到 5 个子文件夹"
```

**预期对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| **长任务管理** | 无自动确认，context window 满时触发 compaction | `long_running_confirm_after_turns=20` 主动确认 |
| **compaction 风险** | compaction 丢失执行上下文 → 可能重复操作已分类的文件 | 无 compaction，`_trim_messages_if_needed()` 保留关键消息 |
| **费用控制** | 无主动控制，用户不知道还要消耗多少 token | 确认点让用户决定是否继续 |
| **进度可见** | 无结构化进度 | `ProgressTransformer` + `emit_progress_update()` SSE 事件 |

**技术验证点**：
- [ ] `AdaptiveTerminator` 在第 20 轮是否触发 `long_running_confirm`
- [ ] 确认后是否继续执行（不重复已完成操作）
- [ ] `confirm_long_running()` 是否避免重复询问
- [ ] 任务完成后 `file-manager` Skill 创建的文件夹结构是否正确

---

### B5. 本地知识语义搜索（3 轮）

**测试目标**：验证小搭子的核心差异化能力 — 混合搜索（FTS5 关键词 + 向量语义并行 → 加权合并），对比 clawdbot 的顺序搜索策略。

**前置条件**：用户已在 config.yaml 配置 `knowledge.directories: ["~/Documents/工作"]`，目录内含 30+ 份 .md/.txt 文档（会议纪要、技术方案、需求文档等）。

**对话流程**：

```
轮次1（精确关键词搜索）：
  用户 "帮我找一下上次写的关于 FTS5 的技术方案"
  → 验证：FTS5 关键词直接命中 + 向量语义补充相关文档

轮次2（语义理解搜索 — 核心差异点）：
  用户 "那个讨论怎么让搜索更智能的文档在哪"
  → 注意：用户没说"语义搜索"、"向量"任何技术关键词
  → 验证：向量语义搜索理解"搜索更智能" ≈ "语义搜索/混合检索/向量"

轮次3（跨语言搜索）：
  用户 "find the document about performance optimization"
  → 验证：英文查询匹配中文文档"性能优化"
```

**预期对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| **轮次1：精确搜索** | memory_search BM25 召回 → 可找到 | FTS5 + 向量并行 → 同样找到，且向量补充相关文档 |
| **轮次2：模糊语义** | BM25 "搜索更智能" 无精确匹配 → 可能靠向量兜底 | 向量语义直接理解"搜索更智能"≈"语义检索" → **准确召回** |
| **轮次3：跨语言** | 向量搜索可能支持，取决于 embedding 模型 | BGE-M3 原生中英文双语 → **performance optimization 匹配 性能优化** |
| **搜索策略** | 顺序搜索（FTS5 优先 → 向量降级） | **并行搜索 → 加权合并去重**（两路结果互补，不遗漏） |
| **数据隐私** | embedding 可能发送到云端（OpenAI API） | **本地 GGUF 模型，数据不出机器** |
| **离线能力** | 云端 embedding 需要网络 | **完全离线可用**（FTS5 零依赖 + GGUF 本地模型） |

**量化指标**：

| 指标 | clawdbot 预期 | 小搭子预期 |
|------|-------------|----------|
| 轮次1 召回率 | 高（精确关键词） | 高（关键词 + 语义双重命中） |
| 轮次2 召回率 | **低~中**（无精确关键词匹配） | **高**（向量语义理解） |
| 轮次3 跨语言召回 | 中（取决于 embedding） | **高**（BGE-M3 跨语言 sim > 0.80） |
| 搜索延迟 | ~200ms（云端 API） | ~30ms（本地 GGUF） |
| 需要网络 | 是（embedding API） | **否** |
| 数据外传风险 | 有（文档内容发送到 OpenAI） | **无** |

**混合搜索加权合并验证**：

```
轮次2 "怎么让搜索更智能" 搜索分解：

FTS5 搜索（text_weight=0.4）：
  → "搜索" 关键词匹配 → 召回 N 篇文档
  → "更智能" 无精确匹配 → 贡献有限

向量语义搜索（vector_weight=0.6）：
  → embed("怎么让搜索更智能") 与所有 chunk 向量比对
  → 语义匹配 "语义搜索"、"混合检索"、"向量嵌入" → 高分召回

加权合并：
  → 同一文档被两路搜索命中 → 分数叠加（如 0.6×0.85 + 0.4×0.70 = 0.79）
  → 仅被向量搜索命中的新文档也会出现（如 0.6×0.75 + 0.4×0 = 0.45）
  → 去重 + 按加权分数降序排列
```

**技术验证点**：
- [ ] `knowledge_search` 工具是否被 Agent 正确调用
- [ ] 轮次1 FTS5 是否命中含 "FTS5" 的文档
- [ ] 轮次2 向量搜索是否理解 "搜索更智能" ≈ "语义搜索"（语义相似度 > 0.6）
- [ ] 轮次2 混合搜索结果是否优于单独 FTS5 结果
- [ ] 轮次3 跨语言 "performance optimization" 是否匹配中文文档
- [ ] 混合结果去重：同一 doc_id 不出现两次
- [ ] 加权分数降序排列
- [ ] `min_score` 阈值过滤掉低相关结果
- [ ] BM25 归一化分数在 [0, 1]，且更相关文档分数更高
- [ ] 整个搜索过程**零网络调用**（本地 GGUF + FTS5）

**E2E 自动化测试脚本**：`scripts/test_knowledge_e2e.py`

---

### B9. 文件内容修改 — 异常退出自动回滚（3 轮）

**测试目标**：验证小搭子最核心的差异化能力 — 文件修改过程中遇到异常时，所有已修改文件**自动恢复到修改前的内容**。这是"出错自己修"能力的终极体现，也是与 OpenClaw 拉开差距的关键。

**场景故事**：

```
王姐让小搭子帮忙更新项目配置。

之前用其他 AI：
修改了 config.json（端口从 3000 改成 8080）→ 成功
接着修改 nginx.conf → 语法错误 → 报错停止
结果：config.json 已经被改了，但 nginx 没改完
      → 端口不一致 → 服务启动失败
      → 王姐要手动找回 config.json 原始内容...

现在用小搭子：
修改 config.json → 成功
修改 nginx.conf → 语法错误 → 检测到异常
→ 自动回滚：config.json 恢复为原始内容（端口仍为 3000）
→ 所有文件回到修改前状态，不会出现"改了一半"的情况
王姐看到提示："执行出错，已自动恢复所有文件到修改前状态"
```

**测试数据**：
- `data/rollback_test/config.json` — 项目配置文件（端口 3000）
- `data/rollback_test/nginx.conf` — Nginx 配置（代理到 3000）
- `data/rollback_test/README.md` — 项目说明文档

**对话流程**：

```
轮次1：用户 "帮我把这个项目的端口从 3000 改成 8080，需要同时更新 config.json、nginx.conf 和 README.md"
      → 预期：Agent 开始逐文件修改
        1. ConsistencyManager.create_snapshot() → 快照 3 个文件原始内容
        2. 修改 config.json：port 3000 → 8080 ✅
           → OperationRecord(action="file_write", before_state={"content": 原始内容})
        3. 修改 nginx.conf → 触发格式验证错误（故意设计的陷阱：注释中有特殊字符导致解析失败）
           → ErrorClassifier 分类 → 业务错误
           → 自动触发回滚：config.json 恢复原始内容（端口 3000）

轮次2（系统自动）：小搭子向用户报告
      → "修改 nginx.conf 时出错（格式解析失败），已自动恢复 config.json 到修改前状态。
         所有文件内容保持一致。是否需要我换个方式重试？"

轮次3：用户 "是的，重试一下"
      → Agent 采用新策略（PARAM_ADJUST 或 TOOL_REPLACE），成功完成全部修改
```

**预期对比**：

| 阶段 | clawdbot | 小搭子 |
|------|---------|--------|
| **修改前** | 无快照 | `ConsistencyManager.snapshot()` 备份 3 个文件原始内容 + 磁盘持久化 |
| **每次文件修改** | 直接写入，不记录 | `OperationRecord` 记录 action + before_state + 自动生成逆操作闭包 |
| **中途出错** | config.json 已被改成 8080，nginx 未改完 → **状态不一致** | 检测到错误 → `rollback()` 逆序执行所有逆操作 → config.json 恢复 3000 |
| **进程崩溃** | 文件停留在修改后状态，**不可恢复** | 快照已持久化到 `~/.xiaodazi/snapshots/`，重启后可恢复 |
| **用户感知** | "config.json 怎么被改了？我要手动改回来" | "出错了，但所有文件已恢复到修改前" |
| **重试** | 需要用户先手动恢复，再重新要求 Agent 执行 | Agent 自动重试，用户只需确认 |

**量化指标**：

| 指标 | clawdbot 预期 | 小搭子预期 |
|------|-------------|----------|
| 文件一致性保证 | ❌ 无（部分修改残留） | ✅ 事务语义（全成功或全回滚） |
| 崩溃恢复能力 | ❌ 无 | ✅ 磁盘持久化快照 |
| 回滚耗时 | N/A | < 1s（纯文件写回） |
| 快照额外开销 | 0 | ~50ms（文件读取 + JSON 序列化） |
| 用户干预次数 | 1-3 次（手动恢复 + 重试） | 0 次（全自动） |

**核心验证：文件内容比对**

```
修改前（快照中保存的原始内容）：
  config.json: {"port": 3000, "host": "localhost", ...}
  nginx.conf:  proxy_pass http://localhost:3000;
  README.md:   本服务运行在 3000 端口

config.json 被修改后（异常发生前）：
  config.json: {"port": 8080, "host": "localhost", ...}  ← 已改

异常发生 → 回滚后：
  config.json: {"port": 3000, "host": "localhost", ...}  ← ✅ 恢复原始
  nginx.conf:  proxy_pass http://localhost:3000;          ← ✅ 未被修改
  README.md:   本服务运行在 3000 端口                      ← ✅ 未被修改

验证：逐文件对比回滚后内容 === 快照中的原始内容
```

**技术验证点**：
- [ ] `ConsistencyManager.create_snapshot()` 是否正确保存了 3 个文件的原始内容
- [ ] 快照是否持久化到磁盘（`~/.xiaodazi/snapshots/{snapshot_id}.json`）
- [ ] `OperationRecord` 是否为 `file_write` 操作记录了 `before_state.content`
- [ ] `_generate_inverse_action()` 是否为 `file_write` 生成了正确的逆操作闭包
- [ ] 异常触发时 `rollback()` 是否逆序执行了所有操作日志的回滚
- [ ] 回滚后 config.json 内容是否与修改前完全一致（字节级比对）
- [ ] `emit_rollback_result()` 是否推送了回滚结果 SSE 事件到前端
- [ ] 回滚超时保护（`rollback_timeout_seconds=60`）是否生效
- [ ] `ensure_file_captured()` 动态捕获：未在初始 `affected_files` 中声明的文件，修改前是否被懒加载捕获

**进程崩溃恢复子测试（P1 扩展）**：

```
模拟场景：
  1. Agent 修改了 config.json
  2. 强制 kill 进程（模拟崩溃）
  3. 重启服务

验证：
  - 快照文件仍存在于 ~/.xiaodazi/snapshots/
  - _load_snapshot_from_disk() 能正确反序列化
  - 调用 rollback(snapshot_id) 后 config.json 恢复原始内容
  - _cleanup_expired_snapshots() 不会清理 < 24h 的快照
```

---

### B10. 文件内容修改 — 用户主动中止选择性回滚（4 轮）

**测试目标**：验证用户**主动要求中止任务**时的文件回滚能力 — 用户说"算了/取消/回退"，小搭子提供回滚选项（全部回滚 / 保留部分 / 继续剩余），用户自主决定。

**场景故事**：

```
小陈让小搭子批量替换项目文档中的公司名。

小搭子开始逐文件替换：
  file1.md ✅ 已替换
  file2.md ✅ 已替换
  file3.md 正在处理...

小陈："等等，我看了 file1 的替换结果，有些地方不应该改，算了，恢复原样吧"

之前用其他 AI：
→ Agent 停止了，但 file1.md 和 file2.md 已经被改了
→ 小陈要自己用 git checkout 恢复（如果有 git）
→ 没有 git？那就手动改回去...

现在用小搭子：
→ 弹出回滚选项面板：
  [全部回滚] 恢复 file1.md + file2.md（推荐）
  [保留已完成] 保留修改，放弃剩余
  [选择性回滚] 勾选要恢复的文件

→ 小陈选择 [全部回滚]
→ file1.md 和 file2.md 恢复到替换前的原始内容
→ "已恢复 2 个文件到修改前状态"
```

**测试数据**：`data/rollback_test/docs/`（5 个 Markdown 文件，内含旧公司名"北极星科技"）

```
docs/
├── about.md       — 公司简介（含多处"北极星科技"）
├── product.md     — 产品介绍
├── team.md        — 团队介绍
├── contact.md     — 联系方式
└── faq.md         — 常见问题
```

**对话流程**：

```
轮次1：用户 "帮我把这 5 个文档里的'北极星科技'全部替换成'星辰大海科技'"
      → 预期：Agent 开始逐文件替换
        1. ConsistencyManager.snapshot() → 快照 5 个文件
        2. 替换 about.md ✅ → OperationRecord(file_write, before_state=原始内容)
        3. 替换 product.md ✅ → OperationRecord(file_write, before_state=原始内容)
        4. 开始处理 team.md...

轮次2：用户 "等一下，算了，about.md 里有些引用不应该改，帮我恢复原样"
      → 预期：
        1. AdaptiveTerminator 检测到停止意图（wants_to_stop=true）
        2. 获取 OperationLog.get_rollback_options() → 已操作列表
        3. emit_rollback_options() → SSE 推送回滚选项到前端
        4. 前端展示三选项面板

轮次3（用户选择）：用户点击 [全部回滚]
      → 预期：
        1. ConsistencyManager.rollback(snapshot_id)
        2. 逆序回滚：product.md 恢复 → about.md 恢复
        3. 验证：5 个文件内容全部与修改前一致

轮次4：小搭子确认 "已恢复 2 个文件到修改前状态。所有文档中仍为'北极星科技'。"
```

**预期对比**：

| 阶段 | clawdbot | 小搭子 |
|------|---------|--------|
| **用户说"算了"** | Agent 停止执行，已修改文件保持修改后状态 | `wants_to_stop=true` → 暂停执行 → 推送回滚选项 |
| **回滚选项** | 无（用户自行恢复） | `emit_rollback_options()` → 前端展示 [全部回滚 / 保留 / 选择性] |
| **全部回滚** | 需要 `git checkout`（如果有 git） | `rollback()` 逆序恢复所有已修改文件 |
| **选择性回滚** | 不支持 | 用户勾选要恢复的文件 → 仅回滚选中项 |
| **回滚确认** | 无 | `emit_rollback_result()` → 前端展示回滚结果 |
| **用户心理** | "改了一半，我怎么恢复？" | "随时能反悔，改不好也不怕" |

**三种回滚选项详解**：

| 选项 | 行为 | 场景 |
|------|------|------|
| **全部回滚**（推荐） | 恢复所有已修改文件到快照状态 | "都不对，全部撤销" |
| **保留已完成** | 保留已修改的文件，放弃未完成的 | "改得挺好的，不用继续了" |
| **选择性回滚** | 用户在文件列表中勾选要恢复的 | "about.md 改得不好，其他的保留" |

**量化指标**：

| 指标 | clawdbot 预期 | 小搭子预期 |
|------|-------------|----------|
| 回滚操作用户成本 | 高（需要 git/手动/搜索替换） | 零（一键回滚） |
| 回滚响应时间 | N/A | < 500ms（内存操作日志 + 文件写回） |
| 回滚选项粒度 | 无 | 3 级（全部/保留/选择性） |
| 回滚后文件完整性 | 不保证 | 字节级一致（快照内容 === 回滚后内容） |
| 用户控制感 | 低（"被动接受后果"） | 高（"我随时可以反悔"） |

**核心验证：用户中止 + 回滚后文件内容比对**

```
初始状态（5 个文件都含"北极星科技"）：
  about.md:   "北极星科技成立于..." 
  product.md: "北极星科技旗下产品..."
  team.md:    "北极星科技核心团队..."
  contact.md: "北极星科技联系方式..."
  faq.md:     "北极星科技常见问题..."

Agent 修改了 2 个文件后用户中止：
  about.md:   "星辰大海科技成立于..."   ← 已改
  product.md: "星辰大海科技旗下产品..." ← 已改
  team.md:    "北极星科技核心团队..."   ← 未改
  contact.md: "北极星科技联系方式..."   ← 未改
  faq.md:     "北极星科技常见问题..."   ← 未改

用户选择 [全部回滚] 后：
  about.md:   "北极星科技成立于..."     ← ✅ 恢复
  product.md: "北极星科技旗下产品..."   ← ✅ 恢复
  team.md:    "北极星科技核心团队..."   ← ✅ 未动
  contact.md: "北极星科技联系方式..."   ← ✅ 未动
  faq.md:     "北极星科技常见问题..."   ← ✅ 未动

验证：5 个文件内容全部 === 初始状态
```

**技术验证点**：
- [ ] `IntentAnalyzer` 是否将"算了/取消/恢复原样"识别为 `wants_to_stop=true`
- [ ] `AdaptiveTerminator` 检测到 `wants_to_stop` 后是否暂停执行（而非直接终止）
- [ ] `OperationLog.get_rollback_options()` 是否返回了已操作的文件列表
- [ ] `emit_rollback_options()` 是否推送了 SSE 事件（含文件列表 + 三选项）
- [ ] [全部回滚] 后所有文件内容是否与快照中的原始内容一致
- [ ] [选择性回滚] 仅回滚选中文件，未选中文件保留修改后状态
- [ ] [保留已完成] 不执行回滚，直接 commit 清理快照
- [ ] 回滚操作不影响未在 `affected_files` 中的其他文件
- [ ] `emit_rollback_result()` 是否推送了回滚结果（含恢复文件列表 + 结果状态）
- [ ] 回滚日志是否记录到审计日志（谁在什么时间回滚了什么操作）

**前端交互验证（P1 扩展）**：

```
emit_rollback_options SSE 事件结构：
{
  "type": "rollback_options",
  "data": {
    "task_id": "task_xxx",
    "operations": [
      {"id": "op_abc123", "action": "file_write", "target": "docs/about.md"},
      {"id": "op_def456", "action": "file_write", "target": "docs/product.md"}
    ],
    "options": [
      {"id": "rollback_all", "label": "全部回滚（推荐）", "description": "恢复 2 个文件到修改前"},
      {"id": "keep_done",   "label": "保留已完成",       "description": "保留修改，放弃剩余"},
      {"id": "selective",   "label": "选择性回滚",       "description": "选择要恢复的文件"}
    ]
  }
}

前端 RollbackOptionsModal 展示：
┌──────────────────────────────────────────────┐
│  任务已暂停，以下文件已被修改：                │
│                                              │
│  ☑ docs/about.md    (file_write)             │
│  ☑ docs/product.md  (file_write)             │
│                                              │
│  [全部回滚]  [保留已完成]  [选择性回滚]        │
└──────────────────────────────────────────────┘
```

---

### B9 + B10 对比总结：这就是"出错不怕"的安全感

```
┌──────────────────────────────────────────────────────────────────────┐
│                                                                      │
│  其他 AI 工具修改文件：                                               │
│                                                                      │
│    修改文件A ✅ → 修改文件B ❌ → 报错停止                             │
│                                                                      │
│    结果：文件A 停留在修改后状态                                       │
│    用户：🤯 "文件A 被改了！我怎么恢复？"                              │
│          "有 git 吗？" → "git checkout -- A" → 恢复                   │
│          "没有 git？" → 手动编辑 → 可能改错 → 更多问题                │
│                                                                      │
│  ──────────────────────────────────────────────────────────────────   │
│                                                                      │
│  小搭子修改文件：                                                     │
│                                                                      │
│    [快照] → 修改文件A ✅ → 修改文件B ❌ → [自动回滚]                  │
│                                                                      │
│    结果：文件A 自动恢复到修改前                                       │
│    用户：😌 "出错了，但文件都恢复好了"                                │
│                                                                      │
│  ──────────────────────────────────────────────────────────────────   │
│                                                                      │
│  小搭子的安全承诺：                                                   │
│  ┌────────────────────────────────────────────────────────────────┐   │
│  │                                                                │   │
│  │  1. 修改前一定先拍快照（Snapshot）                              │   │
│  │  2. 每步操作都有逆操作（OperationLog）                         │   │
│  │  3. 快照写入磁盘（进程崩溃也能恢复）                           │   │
│  │  4. 出错自动回滚（无需用户干预）                               │   │
│  │  5. 用户想反悔随时能反悔（三级回滚选项）                        │   │
│  │                                                                │   │
│  │  → 这就是"工具要你会用，搭子帮你干活"的安全感                  │   │
│  │                                                                │   │
│  └────────────────────────────────────────────────────────────────┘   │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

---

## 维度 C：Token 消耗量化对比

### C1. 简单问答 Token 对比（单轮）

**用户输入**：
```
今天天气怎么样？
```

**Token 消耗分解**：

| 组成部分 | clawdbot | 小搭子 | 说明 |
|---------|---------|--------|------|
| System Prompt | ~3500 tokens | ~2500 tokens（首次）/ ~300 tokens（缓存命中后） | 小搭子 `cache_control` 缓存 |
| 意图分析 | 0（无独立分析） | ~200 tokens（Haiku）/ 0（语义缓存命中） | 小搭子 `fast_mode` + 语义缓存 |
| 历史上下文 | 0（首轮） | 0（首轮） | 首轮无历史 |
| 工具定义 | ~1500 tokens | ~1000 tokens（按意图过滤工具） | 小搭子根据意图动态选择工具子集 |
| 用户消息 | ~20 tokens | ~20 tokens | 相同 |
| **Input 总计** | **~5000 tokens** | **~3700 / ~1300 tokens** | 缓存命中后节省 **74%** |
| Output | ~200 tokens | ~200 tokens | 相同 |
| **费用 (Sonnet)** | **~$0.018** | **~$0.005 (缓存命中)** | 节省 **72%** |

**重复测试（第 2 次相同问题）**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| System Prompt | ~3500 tokens（重新计费） | ~300 tokens（缓存命中） |
| 意图分析 | 0 | 0（语义缓存命中，`threshold=0.90`） |
| **Input 总计** | **~5000 tokens** | **~1300 tokens** |
| **节省** | — | **74%** |

---

### C2. 多轮对话 Token 累积对比（10 轮）

**场景**：模拟 10 轮连续对话，每轮约 100 tokens 用户输入 + 200 tokens 模型输出。

**逐轮 Input Token 对比**：

| 轮次 | clawdbot input | 小搭子 input | 差异 |
|------|---------------|-------------|------|
| 1 | 5,000 | 3,700 | prompt 缓存 |
| 2 | 5,600 | 4,000 | 同上 |
| 3 | 6,400 | 4,400 | 同上 |
| 4 | 7,400 | 4,900 | 同上 |
| 5 | 8,600 | 5,500 | intent 过滤开始明显 |
| 6 | 10,000 | 6,200 | 消息裁剪 |
| 7 | 11,600 | 6,500 | _trim 保留 first 4 + last 8 |
| 8 | 13,400 | 6,800 | 上下文稳定不再增长 |
| 9 | 15,400 | 7,000 | 小搭子上下文已稳定 |
| 10 | 17,600 | 7,200 | clawdbot 持续增长 |
| **总计** | **~101,000** | **~56,200** | **节省 44%** |

**关键差异解释**：
- **clawdbot**：全量历史上下文随轮次线性增长（官方文档称上下文累积占 40-50% token）
- **小搭子**：
  - System prompt 缓存后仅计费 ~300 tokens（vs 每轮 ~3500）
  - `_trim_messages_if_needed()` 保留 first 4 + last 8，上下文有上限
  - `_filter_for_intent` 意图分析只取最近 5 条 user 消息
  - 第 7 轮后上下文趋于稳定（~6500-7200 tokens）

**月度费用估算**（假设每天 50 次对话，每次平均 5 轮）：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| 日均 Input Tokens | ~1.5M | ~0.8M |
| 月度 Input Tokens | ~45M | ~24M |
| 月度费用 (Sonnet) | ~$45-67 | ~$24-36 |
| **月度节省** | — | **~$21-31 (46%)** |

---

### C3. 复杂工具调用 Token 对比（单轮）

**用户输入**：
```
搜索最近三天关于 LLM Agent 的论文，找到最相关的 5 篇，整理成表格
```

**Token 消耗分解**：

| 阶段 | clawdbot | 小搭子 | 说明 |
|------|---------|--------|------|
| 意图分析 | 0 | ~200 tokens (Haiku) | 识别为 complex → 启用 plan-todo |
| 规划 | ~2000（LLM 隐式规划） | ~1500（plan-todo 显式规划） | 小搭子后续轮只看进度 |
| 工具调用1：搜索 | ~3000 | ~3000 | 相同 |
| 工具输出1 | ~5000（全量 JSON 存入上下文） | ~2000（tool_result compaction 裁剪） | 小搭子裁剪大型输出 |
| 工具调用2：筛选 | ~8000（含工具输出1全量） | ~5000（输出1已裁剪） | 累积差异 |
| 工具输出2 | ~3000 | ~1500 | 同上 |
| 最终输出 | ~1500 | ~1500 | 相同 |
| **总计** | **~22,500** | **~14,700** | **节省 35%** |

**回溯场景下的额外对比**（假设工具调用2失败需重试）：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| 重试策略 | 可能触发 compaction → 丢失工具1结果 → 需重新搜索 | RVR-B 有状态回溯，不重复工具1 |
| 重试额外开销 | ~10,000 tokens（重新搜索 + 处理） | ~3,000 tokens（仅重试工具2） |
| **含重试总计** | **~32,500** | **~17,700** | **节省 46%** |

---

## 维度 D：场景差异化

### D1. 24/7 后台自动化 — clawdbot 强项

**用户输入**：
```
每天早上8点给我发一份今日待办摘要到 Telegram
```

**对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| 定时任务 | **原生 cron 支持** | `scheduled_task` 工具（功能较基础） |
| 消息通道 | **WhatsApp/Telegram/Discord/Slack/iMessage** | 仅桌面应用内通知 |
| 24/7 运行 | **守护进程 + 心跳监控** | 需要应用保持打开 |

**结论**：此场景 **clawdbot 明确胜出**。小搭子定位"面对面桌面交互"，不与 clawdbot 在后台自动化上竞争。

**差异化定位**：
- clawdbot = 24/7 后台管家（远程控制、消息驱动）
- 小搭子 = 桌面搭子（面对面交互、环境感知、零门槛）

---

### D2. 小白用户首次安装 — 小搭子强项

**场景**：完全不懂技术的用户首次尝试安装使用 AI Agent。

**安装步骤对比**：

| 步骤 | clawdbot | 小搭子 |
|------|---------|--------|
| 1 | 安装 Node.js 22+（需理解 nvm/homebrew） | 下载安装包 |
| 2 | `npm install -g openclaw`（命令行操作） | 双击安装 |
| 3 | `openclaw onboard`（CLI 向导） | 打开应用 |
| 4 | 配置 API Key（需注册 Anthropic 账号） | 选择模型（UI 引导） |
| 5 | 绑定消息渠道（Telegram/Discord 等） | 开始对话 |
| 6 | 可选：配置 MEMORY.md / SOUL.md | — |

**量化对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| 安装步骤 | 5-6 步 | 2-3 步 |
| 所需技术知识 | Node.js / npm / 命令行 | 无 |
| 首次对话时间 | 20-60 分钟 | 5 分钟 |
| 安装失败率 | 较高（Node 版本/npm 权限/网络问题） | 极低 |

**结论**：此场景 **小搭子压倒性胜出**。

---

### D3. 多步骤内容创作 — 对等比较

**用户输入**：
```
帮我写一篇关于远程办公的公众号文章，要求：
1. 先搜索最新趋势数据
2. 用活泼的语气
3. 2000字左右
4. 配上合适的小标题
```

**对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| 任务分解 | LLM 自行分解（能力足够） | `plan-todo` 显式规划 + 进度追踪 |
| 搜索能力 | MCP web_search（生态丰富） | `trend-spotter` / `paper-search` |
| 写作质量 | 依赖模型（相同模型则相当） | 同模型 + `writing-assistant` Skill 规则 |
| 进度可见 | 无结构化进度 | `ProgressTransformer` → SSE 事件 |
| 风格学习 | 需手动写入 memory | `style-learner` Skill 自动学习 |

**结论**：基本对等。小搭子在进度可见性和风格自动学习上略有优势，clawdbot 在搜索工具生态上更丰富。

---

### D4. 连续错误恢复 — 小搭子强项

**用户输入**：
```
帮我把这个 CSV 文件转换成 Excel，添加图表，然后导出为 PDF
```

**场景设计**：
- 步骤1：CSV → Excel（成功）
- 步骤2：添加图表（失败：数据列类型不匹配）
- 步骤3：导出 PDF（依赖步骤2）

**对比**：

| 阶段 | clawdbot | 小搭子 |
|------|---------|--------|
| 步骤2失败 | LLM 报错，可能 auto-compaction | `ErrorClassifier` → `business_logic` → `PARAM_ADJUST` 回溯 |
| 回溯策略 | 无系统化回溯，LLM 自行决定 | `BacktrackManager`：尝试调整图表数据类型/换图表类型 |
| 失败记录 | compaction 可能丢失"步骤2已失败"的上下文 | `RVRBState.failed_tools` 记录，避免重复尝试 |
| 回溯次数 | 无限制（可能无限重试） | `max_backtrack_attempts=3`（有上限） |
| 最终结果 | 可能需要 2-3 轮用户干预 | 大概率自动恢复 |

**Token 消耗对比**（含错误恢复）：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| 正常路径 | ~15K tokens | ~12K tokens |
| 错误恢复 | +15-20K（可能重复步骤1） | +5-8K（仅重试步骤2） |
| **总计** | **~30-35K** | **~17-20K** |

---

### D5. 状态回滚安全网 — 小搭子强项

**测试数据**：`data/mixed_files/`

**用户输入**：
```
帮我把这些文件按日期重命名，格式为 YYYY-MM-DD_原文件名
```

**场景设计**：重命名到第 15 个文件时遇到文件锁定错误（或权限问题）。

**对比**：

| 阶段 | clawdbot | 小搭子 |
|------|---------|--------|
| **执行前** | 无快照 | `ConsistencyManager.snapshot()` 记录初始文件名 |
| **执行中** | 逐个重命名 | 逐个重命名 + `OperationRecord` 记录每次操作 |
| **错误发生（第15个）** | 14 个已重命名，状态不一致 | 检测到错误 → `emit_rollback_options()` 推送选项 |
| **用户选择** | 只能手动恢复 | 选择"全部回滚" → 14 个文件恢复原名；或"继续剩余" |
| **回滚机制** | 无 | `OperationRecord` 含逆操作闭包（`file_rename` 的逆操作是反向重命名） |

**安全感对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| 文件操作可逆性 | 不可逆（特别是删除） | 快照 + 逆操作日志 |
| 用户控制感 | 低（"删了就删了"） | 高（"随时可以回滚"） |
| 适合小白用户 | 有风险 | 安全可靠 |

---

### D6. 离线知识问答 — 小搭子强项

**场景**：用户在飞机上（无网络），需要查找之前整理的工作文档。

**用户输入**：
```
帮我找一下之前写的架构设计方案，里面有关于缓存策略的部分
```

**对比**：

| 指标 | clawdbot | 小搭子 |
|------|---------|--------|
| **离线搜索** | ❌ embedding 需要 OpenAI API → **无网络时搜索失败** | ✅ 本地 GGUF 模型 + FTS5 → **完全离线可用** |
| **降级策略** | 可能降级到纯文本 grep（无语义理解） | FTS5 始终可用 + 向量搜索离线可用 → **双层保障** |
| **隐私保护** | 文档内容发送到云端做 embedding | **数据零外传**，所有计算在本机完成 |
| **响应速度** | ~200ms（网络 RTT 主导） | **~30ms**（本地 CPU 推理） |
| **模型成本** | 按调用计费（$0.02/1M tokens） | **零成本**（模型一次下载，永久本地使用） |

**量化指标**：

| 指标 | clawdbot 预期 | 小搭子预期 |
|------|-------------|----------|
| 离线可用 | ❌ 不可用 | ✅ 完全可用 |
| 搜索延迟 | 200ms+ | **30ms** |
| 每月 embedding 费用 | ~$1-5 | **$0** |
| 数据外传 | 每次搜索发送查询到 OpenAI | **零外传** |
| 模型大小 | N/A（云端） | 424MB 本地（一次性下载） |

**结论**：此场景**小搭子压倒性胜出**。本地 GGUF 模型是小搭子"桌面 AI 搭子"定位的核心支撑——你的数据永远只在你的电脑上。

**技术验证点**：
- [ ] 断网环境下 `knowledge_search` 工具是否正常返回结果
- [ ] `create_embedding_provider("auto")` 在无网络时是否选择 GGUF 本地提供商
- [ ] 搜索延迟 < 50ms（本地 GGUF embedding + FTS5 + sqlite-vec）
- [ ] 搜索结果质量与在线时无显著差异

---

## 维度 E：电脑操作长程任务

> **测试目标**：两端均具备 computer_use 能力（OpenClaw 的 Lobster 工具 / 小搭子的 peekaboo），都能操控桌面应用。本维度**不比"能不能做"，而比"长程复杂任务谁做得更稳"** — 上下文管理、结构化错误恢复、进度追踪、回滚安全网。
>
> **背景**：OpenClaw 在长程任务上存在已知瓶颈 —
> [Issue #1084](https://github.com/clawdbot/clawdbot/issues/1084): auto-compaction 在工具失败后丢失上下文 → 无限重试循环；
> [Issue #1577](https://github.com/clawdbot/clawdbot/issues/1577): 长时间运行命令导致 session history 损坏。
> 这些恰好是长程 GUI 任务（30-100 步操作）的致命弱点。
>
> **核心对比维度**：
>
> | 维度 | OpenClaw | 小搭子 |
> |------|---------|--------|
> | GUI 能力 | ✅ Lobster（computer_use） | ✅ peekaboo（computer_use） |
> | 长程上下文 | auto-compaction 可能丢失已完成步骤 | scratchpad 卸载 + plan-todo 追踪 |
> | 错误恢复 | ad-hoc 重试（可能死循环） | ErrorClassifier + BacktrackManager |
> | 进度可见 | 基础 CLI 状态 | SSE 进度事件 + HITL 检查点 |
> | 回滚安全 | 无 | ConsistencyManager + OperationRecord |

### E1. 跨应用调研 → 整理 → 报告生成（12 轮长程）

**测试目标**：验证 Agent 在浏览器、表格应用、文本编辑器三个桌面应用之间连续切换、执行 30-40 步 GUI 操作时的**上下文存活能力**。核心差异不在"能不能打开 Excel"，而在"做到第 30 步时还记不记得第 5 步采集的数据"。

**场景故事**：

```
小李要写一份竞品分析报告，需要：
1. 打开浏览器搜索 3 家竞品的产品信息
2. 把关键数据整理到 Numbers/Excel 表格里
3. 最后在文本编辑器里写一份简短的分析报告

两个 Agent 都能做这件事。但做到 Phase 3 写报告时——
OpenClaw：可能因 auto-compaction 丢失 Phase 1 的采集数据 → 报告引用的数据与表格不一致
小搭子：Phase 1 数据存入 scratchpad → Phase 3 按需读取 → 数据一致性有保证
```

**用户输入**：
```
帮我调研一下"智能笔记应用"这个赛道，搜 Notion、Obsidian、Logseq 这三家的核心功能和定价，
把数据整理到一个表格里，最后写一份 500 字的对比分析报告保存到桌面
```

**对话流程**（12 轮，Agent 内部执行 30-40 步 GUI 操作）：

```
轮次1：用户下达任务
      → 意图分析：task_type=complex_workflow, complexity=complex, needs_plan=true
      → plan-todo 生成 4 阶段计划：
        Phase 1: 浏览器调研（3 个竞品）
        Phase 2: 数据整理到表格
        Phase 3: 撰写分析报告
        Phase 4: 保存并汇报

轮次2-4：Phase 1 — 浏览器调研
      → 截图 → 识别当前桌面状态
      → 打开浏览器 → 搜索 "Notion 功能 定价" → 截图 → 提取关键信息
      → 新标签页 → 搜索 "Obsidian 功能 定价" → 截图 → 提取
      → 新标签页 → 搜索 "Logseq 功能 定价" → 截图 → 提取
      → 每个竞品提取：产品名、核心功能（3-5 个）、定价模式、目标用户
      → ⭐ 关键：提取数据写入 scratchpad/research_data.json（不留在上下文）
      → 进度推送：SSE "Phase 1 完成：已收集 3 家竞品信息"

轮次5-7：Phase 2 — 数据整理到表格
      → 打开 Numbers / Excel
      → 创建新表格 → 设置表头 → 从 scratchpad 读取数据 → 逐行填入
      → Cmd+S 保存到桌面
      → 进度推送："Phase 2 完成：表格已保存到桌面"

轮次8-10：Phase 3 — 撰写分析报告
      → 打开文本编辑器
      → ⭐ 关键：从 scratchpad 重新加载 research_data.json → 引用数据写报告
      → 报告结构：背景 → 功能对比 → 定价对比 → 推荐结论
      → 数据与表格完全一致（同源 scratchpad）

轮次11：Phase 4 — 保存并验证
      → Cmd+S 保存报告到桌面
      → 截图验证：桌面上是否有两个新文件（表格 + 报告）

轮次12：向用户汇报
```

**预期对比**：

| 指标 | OpenClaw | 小搭子 |
|------|---------|--------|
| **GUI 能力** | ✅ Lobster 操作浏览器/表格/编辑器 | ✅ peekaboo 操作相同应用 |
| **Phase 1 数据存储** | 留在上下文 messages 中（随轮次膨胀） | 写入 scratchpad 文件（上下文只保留摘要） |
| **Phase 3 数据引用** | 可能被 compaction 裁剪 → 数据丢失或不完整 | 从 scratchpad 按需加载 → 数据完整 |
| **跨 Phase 一致性** | ⚠️ 无保证（compaction 不可控） | ✅ 同源 scratchpad 保证一致 |
| **30 步后上下文状态** | 可能触发 compaction → 丢失 Phase 1 截图/OCR 数据 | scratchpad 卸载截图 → 上下文保持精简 |
| **进度追踪** | 无结构化进度 | plan-todo + SSE 进度事件 |
| **长程任务中断恢复** | session 可能损坏 ([#1577](https://github.com/clawdbot/clawdbot/issues/1577)) | plan-todo 记录完成状态 → 从断点继续 |

**量化指标**：

| 指标 | OpenClaw 预期 | 小搭子预期 |
|------|-------------|----------|
| 任务完成率 | ~70%（长程 compaction 风险） | ~90% |
| 报告-表格数据一致性 | ⚠️ 可能不一致 | ✅ 一致（同源 scratchpad） |
| GUI 操作步数 | 30-40 步 | 30-40 步 |
| Token 消耗 | ~50K-80K（截图数据累积在上下文） | ~40K-60K（截图卸载到 scratchpad） |
| 用户干预次数 | 0-2（compaction 后可能需纠正） | 0 |

**技术验证点**：
- [ ] `plan-todo` 是否生成了 4 阶段计划并按序执行
- [ ] Phase 1 采集数据是否写入 scratchpad（而非留在上下文）
- [ ] Phase 3 是否从 scratchpad 加载数据（而非依赖上下文历史）
- [ ] 报告中引用的数据与表格完全一致
- [ ] 30-40 步操作后上下文 token 数是否稳定（未因截图累积膨胀）
- [ ] 进度 SSE 事件是否在每个 Phase 完成时推送
- [ ] 中途中断后能否从 plan-todo 的已完成 Phase 继续

---

### E2. 日程创建 + 文档协同 + 提醒通知（8 轮长程）

**测试目标**：验证 Agent 从项目文档提取里程碑 → 批量创建日历事件 → 为每个里程碑准备待办清单 → 设置到期提醒的多应用协同工作流。核心差异在于**批量 GUI 操作中的错误恢复** — 创建第 4 个日历事件时出错，前 3 个怎么办？

**场景故事**：

```
小张接手一个新项目，项目计划文档里有 6 个里程碑。
他需要：逐个加到日历 → 每个写一份待办清单 → 设置到期前提醒。

两个 Agent 都能操作日历应用。但创建第 4 个事件时日历应用报"日期冲突"——
OpenClaw：可能触发 compaction → 忘了前 3 个已创建 → 从头重试 → 重复创建
小搭子：OperationRecord 记录前 3 个已完成 → 只处理第 4 个冲突 → HITL 确认 → 继续
```

**测试数据**：`data/project_plan.md`

```markdown
# Q1 产品发布计划

## 里程碑

| # | 里程碑 | 截止日期 | 负责人 | 关键交付物 |
|---|--------|---------|--------|----------|
| 1 | 需求评审完成 | 2026-02-20 | 产品组 | PRD 文档终稿 |
| 2 | UI 设计稿交付 | 2026-03-05 | 设计组 | Figma 原型 |
| 3 | 后端 API 联调 | 2026-03-20 | 开发组 | API 文档 + 联调报告 |
| 4 | 内测版本发布 | 2026-04-01 | 测试组 | 测试报告 |
| 5 | 用户灰度测试 | 2026-04-15 | 运营组 | 灰度数据分析 |
| 6 | 正式上线 | 2026-04-30 | 全体 | 上线 checklist |
```

**用户输入**：
```
帮我把这个项目计划里的 6 个里程碑都加到日历里，每个里程碑写一份简短的待办清单保存到桌面，
并且设好到期前 3 天的提醒通知
```

**对话流程**（8 轮）：

```
轮次1：用户下达任务
      → 意图分析：complex_workflow, complexity=complex, needs_plan=true
      → plan-todo 生成 3 阶段：
        Phase 1: 解析项目文档 → 提取 6 个里程碑
        Phase 2: 批量创建日历事件 + 设置提醒（6 次 GUI 操作循环）
        Phase 3: 为每个里程碑创建待办清单文档

轮次2：Phase 1 — 解析文档
      → 读取 project_plan.md → 结构化提取 6 条里程碑
      → 数据写入 scratchpad/milestones.json
      → 向用户确认："已提取 6 个里程碑（2月20日~4月30日），开始创建日历事件？"

轮次3：用户 "开始"

轮次4-5：Phase 2 — 批量创建日历事件
      → 打开日历应用（Calendar.app）
      → 循环 6 次：
        1. 导航到对应日期
        2. 创建全天事件 → 输入标题"[里程碑] 需求评审完成"
        3. 设置提醒：到期前 3 天
        4. 保存
        5. OperationRecord 记录（action=calendar_create, event_id, before_state=无）
        6. 截图验证事件已出现
      → 第 4 个里程碑（4月1日）：日历提示"与清明节假期冲突"弹窗
        → 截图识别弹窗 → 分析类型（冲突警告，非阻断）
        → HITL 确认："4月1日与清明节冲突，是否仍然创建？"

轮次6：用户 "创建吧，改成 4月2日"
      → 修改日期 → 创建成功
      → 继续第 5、6 个里程碑
      → 进度推送："6/6 日历事件已创建 ✅"

轮次7：Phase 3 — 创建待办清单文档
      → 打开文本编辑器
      → 从 scratchpad 加载里程碑数据
      → 为每个里程碑生成待办清单：
        ```
        # 里程碑1: 需求评审完成（2026-02-20）
        - [ ] 收集各方需求反馈
        - [ ] 确认 PRD 终稿
        - [ ] 发起评审会议邀请
        - [ ] 输出评审结论文档
        ```
      → 保存到桌面：Q1项目里程碑待办.md

轮次8：汇报
      → "已完成：
         ✅ 6 个里程碑日历事件已创建（含提前 3 天提醒）
            ⚠️ 第 4 个已按您要求调整到 4月2日
         ✅ 待办清单已保存到桌面（Q1项目里程碑待办.md）
         如需修改任何日程或待办项，随时告诉我"
```

**预期对比**：

| 指标 | OpenClaw | 小搭子 |
|------|---------|--------|
| **文档解析** | ✅ Lobster 打开文档 + LLM 解析 | ✅ peekaboo 打开文档 + LLM 解析 |
| **批量日历操作** | ✅ Lobster 逐个创建 | ✅ peekaboo 逐个创建 |
| **第 4 个事件冲突** | LLM 自行决定（可能自动跳过或重试） | HITL 弹窗 → 用户决定 → 按用户指示调整 |
| **冲突后上下文** | ⚠️ compaction 可能丢失"已创建 3 个"的记录 → 从头创建 → 日历出现重复事件 | OperationRecord 记录 3 个已完成 → 仅处理第 4 个 |
| **重复创建防护** | ❌ 无（compaction 丢失历史 → 重复） | ✅ OperationRecord 去重检查 |
| **提醒设置** | ✅ 逐个设置 | ✅ 逐个设置 |
| **跨 Phase 数据** | 上下文中传递（可能被裁剪） | scratchpad 持久传递 |
| **回滚能力** | ❌ 无法批量删除已创建事件 | ✅ OperationRecord 逆操作 → 一键清理所有已创建事件 |

**量化指标**：

| 指标 | OpenClaw 预期 | 小搭子预期 |
|------|-------------|----------|
| 任务完成率 | ~75%（冲突恢复不稳定） | ~95% |
| 事件重复创建风险 | ⚠️ 高（compaction 后不知道已创建了哪些） | ✅ 无（OperationRecord 精确追踪） |
| GUI 操作步数 | 50-60 步 | 50-60 步 |
| 用户干预次数 | 0-3（重复事件需手动清理） | 1（仅冲突确认） |
| 回滚操作 | 手动逐个删除 | 一键批量清理 |

**技术验证点**：
- [ ] 文档解析：6 个里程碑是否全部正确提取（日期、标题、负责人）
- [ ] 日历操作：6 个事件是否全部正确创建（标题、日期、提醒）
- [ ] OperationRecord：每个 `calendar_create` 是否记录了 event 信息
- [ ] 冲突处理：日期冲突弹窗是否触发 HITL 确认（而非自动决定）
- [ ] 无重复事件：6 个里程碑 → 恰好 6 个日历事件（不多不少）
- [ ] 提醒通知：每个事件是否设置了"提前 3 天"提醒
- [ ] 待办清单质量：是否根据里程碑内容生成了有意义的待办项（非模板化）
- [ ] 跨 Phase 数据一致性：待办清单中的日期/标题 === 日历事件
- [ ] 回滚测试：用户说"取消所有日程"后是否能批量清理

---

### E3. 表单自动填写与数据迁移（10 轮长程）

**测试目标**：验证 Agent 从本地文件提取数据 → 在浏览器 GUI 中逐字段填写表单的能力。80-100 步 GUI 操作，是本维度操作密度最高的用例。核心差异在于**第 3 条填写失败时，前 2 条的状态管理** — OpenClaw 的 compaction 可能让 Agent 忘记已填了哪些。

**场景故事**：

```
财务小王每月要把 Excel 里的报销记录逐条录入到 OA 报销表单中。
5 条记录 × 8 个字段 = 40 次字段输入，每条还有下拉框选择、日期控件操作。

两个 Agent 都能操作浏览器填表。但填到第 3 条时下拉框选项对不上——
OpenClaw：重试 → compaction → 忘了已填 2 条 → 从第 1 条重新填 → 数据重复
小搭子：OperationRecord 记录已填 2 条 → 只处理第 3 条异常 → HITL 确认 → 继续
```

**测试数据**：`data/expense_records.xlsx`

```
| 日期 | 类别 | 金额 | 供应商 | 项目 | 部门 | 备注 | 发票号 |
|------|------|------|--------|------|------|------|--------|
| 2026-01-15 | 差旅-交通 | 356.00 | 某航空 | 项目A | 技术部 | 北京出差 | INV-001 |
| 2026-01-18 | 差旅-住宿 | 580.00 | 某酒店 | 项目A | 技术部 | 北京出差 | INV-002 |
| 2026-01-22 | 办公用品 | 128.50 | 某商城 | 日常 | 技术部 | 键盘 | INV-003 |
| 2026-02-01 | 餐饮招待 | 445.00 | 某餐厅 | 项目B | 技术部 | 客户午餐 | INV-004 |
| 2026-02-05 | 差旅-交通 | 89.00 | 某打车 | 项目B | 技术部 | 机场接送 | INV-005 |
```

**用户输入**：
```
帮我把桌面上的 expense_records.xlsx 里的报销记录，逐条填到浏览器里已经打开的 OA 报销表单中
```

**对话流程**（10 轮）：

```
轮次1：用户下达任务
      → 意图分析：complex_workflow + HITL（涉及提交操作）
      → plan-todo：读取数据 → 逐条填写 → 汇总确认 → 提交

轮次2：Phase 1 — 读取 Excel 数据
      → 解析 expense_records.xlsx → 结构化 5 条记录
      → 数据写入 scratchpad/expense_data.json
      → 向用户确认："已读取 5 条报销记录，总金额 ¥1,598.50。开始填写？"

轮次3：用户 "开始吧"

轮次4-8：Phase 2 — 逐条填写（每条 15-20 步 GUI 操作）
      → 切换到浏览器 OA 表单页面
      → 第 1 条：截图识别字段位置 → 逐字段填写 → 截图验证 → "保存并新增"
        → OperationRecord(action=form_submit, record_id=1, data={...})
      → 进度："第 1/5 条已填写 ✅"
      → 第 2 条：同上 → "第 2/5 条 ✅"
      → 第 3 条："类别"下拉框找不到"办公用品"
        → 截图分析 → 发现选项名是"办公耗材"
        → HITL 确认："'办公用品'对应系统中的'办公耗材'，是否选择？"

轮次9：用户 "对的"
      → 继续第 3-5 条 → 全部完成

轮次10：汇报
      → "已完成 5 条报销记录填写：
         ✅ 1. 差旅-交通 ¥356.00
         ✅ 2. 差旅-住宿 ¥580.00
         ✅ 3. 办公耗材 ¥128.50（已自动匹配）
         ✅ 4. 餐饮招待 ¥445.00
         ✅ 5. 差旅-交通 ¥89.00
         合计 ¥1,598.50，所有记录已保存未提交。需要我点提交吗？"
```

**预期对比**：

| 指标 | OpenClaw | 小搭子 |
|------|---------|--------|
| **数据读取** | ✅ 工具/Lobster 解析 Excel | ✅ Skill 解析 + scratchpad 存储 |
| **表单填写** | ✅ Lobster 逐字段操作 | ✅ peekaboo 逐字段操作 |
| **下拉框匹配** | LLM 自行判断（可能选错或跳过） | 截图 → LLM 语义匹配 → HITL 确认 |
| **第 3 条失败后** | ⚠️ 重试触发 compaction → 丢失"已填 2 条"记录 → 从头重填 | OperationRecord 记录已完成 → 仅重试第 3 条 |
| **数据重复风险** | ⚠️ 高（compaction 后不知道已提交了哪些） | ✅ 无（OperationRecord 去重） |
| **提交保护** | LLM 可能自行点提交 | HITL 强制确认 |
| **进度可见** | 无 | 每条 SSE 推送 |
| **批量撤销** | ❌ 手动逐条删除 | ✅ OperationRecord 逆操作 |

**量化指标**：

| 指标 | OpenClaw 预期 | 小搭子预期 |
|------|-------------|----------|
| 任务完成率 | ~70%（80+ 步中 compaction 风险高） | ~90% |
| 数据重复提交风险 | ⚠️ 有 | ✅ 无 |
| GUI 操作步数 | 80-100 步 | 80-100 步 |
| Token 消耗 | ~60K-100K（截图累积 + 可能重试全量） | ~40K-60K（scratchpad 卸载 + 仅重试失败条目） |
| 用户干预 | 0-3 次（重复/遗漏需手动处理） | 1 次（仅语义匹配确认） |

**技术验证点**：
- [ ] Excel 数据解析：5 条记录 + 8 个字段是否全部正确
- [ ] 表单字段定位：截图是否正确识别每个字段位置
- [ ] 下拉框操作：点击 → 展开 → 识别选项列表 → 选择目标项
- [ ] 语义匹配："办公用品"→"办公耗材"是否 HITL 确认
- [ ] OperationRecord：每条提交后是否记录（防重复提交）
- [ ] 进度追踪：每条完成后是否推送 SSE
- [ ] 填写验证：每条填完后截图对比是否正确
- [ ] 提交保护：最终提交必须 HITL 确认
- [ ] 错误恢复：第 3 条失败 → 不影响已填的 1、2 条 → 仅重试第 3 条

---

### E4. 长程 GUI 任务中途异常恢复（6 轮长程）

**测试目标**：验证长程 GUI 任务执行中遇到应用弹窗/卡死/崩溃时的**结构化恢复能力**。两端都有 computer_use，都能检测截图异常。差异在于：OpenClaw 恢复后可能因 compaction 丢失"已完成列表" → 重复操作或遗漏；小搭子用 OperationRecord 精确断点续做。

**场景故事**：

```
小搭子正在帮用户批量重命名照片文件夹里的 50 张照片。
处理到第 20 张时，系统弹出"磁盘空间不足"弹窗，遮挡了操作区域。

两个 Agent 都能识别弹窗并关闭它。关键差异在"关闭弹窗之后"——
OpenClaw：关闭弹窗 → 继续执行 → 但 compaction 可能已裁剪"已重命名 1-20"的记录
         → Agent 不确定从哪里继续 → 可能从头开始 → 前 20 个文件被重命名两次
小搭子：关闭弹窗 → 查询 OperationRecord → 确认已完成 20 个 → 精确从第 21 个继续
```

**测试数据**：`data/photos/`（50 张图片文件）

**用户输入**：
```
帮我把照片文件夹里的图片按拍摄日期重命名，格式是"YYYY-MM-DD_序号.jpg"
```

**对话流程**（6 轮）：

```
轮次1：用户下达任务
      → plan-todo：扫描文件 → 提取日期 → 批量重命名
      → ConsistencyManager.snapshot() 记录 50 个文件的原始名称

轮次2-3：Phase 1 — 正常执行
      → 打开 Finder → 导航到照片文件夹
      → 逐个重命名 → 第 1-20 个成功
      → OperationRecord 逐条记录（file_rename, old_name, new_name）
      → 进度："已重命名 10/50"、"已重命名 20/50"

轮次4：Phase 2 — 异常检测与恢复
      → 第 21 个操作时截图 → 检测到系统弹窗"磁盘空间不足"
      → 异常处理：
        1. 识别弹窗类型（系统警告，非致命）
        2. 点击"好"按钮关闭
        3. 截图验证 Finder 恢复正常
        4. ⭐ 查询 OperationRecord → 确认已完成 20 个 → 从第 21 个继续
      → 通知用户："遇到磁盘空间提示（已关闭），继续处理剩余 30 个"

轮次5：Phase 3 — 断点续做
      → 从第 21 个精确继续 → 全部完成
      → 进度："30/50"、"40/50"、"50/50 ✅"

轮次6：汇报
      → "已完成 50 张照片重命名 ✅
         ⚠️ 过程中遇到一次系统弹窗（已自动处理）
         如需恢复原始文件名，随时告诉我"
```

**预期对比**：

| 阶段 | OpenClaw | 小搭子 |
|------|---------|--------|
| **正常执行** | ✅ Lobster + exec 操作文件 | ✅ peekaboo + exec 操作文件 |
| **弹窗检测** | ✅ 截图识别弹窗 | ✅ 截图识别弹窗 |
| **关闭弹窗** | ✅ 点击关闭 | ✅ 点击关闭 |
| **恢复后定位** | ⚠️ 不确定从哪继续（compaction 可能丢失已完成列表） | ✅ 查询 OperationRecord → 精确第 21 个 |
| **重复操作风险** | ⚠️ 可能重命名已处理的文件（二次重命名 → 文件名错误） | ✅ 无（精确断点） |
| **Session 稳定性** | ⚠️ 长命令可能触发 session 损坏 ([#1577](https://github.com/clawdbot/clawdbot/issues/1577)) | ✅ 稳定（plan-todo + OperationRecord） |
| **回滚能力** | ❌ 无批量回滚 | ✅ snapshot → 一键恢复所有原始文件名 |

**异常场景矩阵**（两端对比）：

| 异常类型 | OpenClaw 恢复 | 小搭子恢复 |
|---------|-------------|-----------|
| 系统弹窗 | 截图识别 → 关闭 → 继续（但可能重复） | 截图识别 → 关闭 → 查 OperationRecord → 精确续做 |
| 应用无响应 | kill → 重启 → 从头开始 | kill → 重启 → 从 OperationRecord 断点续做 |
| 应用崩溃 | 重启 → 从头开始 | 重启 → 从 snapshot + OperationRecord 恢复 |
| 权限弹窗 | LLM 自行决定 | HITL → 用户确认授权 |
| 长时间无响应 | session 可能损坏 | AdaptiveTerminator 超时 → 保存状态 → 重试 |

**量化指标**：

| 指标 | OpenClaw 预期 | 小搭子预期 |
|------|-------------|----------|
| 弹窗检测 | ✅ 成功 | ✅ 成功 |
| 断点精确度 | ⚠️ ±5（可能重复或遗漏 1-5 个） | ✅ 精确（OperationRecord 逐条追踪） |
| 异常恢复后完成率 | ~80%（可能重复/遗漏） | ~95%（精确续做） |
| 回滚能力 | ❌ 需手动逐个恢复 | ✅ 一键全部恢复 |
| 异常处理耗时 | 5-10 秒 | 3-5 秒 |

**技术验证点**：
- [ ] `ConsistencyManager.snapshot()` 任务开始前保存 50 个原始文件名
- [ ] `OperationRecord` 逐条记录 `file_rename`（old_name → new_name）
- [ ] 弹窗检测：截图是否区分出系统弹窗
- [ ] 断点精确性：恢复后是否**精确**从第 21 个继续（不重复 1-20）
- [ ] 进度连续性：异常恢复后进度 20 → 21（不重置为 1）
- [ ] 回滚功能：用户说"恢复原样"→ 50 个文件全部恢复原名
- [ ] 二次重命名防护：已重命名的文件不被再次操作
- [ ] 异常日志：弹窗事件记录到审计日志

---

### E1-E4 核心差异总结

```
┌──────────────────────────────────────────────────────────────────────┐
│                                                                      │
│  维度 E 的核心洞察：                                                  │
│                                                                      │
│  GUI 能力本身不是差异化 — 两端都有 computer_use                       │
│                                                                      │
│  真正的差异在于：长程任务执行的「可靠性基础设施」                       │
│                                                                      │
│  ┌────────────────────────────────────────────────────────────────┐   │
│  │                                                                │   │
│  │  OpenClaw (Lobster)              小搭子 (peekaboo)              │   │
│  │  ────────────────                ─────────────────              │   │
│  │                                                                │   │
│  │  GUI 操作   ✅ 能做              ✅ 能做                        │   │
│  │  短任务     ✅ 表现好            ✅ 表现好                      │   │
│  │                                                                │   │
│  │  ---- 以下是长程任务（30-100 步）的分水岭 ----                  │   │
│  │                                                                │   │
│  │  上下文管理  ⚠️ compaction 丢数据  ✅ scratchpad 持久存储       │   │
│  │  错误恢复   ⚠️ ad-hoc 重试       ✅ ErrorClassifier 分类恢复   │   │
│  │  断点续做   ⚠️ 可能重复/遗漏     ✅ OperationRecord 精确       │   │
│  │  进度追踪   ❌ 无结构化进度       ✅ plan-todo + SSE 事件       │   │
│  │  回滚安全   ❌ 无                 ✅ snapshot + 逆操作           │   │
│  │  会话稳定   ⚠️ #1577 session 损坏 ✅ 稳定                      │   │
│  │  死循环风险  ⚠️ #1084 重试死循环  ✅ max_backtrack + 分类       │   │
│  │                                                                │   │
│  │  类比：                                                        │   │
│  │  OpenClaw = 会开车，但没有安全带、没有导航、没有行车记录仪       │   │
│  │  小搭子   = 会开车，而且有安全带、导航、行车记录仪、自动刹车     │   │
│  │                                                                │   │
│  │  路况好时（短任务）两者都能安全到达                              │   │
│  │  路况差时（长程复杂任务）安全设施决定成败                        │   │
│  │                                                                │   │
│  └────────────────────────────────────────────────────────────────┘   │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

---

## 维度 F：开发者体验（面向开源社区）

> **测试目标**：面向开源发布后的开发者群体，验证框架的可扩展性、多模型兼容性、配置灵活度和上下文工程质量。这些是挑剔的开发者决定是否 star/fork/贡献的关键考量。

### F1. 自定义 Skill 开发全流程（5 轮）

**测试目标**：验证开发者从零创建一个自定义 Skill 到被 Agent 正确调用的完整流程。对开源社区而言，Skill 生态的易扩展性直接决定社区活跃度。

**场景故事**：

```
开发者小张想为小搭子贡献一个"Markdown 目录生成"Skill。

他需要：
1. 创建 Skill 目录和 SKILL.md
2. 编写 scripts/run.py 实现逻辑
3. 注册到 skills.yaml
4. 验证 Agent 能正确发现并调用

关键验证：不需要改框架代码，纯配置 + Skill 文件就能扩展能力
```

**对话流程**：

```
轮次1：用户 "我有一份很长的 Markdown 文档，帮我生成一个目录"
      → 测试前置：instances/xiaodazi/skills/markdown-toc/ 已创建
      → SKILL.md 声明了 name: markdown-toc, description: "为 Markdown 文档生成目录"
      → skills.yaml 的 common.builtin 已注册此 Skill
      → 预期：IntentAnalyzer 识别 → Skill 被注入到 system prompt → Agent 调用

轮次2：Agent 调用 markdown-toc Skill → scripts/run.py 执行
      → 预期：正确解析 Markdown 标题层级 → 输出 TOC

轮次3：用户 "只保留二级和三级标题"
      → 预期：is_follow_up=true → 复用 plan_cache → 传递参数 max_depth=3, min_depth=2

轮次4：用户 "加上锚点链接"
      → 预期：追问继承 → Skill 参数调整 → 输出带 anchor 的 TOC

轮次5：验证 Skill 输出质量和格式正确性
```

**测试数据**：`data/long_markdown_doc.md`（200+ 行，含 h1-h4 多级标题）

**Skill 文件结构**：

```
instances/xiaodazi/skills/markdown-toc/
├── SKILL.md          # Skill 声明（name, description, input_schema）
├── scripts/
│   └── run.py        # 实现逻辑
└── README.md         # 开发者文档
```

**量化指标**：

| 指标 | 预期 |
|------|------|
| Skill 注册到可调用 | < 1 次重启（热加载优先） |
| Agent 首次发现 Skill | SkillLoader lazy 加载 metadata → SKILL.md |
| 意图匹配准确度 | "生成目录" → 命中 markdown-toc |
| 追问参数传递 | max_depth/min_depth 正确传入 run.py |

**技术验证点**：
- [ ] `SkillLoader` 是否从 `instances/xiaodazi/skills/` 发现新 Skill
- [ ] `SKILL.md` 的 `input_schema` 是否被正确解析为工具参数
- [ ] `skills.yaml` 注册后，IntentAnalyzer 的 skill_groups 是否包含新 Skill
- [ ] Agent 调用时 `scripts/run.py` 是否收到正确的 JSON 参数
- [ ] Skill 输出是否遵守 `max_context_output` 约束（≤ 3000 字符）
- [ ] 追问场景 `is_follow_up=true` 时参数是否正确累加（不覆盖之前的参数）

---

### F2. 多模型无缝切换（4 轮，跨 2 会话）

**测试目标**：验证在同一对话或跨会话中切换 LLM 模型（Claude ↔ GPT ↔ Qwen）时，上下文、记忆、工具调用的一致性。开源用户可能使用各种模型，框架不能绑定单一供应商。

**场景故事**：

```
开发者小王在本地部署了三个模型：
- Claude Sonnet（主力，质量高）
- GPT-4o（备用，某些场景更快）
- Qwen-Plus（中文场景偏好）

他希望：
- 默认用 Claude 处理复杂任务
- 中文写作切换到 Qwen
- Claude API 超额时自动降级到 GPT

关键验证：切换模型后对话上下文不丢失，工具定义格式自动适配
```

**前置条件**：`llm_profiles.yaml` 已配置三个 profile

**对话流程**：

```
=== 会话 1（Claude Sonnet）===
轮次1：用户 "帮我分析这份 CSV 的销售趋势"
      → 使用 claude-sonnet 处理 → excel-analyzer Skill 调用 → 输出分析

轮次2：用户 "换成 Qwen 帮我把这个分析结果写成中文报告"
      → 模型切换：claude-sonnet → qwen-plus
      → 关键验证：
        • 轮次1 的分析结果是否保留在上下文中（不丢失）
        • 工具定义格式是否从 Anthropic 格式适配为 OpenAI 兼容格式
        • system prompt 缓存是否正确失效并重建

=== 会话 2（模拟 Claude API 超额）===
轮次3：用户 "搜索最新的 LLM Agent 论文"
      → 模拟 Claude API 返回 429 (rate_limit_exceeded)
      → 预期：自动降级到 GPT-4o（llm_profiles.yaml 的 fallback 配置）
      → 降级后工具调用是否正常（paper-search Skill）

轮次4：用户 "把刚才的论文列表翻译成中文"
      → 预期：仍使用降级后的 GPT-4o
      → 上下文中轮次3 的结果是否保留
```

**预期对比（与单模型框架）**：

| 指标 | 单模型框架 | 小搭子 |
|------|----------|--------|
| 模型切换 | 需要重启/新会话 | 对话内热切换 |
| 上下文保留 | 切换后丢失 | 切换后保留（消息格式自动适配） |
| 工具格式适配 | 手动改配置 | `ClaudeClient` / `QwenClient` 自动适配 |
| API 降级 | 手动切换 | 自动 fallback + 通知用户 |
| 多模型记忆共享 | 不支持 | MEMORY.md 跨模型共享 |

**量化指标**：

| 指标 | 预期 |
|------|------|
| 模型切换耗时 | < 2s（客户端初始化） |
| 切换后上下文完整性 | 100%（消息不丢失） |
| 工具调用成功率 | 与单模型一致（> 95%） |
| 自动降级响应时间 | < 5s（检测 429 → fallback → 重试） |

**技术验证点**：
- [ ] `llm_profiles.yaml` 的 `fallback` 配置是否被正确解析
- [ ] 模型切换时 `messages` 格式是否自动转换（Anthropic ↔ OpenAI 格式）
- [ ] `system_prompt` 缓存是否在模型切换后正确失效
- [ ] 工具定义（`tools` 参数）是否适配目标模型的 API 格式
- [ ] API 429 错误是否触发自动降级（而非直接报错给用户）
- [ ] 降级后 `emit_model_switch()` SSE 是否通知前端显示当前模型
- [ ] MEMORY.md 记忆是否跨模型共享（不受模型切换影响）

---

### F3. 实例创建与配置扩展（3 轮）

**测试目标**：验证开发者从 `_template` 创建全新实例的完整流程，以及实例之间的存储隔离。这是开源框架的核心扩展点——不修改框架代码，通过实例配置创建完全不同的 Agent。

**场景故事**：

```
开发者想基于 ZenFlux 创建一个"代码审查搭子"实例，与 xiaodazi 共存但完全隔离。

他需要：
1. 从 _template 复制创建 instances/code-reviewer/
2. 配置 config.yaml（启用 code 相关 Skills）
3. 编写 prompt.md（代码审查专家人格）
4. 验证两个实例存储隔离（memory/db/knowledge 互不影响）
```

**操作流程**：

```
轮次1：创建实例
      → 复制 instances/_template/ → instances/code-reviewer/
      → 修改 config.yaml：
        - instance.name: "code-reviewer"
        - agent.model: "claude-sonnet-4-5-20250929"
        - skill_groups 添加 code 组
      → 编写 prompt.md："你是一个资深代码审查专家..."
      → 设置 AGENT_INSTANCE=code-reviewer
      → 启动服务

轮次2：验证实例功能
      → 用户 "审查这段 Python 代码的安全问题"（发送代码片段）
      → 预期：使用 code-reviewer 的 prompt.md 人格 + github Skill
      → 输出代码审查报告

轮次3：验证存储隔离
      → code-reviewer 的记忆写入 data/instances/code-reviewer/memory/
      → xiaodazi 的记忆在 data/instances/xiaodazi/memory/
      → 两个实例的 instance.db 完全独立
      → 切换回 AGENT_INSTANCE=xiaodazi → 不能看到 code-reviewer 的对话历史
```

**量化指标**：

| 指标 | 预期 |
|------|------|
| 实例创建到可用 | < 5 分钟（配置文件 + 重启） |
| 存储路径隔离 | `data/instances/{name}/` 独立 |
| Skill 配置独立 | 各实例 skills.yaml 独立 |
| 记忆隔离 | MEMORY.md 互不可见 |
| 环境变量驱动 | `AGENT_INSTANCE` 单一控制点 |

**技术验证点**：
- [ ] `instance_loader.py` 的 `create_agent_from_instance()` 是否正确加载新实例
- [ ] `get_instance_db_dir("code-reviewer")` 路径是否正确
- [ ] `get_instance_memory_dir("code-reviewer")` 路径是否正确
- [ ] 新实例的 `prompt.md` 是否注入到 system prompt
- [ ] 新实例的 `skills.yaml` 是否独立于 xiaodazi
- [ ] `main.py` 多实例检测是否提示用户选择
- [ ] 共享模型目录 `data/shared/models/` 是否跨实例可用

---

### F4. 上下文工程极限验证（6 轮）

**测试目标**：验证在 200K token 窗口下，上下文工程的各层机制（注入预算、工具压缩、历史折叠、scratchpad 卸载）是否协同工作，保证长对话不退化。这是框架核心技术壁垒的验证。

**场景故事**：

```
资深开发者用小搭子处理一个复杂项目：
- 上传 3 份大文件（总计 50KB+）
- 进行 6 轮深度对话
- 涉及搜索、分析、写作多种工具

关键验证：第 6 轮对话时，Agent 是否还能准确引用第 1 轮的关键信息
```

**测试数据**：
- `data/context_test/report_q1.txt`（15KB 季度报告）
- `data/context_test/report_q2.txt`（18KB 季度报告）
- `data/context_test/competitor_analysis.md`（20KB 竞品分析）

**对话流程**：

```
轮次1：用户上传 report_q1.txt + "分析 Q1 收入趋势"
      → 预期：15KB 文件 → scratchpad（上下文只放摘要 + 路径）
      → excel-analyzer Skill 分析 → 输出趋势（Q1 收入 ¥120 万，环比 +15%）

轮次2：用户上传 report_q2.txt + "对比 Q1 和 Q2"
      → 预期：Q2 数据同样走 scratchpad
      → Agent 从 scratchpad 加载 Q1 数据 + Q2 数据 → 对比分析

轮次3：用户上传 competitor_analysis.md + "结合竞品情况评估我们的市场地位"
      → 预期：20KB 竞品报告 → scratchpad（上下文只放前 500 字摘要）
      → Agent 综合 Q1+Q2+竞品 → 输出市场地位评估

轮次4：用户 "搜索一下行业最新动态"
      → web_search 工具调用 → 返回结果
      → 预期：搜索结果 top-5 摘要进上下文，完整 JSON 写 scratchpad

轮次5：用户 "综合以上所有信息，写一份 2000 字的战略分析报告"
      → ⭐ 关键验证：Agent 是否能引用轮次1-4 的全部数据
      → 预期：从 scratchpad 按需加载各轮数据 → 写出引用准确的报告
      → 报告中的 Q1 数据（¥120 万）必须与轮次1 一致

轮次6：用户 "检查一下报告中的数据是否与原始文件一致"
      → 预期：Agent 重新读取 scratchpad 中的原始数据 → 逐条比对 → 确认一致
```

**上下文管理验证矩阵**：

| 轮次 | 注入的数据 | 上下文策略 | 预期 token |
|------|-----------|-----------|----------|
| 1 | 15KB 文件 | scratchpad + 500 字摘要 | ~5K |
| 2 | 18KB 文件 + 轮次1 结论 | scratchpad + 摘要 + 轮次1 折叠为结论 | ~6K |
| 3 | 20KB 文件 + 轮次1-2 结论 | scratchpad + 摘要 + 轮次1-2 折叠 | ~7K |
| 4 | 搜索结果 + 轮次1-3 结论 | top-5 摘要 + 历史折叠 | ~7K |
| 5 | 写报告指令 + 全部结论 | scratchpad 按需加载 | ~8K |
| 6 | 验证指令 | scratchpad 引用 | ~6K |

**关键指标**：

| 指标 | 预期 |
|------|------|
| 上下文 token 增长 | 线性收敛（第 4 轮后 ≤ 8K），不线性膨胀 |
| 第 6 轮数据引用准确率 | 100%（Q1 ¥120 万必须精确） |
| 文件 > 2KB 走 scratchpad | 100%（3 份文件全部走 scratchpad） |
| 旧轮次 tool_result 折叠 | 第 3 轮前的工具结果折叠为一句话 |
| Injector token 预算 | 每个 Injector ≤ 声明的 MAX_TOKENS |

**技术验证点**：
- [ ] 15KB/18KB/20KB 文件是否写入 `scratchpad/{session_id}/`
- [ ] 上下文中文件引用是否为"摘要 + 路径"格式（非原文）
- [ ] `_compress_old_tool_results()` 是否在第 3 轮后压缩轮次1 的工具结果
- [ ] `_trim_messages_if_needed()` 是否保持 `first_n + last_m` 策略
- [ ] 轮次5 报告引用的数据与 scratchpad 中原始数据一致
- [ ] 搜索结果是否 top-5 摘要进上下文 + 完整 JSON 写 scratchpad
- [ ] 各 Injector（Memory/Playbook/Knowledge）是否遵守 token 预算
- [ ] 第 6 轮总 input token 是否 < 10K（上下文未膨胀）

---

## 维度 G：垂直场景深度验证

> **测试目标**：将产品规划中的 5 大爆点 Use Case 转化为可量化的端到端测试。这些场景是开源宣发时的核心 Demo，必须无瑕疵通过。每个用例都对应产品的一句核心金句。

### G1. 写稿搭子 — 多项目风格隔离（8 轮，跨 3 会话 + 2 项目）

**产品金句**："3 个月后，它比我自己更懂我的写作风格"

**测试目标**：验证多项目场景下风格记忆的**完全隔离** — 在"毒舌咖啡馆"项目学到的毒舌风格，不会污染"温暖读书会"项目的治愈风格。

**场景故事**：

```
小雨运营 2 个公众号，风格完全不同：
- 「毒舌咖啡馆」：毒舌犀利但有干货
- 「温暖读书会」：治愈温暖有感悟

她需要小搭子在两个项目间自由切换，每个项目"记住"各自的写作风格，
而不是把毒舌风格带到读书会文章里。
```

**测试数据**：
- `data/style_test/coffee_sample.txt`（毒舌风格范文 3 篇）
- `data/style_test/book_sample.txt`（治愈风格范文 3 篇）

**对话流程**：

```
=== 项目 A：毒舌咖啡馆 ===
轮次1：用户切换到项目"毒舌咖啡馆" + 上传 coffee_sample.txt
      → "学习一下这几篇文章的风格"
      → style-learner Skill 提取：毒舌、犀利、反转、有干货
      → 写入 projects/毒舌咖啡馆/MEMORY.md style 段落

轮次2：用户 "用这个风格写一篇关于奶茶的文章"
      → recall() 召回风格记忆 → 输出毒舌+干货风格的奶茶文章
      → 验证：文中有犀利吐槽 + 数据干货

=== 项目 B：温暖读书会 ===
轮次3：用户切换到项目"温暖读书会" + 上传 book_sample.txt
      → "学习一下这几篇文章的风格"
      → style-learner Skill 提取：温暖、治愈、有感悟、引发思考
      → 写入 projects/温暖读书会/MEMORY.md style 段落

轮次4：用户 "用这个风格写一篇《小王子》的读后感"
      → recall() 召回温暖读书会的风格 → 输出治愈+感悟风格
      → ⭐ 关键验证：不能出现毒舌/犀利/吐槽等毒舌咖啡馆的风格词

=== 切回项目 A ===
轮次5：用户切换回"毒舌咖啡馆"
      → 加载项目 A 的上下文和记忆
      → 用户 "写一篇关于健身房的文章"
      → ⭐ 关键验证：风格必须是毒舌+干货，不能变成温暖治愈

=== 新会话（项目 B）===
轮次6：新会话 + 切换到"温暖读书会"
      → 用户 "写一篇《追风筝的人》的读后感"
      → 预期：自动召回温暖风格（跨会话记忆生效）
      → 验证风格一致性

轮次7：用户 "小搭子你记住了什么关于我的写作偏好？"
      → 预期：分别列出两个项目的风格特征
      → "毒舌咖啡馆：犀利毒舌、反转、有数据干货"
      → "温暖读书会：温暖治愈、有感悟、引人深思"

轮次8：用户 "在温暖读书会项目里把风格改成幽默轻松"
      → 更新 projects/温暖读书会/MEMORY.md
      → QualityController 冲突检测："温暖治愈" → "幽默轻松" → 替换
```

**核心验证矩阵**：

| 检查项 | 毒舌咖啡馆 | 温暖读书会 |
|--------|-----------|-----------|
| 风格关键词 | 毒舌、犀利、反转、干货 | 温暖、治愈、感悟、引人深思 |
| 绝对不能出现 | 温暖、治愈、鸡汤 | 毒舌、犀利、吐槽 |
| MEMORY.md 路径 | `projects/毒舌咖啡馆/MEMORY.md` | `projects/温暖读书会/MEMORY.md` |
| 记忆召回范围 | 仅召回当前项目 | 仅召回当前项目 |

**量化指标**：

| 指标 | 预期 |
|------|------|
| 风格隔离准确率 | 100%（跨项目零污染） |
| 跨会话风格保持率 | > 90%（风格特征一致） |
| 项目切换耗时 | < 2s（加载记忆 + 切换上下文） |
| 记忆冲突处理 | QualityController 检测到 "温暖治愈" → "幽默轻松" 冲突 → 替换 |

**技术验证点**：
- [ ] 项目切换时 `recall()` 的搜索范围是否限定在当前 `project_id`
- [ ] `flush()` 写入的记忆路径是否包含 `project_id`
- [ ] 轮次4 的输出风格评分（LLM-as-Judge 判断是否治愈风格）
- [ ] 轮次5 的输出风格评分（LLM-as-Judge 判断是否毒舌风格）
- [ ] 项目 A 的 MEMORY.md 与项目 B 的 MEMORY.md 内容完全独立
- [ ] 风格更新后 `QualityController` 是否触发冲突检测并正确替换

---

### G2. 表格搭子 — 端到端数据管道（5 轮）

**产品金句**："以前我要教 AI 怎么处理错误，现在它自己就搞定了"

**测试目标**：验证从原始 Excel 到最终分析报告的全链路自动化，特别关注**数据清洗 → 分析 → 可视化 → 报告生成**的管道质量。核心差异点是错误自愈能力。

**场景故事**：

```
财务王姐每季度要做一次销售数据分析。
她手里的 Excel 是从各地区汇总来的，格式五花八门。

之前用 AI：
"分析这个表格" → "第 3 列数据类型不一致，无法处理" → 卡住
"那你清洗一下" → "清洗后行数从 200 变成 150，丢了 50 行" → 数据不对

现在用小搭子：
拖入 Excel → "帮我做季度销售分析，出报告"
→ 自动检测 6 种格式问题 → 自动修复（零丢行）
→ 分析趋势 → 生成图表描述 → 输出完整报告
全程零干预，王姐只需要看结果
```

**测试数据**：`data/pipeline_test/quarterly_sales_raw.xlsx`

**数据特征**（真实企业数据的典型问题）：
- 200 行数据，5 个地区
- 日期列：3 种格式混合（`2025-01-15` / `1月15日` / `01/15/2025`）
- 金额列：数字和带符号文本混合（`12345` / `¥12,345` / `12345.00元`）
- 第 45、89、156 行为空行
- 第 100-105 行"地区"列为合并单元格
- 第 200 行为"合计"文本行（非数据行）
- 有一列"备注"全部为空（噪音列）

**对话流程**：

```
轮次1：用户拖入文件 + "帮我做季度销售分析，出一份完整报告"
      → 意图分析：task_type=data_analysis, complexity=complex, needs_plan=true
      → plan-todo 生成 4 步：数据清洗 → 统计分析 → 趋势发现 → 报告输出

轮次2：Phase 1 — 数据清洗（核心差异点）
      → excel-analyzer Skill 首次尝试 → 可能因混合格式报错
      → ErrorClassifier → business_logic → PARAM_ADJUST 回溯
      → excel-fixer Skill 介入 → 自动修复：
        • 日期统一为 YYYY-MM-DD
        • 金额去符号转数字
        • 删除空行和合计行
        • 拆分合并单元格
        • 删除全空噪音列
      → ⭐ 关键验证：清洗后行数 = 195（仅去除 5 个空行，零数据丢失）

轮次3：Phase 2+3 — 统计分析 + 趋势发现
      → 输出：各地区销售额排名、月度环比增长率、同比对比
      → 异常检测：某地区某月异常下降 → 标注为异常值

轮次4：Phase 4 — 报告生成
      → elegant-reports Skill 生成 PDF 报告
      → 报告结构：摘要 → 数据总览 → 地区分析 → 趋势 → 建议
      → 数据准确性：报告中的合计数 === 原始数据（清洗后）的 sum

轮次5：用户 "分析得不错，但我想看看各地区的占比"
      → is_follow_up=true → 追加饼图描述 + 占比表格
      → 复用 plan_cache → 不重新清洗数据
```

**错误自愈流程详解**：

```
Agent 首次读取 Excel
    │
    ├─ 检测到混合日期格式 → ErrorClassifier: business_logic
    │   → BacktrackManager: PARAM_ADJUST
    │   → excel-fixer Skill: 统一日期格式
    │
    ├─ 检测到金额列含文本 → ErrorClassifier: business_logic
    │   → BacktrackManager: PARAM_ADJUST
    │   → excel-fixer Skill: 去符号、转数字
    │
    ├─ 检测到空行/合计行 → 自动过滤（不需要回溯）
    │
    └─ 数据清洗完成 → 195 行干净数据 → 继续分析
    
全程零用户干预，回溯次数 ≤ 3
```

**量化指标**：

| 指标 | 预期 |
|------|------|
| 数据清洗后完整性 | 195/200（仅去空行，零数据丢失） |
| 格式修复成功率 | 100%（6 种问题全部修复） |
| 回溯次数 | ≤ 3 |
| 用户干预次数 | 0 |
| 报告数据准确性 | 合计金额 === sum(清洗后数据) |
| 全流程耗时 | < 3 分钟 |

**技术验证点**：
- [ ] `excel-analyzer` Skill 是否正确调用（lightweight，自动安装 pandas+openpyxl）
- [ ] `excel-fixer` Skill 是否在回溯后自动介入（而非手动调用）
- [ ] `ErrorClassifier` 是否将格式问题分类为 `business_logic`（非 `infrastructure`）
- [ ] 数据清洗后行数校验：195 行（非 200，非 < 195）
- [ ] 日期列统一后是否全部为 `datetime` 类型
- [ ] 金额列统一后是否全部为 `float` 类型
- [ ] 报告 PDF 中的数字是否与清洗后数据完全一致
- [ ] 追问"占比"时是否复用 plan_cache（不重新清洗）

---

### G3. 研究搭子 — 文献管理全流程（6 轮）

**产品金句**："双击安装，5 分钟上手，不用折腾"

**测试目标**：验证研究场景的全链路能力 — 论文搜索 → PDF 解析 → 引用提取 → 文献综述 → 论文润色。关键是零配置体验和学术内容的专业性。

**场景故事**：

```
研三学生小陈正在写毕业论文的文献综述部分。
他需要：搜论文 → 读论文 → 整理引用 → 写综述 → 润色

之前用通用 AI："搜 5 篇 LLM Agent 论文" → 给了一堆不存在的论文（幻觉）
现在用小搭子：paper-search 调用 Semantic Scholar API → 真实论文 → PDF 解析 → 结构化引用
```

**测试数据**：`data/research_test/my_draft.md`（用户论文初稿，含参考文献格式需求）

**对话流程**：

```
轮次1：用户 "帮我搜索 5 篇关于 LLM Agent 记忆机制的最新论文（2024-2025 年）"
      → paper-search Skill 调用 Semantic Scholar API
      → 预期：返回 5 篇真实论文（含标题、作者、年份、摘要、链接）
      → ⭐ 关键验证：论文必须真实存在（非 LLM 幻觉）

轮次2：用户 "下载第 1 和第 3 篇的 PDF，帮我提取核心观点"
      → nano-pdf Skill 解析 PDF
      → 预期：提取每篇论文的研究问题、方法、核心发现
      → 大文件走 scratchpad，上下文只放摘要

轮次3：用户 "按 APA 格式整理这 5 篇论文的引用"
      → literature-reviewer Skill 处理
      → 预期输出：
        [1] Wang, X., et al. (2024). Memory Mechanisms in LLM-based Agents. ...
        [2] Zhang, Y., et al. (2025). Context Engineering for Autonomous Agents. ...
        ...
      → 验证：格式严格符合 APA 7th Edition

轮次4：用户 "基于这些论文，帮我写一段 800 字的文献综述"
      → 从 scratchpad 加载轮次1-3 的数据
      → writing-assistant Skill 生成学术风格综述
      → 预期：引用 [1][2][3] 标注清晰，不编造不存在的观点

轮次5：用户 "这是我的论文初稿（上传 my_draft.md），帮我润色摘要部分"
      → 学术风格润色
      → 保持原意不改变，仅优化表达
      → 术语一致性检查（同一概念不用不同表述）

轮次6：用户 "帮我检查初稿里有没有可能的引用遗漏"
      → 对比初稿提到的研究 vs 参考文献列表
      → 标注"第 3 段提到'记忆衰减机制'但未引用来源"
```

**量化指标**：

| 指标 | 预期 |
|------|------|
| 论文搜索真实性 | 100%（5/5 可在 Semantic Scholar 验证） |
| PDF 解析完整度 | > 90%（扫描页标注 "[需 OCR]"） |
| APA 格式准确率 | > 95%（作者、年份、标题、期刊全正确） |
| 文献综述引用准确 | 100%（引用内容可追溯到原文） |
| 润色后语义保真度 | > 95%（不改变原意） |

**技术验证点**：
- [ ] `paper-search` Skill 是否调用真实 API（Semantic Scholar / arXiv）
- [ ] 搜索结果是否包含 DOI/URL（可验证真实性）
- [ ] `nano-pdf` Skill 是否正确处理多页 PDF（扫描页标注）
- [ ] `literature-reviewer` Skill 输出的 APA 格式是否规范
- [ ] 文献综述中的引用标注 [1][2] 是否与参考文献列表对应
- [ ] 大 PDF 内容是否走 scratchpad（上下文只放摘要）
- [ ] 润色后术语是否一致（不出现 "memory" 和 "记忆" 混用）

---

### G4. 办公搭子 — 会议纪要到行动项全链路（5 轮）

**产品金句**："工具要你会用，搭子帮你干活"

**测试目标**：验证从原始会议录音/纪要到结构化行动项 → 日程安排 → 汇报邮件的完整办公自动化流程。体现"说一句它就干"的执行力。

**场景故事**：

```
产品经理老李刚开完一个 2 小时的需求评审会。
他只有一份粗糙的会议纪要文本，需要：
1. 提取所有行动项（谁、做什么、截止日期）
2. 生成一份结构化会议纪要
3. 草拟一封发给所有参会者的会后邮件

之前：手动整理 1.5 小时
现在用小搭子：5 分钟搞定
```

**测试数据**：`data/office_test/meeting_notes_raw.txt`

```
产品需求评审会 - 2026-02-10
参会人：老李（PM）、小张（前端）、小王（后端）、小陈（设计）

老李说新版本要加个搜索功能，小张说前端大概需要两周，
小王说后端API可以先出mock，小陈说搜索结果页面设计要和首页风格统一。
后来讨论了一下排期，决定2月底前完成技术方案评审。
小王提了个性能问题，说搜索量大的话可能要加缓存，
老李让小王出一份缓存方案，下周三前给他。
还讨论了首页改版的事，小陈下周出新设计稿。
```

**对话流程**：

```
轮次1：用户上传 meeting_notes_raw.txt + "帮我整理会议纪要并提取行动项"
      → meeting-insights-analyzer Skill 分析
      → meeting-notes-to-action-items Skill 提取
      → 输出结构化纪要：
        ## 会议纪要
        **主题**：产品需求评审 — 搜索功能
        **日期**：2026-02-10
        **参会人**：老李、小张、小王、小陈

        ### 讨论要点
        1. 搜索功能开发 — 前端约 2 周
        2. 后端先出 Mock API
        3. 搜索结果页设计需与首页统一
        4. 性能考量：大流量场景需缓存方案

        ### 行动项
        | 责任人 | 行动项 | 截止日期 |
        |--------|--------|----------|
        | 小王 | 出缓存方案 | 2026-02-17（下周三）|
        | 小陈 | 首页改版设计稿 | 2026-02-17（下周）|
        | 全体 | 技术方案评审 | 2026-02-28 |

轮次2：用户 "把行动项添加到我的日历，每项到期前 2 天提醒"
      → apple-calendar Skill（macOS）或等效日历操作
      → 创建 3 个日历事件 + 提醒
      → OperationRecord 记录（支持回滚）

轮次3：用户 "帮我草拟一封会后邮件，发给所有参会者"
      → smart-email-assistant Skill
      → 预期输出：
        主题：【会议纪要】产品需求评审 — 搜索功能（2026-02-10）
        正文：包含讨论要点摘要 + 行动项表格 + 下次会议时间
      → 语气正式但不生硬

轮次4：用户 "邮件结尾加上'如有异议请在周五前反馈'，然后把纪要保存到桌面"
      → 追问处理：修改邮件 + 保存纪要为 Markdown 文件
      → is_follow_up=true → 不重新分析会议内容

轮次5：用户 "做得很好，以后我的会议纪要都按这个格式整理"
      → flush() → 提取偏好：会议纪要格式偏好（表格行动项、日历提醒、会后邮件）
      → 写入 MEMORY.md → 下次自动应用
```

**量化指标**：

| 指标 | 预期 |
|------|------|
| 行动项提取完整性 | 3/3（零遗漏） |
| 责任人匹配准确率 | 100% |
| 截止日期推算 | "下周三" → 2026-02-17（根据会议日期推算） |
| 邮件专业度评分 | ≥ 4/5（LLM-as-Judge） |
| 全流程用户干预 | ≤ 1 次（仅修改邮件结尾） |

**技术验证点**：
- [ ] `meeting-insights-analyzer` 是否从非结构化文本正确提取讨论要点
- [ ] `meeting-notes-to-action-items` 是否提取了全部 3 个行动项
- [ ] "下周三"是否根据会议日期（2026-02-10 周二）推算为 2026-02-17
- [ ] 日历事件是否正确创建（标题、日期、提醒）
- [ ] 邮件格式是否专业（主题含日期、正文含行动项表格）
- [ ] 轮次5 偏好是否写入 MEMORY.md（下次可自动应用）

---

### G5. 隐私搭子 — 全离线敏感文档处理（4 轮）

**产品金句**："需要的时候，数据可以完全不出电脑"

**测试目标**：验证在**完全断网环境**下，小搭子仍能完成文档分析、知识检索、文件处理等核心任务。这是"数据主权"卖点的终极验证。

**场景故事**：

```
张律师处理一份保密并购合同，绝对不能上传到云端。
他需要小搭子在断网模式下：
1. 分析合同中的关键条款
2. 对照之前的合同模板找差异
3. 标注风险点

之前用云端 AI：数据必须上传 → 客户不允许 → 无法使用
现在用小搭子（离线模式）：本地模型 + 本地搜索 → 数据零外传
```

**前置条件**：
- 配置 `llm_profiles.yaml` 使用本地模型（Ollama / llama.cpp）
- `knowledge.directories` 已索引 `~/Documents/合同模板/`
- **断开网络连接**

**对话流程**：

```
轮次1：用户上传合同 PDF + "分析这份合同的关键条款和风险点"
      → 断网环境验证：LLM 使用本地模型（非云端 API）
      → nano-pdf Skill 解析 PDF → 本地处理
      → 预期：提取关键条款（金额、期限、违约责任、竞业条款）

轮次2：用户 "找一下我之前的合同模板，对比一下有什么不同"
      → knowledge_search → FTS5 本地全文搜索（零网络调用）
      → 预期：从 ~/Documents/合同模板/ 找到最相关的模板
      → 输出差异对比表

轮次3：用户 "标注风险点，特别关注违约金条款"
      → 本地模型分析 → 标注 3-5 个风险点
      → 每个风险点：条款位置 + 风险描述 + 建议

轮次4：用户 "生成一份审核意见书保存到桌面"
      → word-processor Skill 生成 Word 文档
      → 文档结构：合同概述 → 关键条款 → 风险分析 → 审核意见
      → 保存到桌面
```

**离线能力验证矩阵**：

| 功能 | 在线模式 | 离线模式 | 预期 |
|------|---------|---------|------|
| LLM 推理 | Claude API | Ollama 本地 | 可用（质量略降） |
| 文件解析 | nano-pdf | nano-pdf（纯本地） | 完全可用 |
| 知识搜索 | FTS5 + 向量（GGUF） | FTS5 + 向量（GGUF） | 完全可用 |
| 文件生成 | python-docx | python-docx（纯本地） | 完全可用 |
| 网络请求 | 正常 | 全部拒绝 | 零外传 |

**量化指标**：

| 指标 | 预期 |
|------|------|
| 网络请求数 | **0**（全流程零网络调用） |
| 关键条款提取完整度 | > 80%（本地模型质量略低） |
| 知识搜索响应时间 | < 50ms（FTS5 + GGUF 本地） |
| 文件生成成功率 | 100%（纯本地工具链） |
| 数据外传风险 | **零**（网络隔离验证） |

**技术验证点**：
- [ ] 断网环境下 LLM 客户端是否正确切换到本地模型
- [ ] `nano-pdf` Skill 是否纯本地执行（不调用云端 OCR）
- [ ] `knowledge_search` 是否使用 FTS5（零网络依赖）
- [ ] 向量搜索是否使用本地 GGUF 模型（非 OpenAI embedding API）
- [ ] 全流程是否有网络请求（`tcpdump` / `lsof -i` 验证）
- [ ] `word-processor` Skill 是否纯本地生成 Word
- [ ] 本地模型推理质量是否达到基本可用（条款提取 > 80%）

---

## 维度 H：产品健壮性（面向挑剔用户）

> **测试目标**：验证产品在边界条件、异常场景、资源压力下的表现。挑剔的用户（尤其开发者）会主动尝试打破系统边界，优雅的降级和清晰的错误信息比崩溃更重要。

### H1. LLM 服务降级与恢复（5 轮）

**测试目标**：验证主力 LLM 不可用时的降级策略 — 自动切换备选模型 → 通知用户 → 主力恢复后自动切回。核心验证：降级过程不丢失上下文，不中断任务。

**场景设计**：

```
轮次1：正常（Claude Sonnet）
      → 用户 "帮我分析这份数据" → 正常响应

轮次2：正常（Claude Sonnet）
      → 用户 "接着帮我写个总结" → 正常响应

轮次3：模拟 Claude API 故障（注入 500 错误）
      → 用户 "把总结翻译成英文"
      → 预期：
        1. 检测到 API 错误（500 Internal Server Error）
        2. 自动切换到 fallback 模型（GPT-4o / Qwen）
        3. SSE 通知用户："主模型暂时不可用，已切换到备选模型"
        4. 继续处理翻译请求（不中断）
        5. 轮次1-2 的上下文保留（翻译的"总结"内容与轮次2 一致）

轮次4：模拟 Claude API 恢复
      → 用户 "再帮我改一下格式"
      → 预期：自动检测主力恢复 → 切回 Claude → 通知用户

轮次5：主力模型也不可用，备选也不可用（极端场景）
      → 预期：
        1. 友好错误提示（非 stack trace）
        2. 建议用户检查 API Key / 网络连接
        3. 如果配置了本地模型 → 自动降级到本地
        4. 对话历史不丢失（可恢复）
```

**降级链路**：

```
Claude Sonnet (主力)
    │ 故障
    ▼
GPT-4o (备选 1)
    │ 故障
    ▼
Qwen-Plus (备选 2)
    │ 故障
    ▼
Ollama 本地 (最终兜底)
    │ 未配置
    ▼
友好错误提示 + 建议
```

**量化指标**：

| 指标 | 预期 |
|------|------|
| 降级检测时间 | < 5s（首次 API 错误 → 触发切换） |
| 降级后上下文完整性 | 100%（消息不丢失） |
| 降级通知 | SSE 事件 + 前端 toast |
| 恢复检测 | 下次请求自动探测主力是否恢复 |
| 极端降级友好度 | 用户看到清晰指引，非 traceback |

**技术验证点**：
- [ ] `llm_profiles.yaml` 的 `fallback` 链是否正确解析
- [ ] API 500/429/timeout 是否触发降级（而非直接报错）
- [ ] 降级后 `messages` 格式是否自动适配新模型
- [ ] `emit_model_switch()` SSE 是否推送到前端
- [ ] 主力恢复后是否自动切回（probe 机制）
- [ ] 所有模型不可用时是否输出友好错误（含排查建议）
- [ ] 降级期间对话历史是否完整持久化

---

### H2. 大文件与长上下文压力测试（4 轮）

**测试目标**：验证极端文件大小和对话长度下的系统稳定性。挑剔的开发者会尝试上传超大文件、进行超长对话，验证系统是否优雅处理而非崩溃。

**场景设计**：

```
轮次1：上传 100KB 文本文件
      → 预期：超过 50KB 阈值 → 仅文件路径 + 元数据进上下文
      → 用户 "总结这个文件的前 5 个要点"
      → Agent 使用 file-manager 按需读取片段 → 不将 100KB 全部加载到上下文

轮次2：上传 500KB CSV 文件
      → 预期：excel-analyzer Skill 处理
      → 文件不进入上下文 → scratchpad 存储
      → 输出统计摘要（行数、列数、数据类型、前 5 行预览）

轮次3：模拟第 25 轮对话（注入 24 轮历史消息）
      → 预期：_trim_messages_if_needed() 触发
      → first_n=4 + last_m=8 保留策略
      → 中间消息折叠为摘要
      → 总 input token ≤ 10K（不因历史膨胀）

轮次4：用户 "回顾一下我们之前讨论的要点"
      → 预期：Agent 从折叠的历史摘要中提取要点
      → 输出不遗漏关键结论（虽然原始消息已折叠）
```

**文件大小分级验证**：

| 文件大小 | 预期处理方式 | 上下文占用 |
|---------|------------|----------|
| 1KB | 直接放入上下文 | ~250 tokens |
| 5KB | 前 500 字 + 尾 200 字 + scratchpad 路径 | ~200 tokens |
| 50KB | LLM 摘要（≤ 500 字）+ scratchpad 路径 | ~150 tokens |
| 100KB | 仅路径 + 元数据（大小/类型/行数） | ~50 tokens |
| 500KB | 仅路径 + 元数据 + 警告"文件较大" | ~50 tokens |

**技术验证点**：
- [ ] 100KB 文件是否仅以路径形式出现在上下文中
- [ ] 500KB CSV 是否走 scratchpad（不进入消息）
- [ ] `_trim_messages_if_needed()` 是否在第 25 轮触发
- [ ] 折叠后的历史摘要是否保留关键结论
- [ ] 内存占用是否稳定（不因大文件/长对话线性增长）
- [ ] `tool_result` 超过 3000 字符的是否触发压缩

---

### H3. 中英文混合与多语言场景（4 轮）

**测试目标**：面向国际化开源社区，验证中英文混合指令、跨语言搜索、多语言文件处理的稳定性。中国开发者习惯中英混用，全球用户需要英文支持。

**场景设计**：

```
轮次1：中英混合指令
      → 用户 "帮我把这份 report 里的 key findings 整理成中文要点"
      → 预期：正确理解"report"="报告"、"key findings"="核心发现"
      → 输出中文要点

轮次2：英文指令处理中文内容
      → 用户 "Summarize this Chinese document in English"（附中文文件）
      → 预期：读取中文内容 → 输出英文摘要
      → translator Skill 协助 or LLM 直接处理

轮次3：跨语言知识检索
      → 用户 "find documents about 性能优化"（中英混合查询）
      → knowledge_search → FTS5 处理中文分词 + 向量处理语义
      → 预期：同时匹配含"性能优化"和"performance optimization"的文档

轮次4：多语言文件批处理
      → 用户 "把这三个文件都翻译成英文"（1 中文、1 日文、1 混合）
      → translator Skill 批量处理
      → 预期：正确识别每个文件的源语言 → 输出英文
```

**量化指标**：

| 指标 | 预期 |
|------|------|
| 中英混合指令理解率 | > 95% |
| 跨语言检索召回率 | > 80%（中英同义词匹配） |
| 翻译输出质量 | ≥ 4/5（LLM-as-Judge） |
| 多语言文件识别 | 100%（正确识别中/英/日） |

**技术验证点**：
- [ ] `IntentAnalyzer` 是否正确处理中英混合输入
- [ ] FTS5 的 `tokenizer` 是否支持中文分词
- [ ] 向量搜索是否支持跨语言语义匹配（BGE-M3）
- [ ] `translator` Skill 是否正确识别源语言
- [ ] 输出语言是否与用户指令一致（中文指令 → 中文输出）

---

### H4. 并发多任务与中断恢复（5 轮）

**测试目标**：验证用户在 Agent 执行长任务时发送新指令的处理策略，以及任务被中断后的状态恢复能力。现实中用户不会乖乖等任务完成才说话。

**场景故事**：

```
小搭子正在帮用户整理 50 个文件，大概需要 3 分钟。
整理到一半时，用户突然说"等等，先帮我查个天气"。

期望行为：
1. 暂停文件整理（记住已完成 25 个）
2. 快速回答天气查询
3. 自动恢复文件整理（从第 26 个继续，不重复）
```

**对话流程**：

```
轮次1：用户 "帮我把下载文件夹里的文件按类型分类到子文件夹"
      → plan-todo 生成 → 开始执行 → 逐个移动文件
      → OperationRecord 逐条记录

轮次2（执行中）：用户 "等一下，今天天气怎么样？"
      → 预期：
        1. 检测到新意图（天气查询，与当前任务无关）
        2. 暂停文件整理任务（记录断点：已完成 N 个）
        3. 处理天气查询 → 返回天气信息
        4. 提示："天气查询完成。文件整理已暂停在第 N 个，是否继续？"

轮次3：用户 "继续整理"
      → 从 OperationRecord 获取断点 → 从第 N+1 个继续
      → ⭐ 关键验证：不重复已整理的文件

轮次4（执行中）：模拟进程异常退出（kill -9）
      → 预期：
        1. 快照已持久化到磁盘
        2. OperationRecord 已记录已完成的操作
        3. 重启后可从断点恢复

轮次5：重启服务后
      → 用户 "刚才的文件整理完成了吗？"
      → 预期：
        1. 检测到未完成的任务（快照 + OperationRecord）
        2. 报告："上次整理到第 M 个，还有 N 个未完成。是否继续？"
        3. 用户确认后从断点续做
```

**量化指标**：

| 指标 | 预期 |
|------|------|
| 任务暂停响应时间 | < 2s（检测新意图 → 暂停当前） |
| 断点精确度 | 100%（OperationRecord 逐条追踪） |
| 中断后恢复成功率 | > 95% |
| 进程崩溃后恢复率 | > 90%（依赖磁盘持久化快照） |
| 文件重复操作 | 0 次 |

**技术验证点**：
- [ ] `IntentAnalyzer` 是否识别"天气查询"为新意图（非追问）
- [ ] 任务暂停时 plan-todo 状态是否保存（in_progress 标记）
- [ ] OperationRecord 的断点是否精确（第 N 个文件）
- [ ] 恢复后是否从第 N+1 个开始（不重复）
- [ ] 进程崩溃后快照文件是否存在于 `snapshots/` 目录
- [ ] `_load_snapshot_from_disk()` 是否正确反序列化
- [ ] 恢复后 plan-todo 的已完成项是否标记为 completed

---

## 维度 I：RVR-B 回溯机制验证

> **测试目标**：用精心构造的"毒药数据"可控触发回溯机制，验证 ErrorClassifier 错误分类、BacktrackManager 策略选择、回溯恢复质量、以及死循环防护是否真正有效。这是"出错自己修"核心卖点的硬验证——不是看它能不能跑完，而是看它**遇到错误后能不能自己修好**。
>
> **验证体系**：每个用例同时使用两层验证：
> 1. **Code Grader** `check_backtrack_occurred(min_count=N)` — 硬门槛：回溯必须发生
> 2. **LLM Rubric** `grade_backtrack_quality` — 软评估：回溯策略是否合理、结果是否改善

### I1. PARAM_ADJUST 回溯 — 毒药 CSV 格式修复（单轮）

**测试目标**：验证 Agent 处理格式损坏数据时，能通过调整参数（encoding/dtype）自动修复。

**毒药数据**：`data/backtrack_test/poison_csv.csv`

**数据特征**（精心设计的陷阱）：
- 前 10 行正常（UTF-8 编码，数字金额）
- 第 20 行：日期列值为 `N/A`（非法日期）
- 第 25 行：金额列值为 `ERR#REF!`（Excel 错误值混入 CSV）
- 30 行数据，3 处毒药

**用户输入**：
```
帮我分析这个 CSV 文件的销售趋势
```

**预期回溯链**：
```
Agent 首次读取 CSV
    → pandas 在 N/A 行或 ERR#REF! 行报错（类型转换失败）
    → ErrorClassifier: PARAMETER_ERROR (参数/类型问题)
    → BacktrackManager: PARAM_ADJUST
    → Agent 调整参数（errors='coerce' / 显式 dtype / 跳过错误行）
    → 重试成功 → 输出分析结果
```

**Graders**：
- `check_backtrack_occurred(min_count=1)` — 必须至少 1 次回溯
- `grade_backtrack_quality` — 策略应为 PARAM_ADJUST，回溯后结果应改善

**量化指标**：

| 指标 | 预期 |
|------|------|
| 回溯次数 | 1-2 |
| 回溯策略 | PARAM_ADJUST |
| 最终任务完成 | 是（28/30 正常行被分析） |
| 毒药行处理 | 跳过/标注/清洗（不报错停止） |

---

### I2. TOOL_REPLACE 回溯 — 扫描件内容提取（单轮）

**测试目标**：验证 Agent 遇到工具无法处理的内容时，能切换到替代方案。

**毒药数据**：`data/backtrack_test/scanned_report.txt`

**数据特征**：
- 第 1-3 页：正常文本（财务数据、表格）
- 第 4 页：`[SCANNED IMAGE - NO TEXT AVAILABLE]`（模拟扫描件无文字）
- 第 5 页：正常文本

**用户输入**：
```
帮我提取这份报告的所有关键数据
```

**预期回溯链**：
```
Agent 尝试读取全部内容
    → 发现第 4 页无有效文字
    → ErrorClassifier: TOOL_MISMATCH (当前工具不适用)
    → BacktrackManager: TOOL_REPLACE
    → Agent 选择替代方案：跳过第 4 页 + 标注"[扫描件，需 OCR]"
    → 输出第 1-3、5 页的数据
```

**Graders**：
- `check_backtrack_occurred(min_count=1)`
- `grade_backtrack_quality` — 是否尝试了替代方案（非"我做不到"）

---

### I3. PLAN_REPLAN 回溯 — 多步骤连锁失败（单轮）

**测试目标**：验证多步骤任务中间步骤失败时，Agent 能重新规划而非卡死。

**毒药数据**：`data/backtrack_test/multi_step_data.xlsx`

**数据特征**：
- 数据本身有效（12 个月的销售数据）
- 列名使用日文假名：`売上高（万円）`、`利益率（％）`、`顧客数`
- Step 1（读取）正常，Step 2（图表）可能因字体/编码失败

**用户输入**：
```
把这个数据做成图表，然后导出 PDF 报告
```

**预期回溯链**：
```
Step 1: 读取 Excel → 成功
Step 2: matplotlib 图表生成 → 可能因日文特殊字符失败
    → ErrorClassifier: EXECUTION_LOGIC_ERROR
    → BacktrackManager: PARAM_ADJUST（先尝试换字体）
    → 如果仍失败 → 升级为 PLAN_REPLAN
    → 重新规划：改用纯文本报告 / ASCII 安全标签 / 跳过图表
```

**Graders**：
- `check_backtrack_occurred(min_count=1)`
- `grade_backtrack_quality` — 验证策略升级链是否合理

---

### I4. 回溯升级链压测 — 3 层递进失败（单轮）

**测试目标**：验证回溯升级链（PARAM_ADJUST → TOOL_REPLACE → PLAN_REPLAN）和死循环防护。

**毒药数据**：`data/backtrack_test/triple_poison.json`

**数据特征**（3 层毒药）：
- `database.connections`：50 层深度嵌套（可能触发 RecursionError）
- `api.endpoints`：值中含 Unicode 控制字符 `\x00`、`\x01`
- `logging.format`：自引用 `${logging.format}`

**用户输入**：
```
帮我解析这个 JSON 配置文件，列出所有配置项及其值
```

**预期回溯链**：
```
尝试 1: 标准 json.loads → 深度嵌套或控制字符可能报错
    → PARAM_ADJUST（调整 max_depth / 忽略控制字符）
尝试 2: 仍有问题 → TOOL_REPLACE（换解析方法）
尝试 3: 仍有问题 → PLAN_REPLAN（先清洗 JSON 再解析）
最终: 列出可解析的配置项 + 标注无法解析的部分
```

**Graders**：
- `check_backtrack_occurred(min_count=2)` — 至少 2 次回溯（升级链）
- `grade_backtrack_quality` — 验证没有死循环，failed_tools 被记录

**量化指标**：

| 指标 | 预期 |
|------|------|
| 回溯次数 | 2-3（升级链） |
| failed_tools 记录 | 非空（记录了失败的方法） |
| 死循环防护 | 回溯不超过 max_backtracks(5) |
| 最终结果 | 列出可读配置项 或 优雅失败（说明哪些不可解析） |

---

### I1-I4 核心差异总结

```
传统 AI 遇到毒药数据：
  → 报错停止："解析失败，请检查文件格式"
  → 用户自己修 → 重新提交 → 可能再报错

小搭子（RVR-B）遇到毒药数据：
  → 检测到错误 → 分类错误类型
  → 选择恢复策略（PARAM_ADJUST / TOOL_REPLACE / PLAN_REPLAN）
  → 自动重试 → 如果失败升级策略
  → 最终完成任务 或 优雅告知用户具体原因 + 已尝试的修复方案

这就是"出错自己修"的真正验证——不是看能不能跑完，
是看遇到错误时回溯机制是否 (1) 触发了 (2) 选对了策略 (3) 改善了结果。
```

---

## 维度 P：Playbook 在线学习（全生命周期验证）

> **测试目标**：验证 Playbook 在线学习机制的完整 CRUD 生命周期——从策略自动提取、持久化、用户确认、语义索引匹配、上下文注入、相似任务执行，到拒绝隔离和删除清理。这是"越用越聪明"核心卖点的硬验证。

### P1 — 策略提取 + 持久化

| 项 | 内容 |
|----|------|
| **维度** | 提取触发 + JSON 落盘 + Mem0 索引 + WebSocket 推送 |
| **Query** | 帮我分析这份产品反馈数据，找出用户最不满意的 3 个方面 |
| **Data** | `playbook_test/product_feedback.xlsx`（150 行，5 类问题，界面卡顿占 35%） |
| **Expected** | 策略提取触发、DRAFT 状态、JSON 持久化、tool_sequence 非空 |
| **Grader 铁律** | 有工具调用但未提取 → <= 2 分；tool_sequence 为空 → <= 2 分；JSON 未落盘 → <= 2 分 |

**技术验证点**：
1. `playbook_extraction` 后台任务在 `message_stop` 后触发（fire-and-forget）
2. 提取前置条件：助手回复 >= 100 字符、用户消息 >= 10 字符、工具调用 >= 1 次
3. PlaybookEntry 的 `source_session_id` 非空（会话可溯源）
4. `data/instances/xiaodazi/playbooks/{id}.json` 文件存在
5. `index.json` 包含新 ID

### P2 — 确认 + 索引匹配 + 策略注入

| 项 | 内容 |
|----|------|
| **维度** | approve 确认 → Mem0 语义匹配 → `<playbook_hint>` 注入 → 相似执行 |
| **前置** | 预先创建 APPROVED 数据分析类策略 |
| **Query** | 分析这份客户满意度调查数据，找出满意度最低的服务类别 |
| **Data** | `playbook_test/customer_survey.xlsx`（200 行，6 类服务，售后服务最低） |
| **Expected** | PlaybookHintInjector 触发、hint confidence >= 0.3、正确识别售后服务 |
| **Grader 铁律** | 有 APPROVED 策略但未注入 → <= 2 分；语义相似但 score < 0.3 → <= 2 分 |

**技术验证点**：
1. 两层匹配：task_type 预筛（<1ms）+ Mem0 语义搜索（score >= 0.3）
2. "产品反馈分析"与"客户满意度分析"应被 Mem0 判为语义相似
3. `<playbook_hint>` 注入到 Phase 2 用户上下文
4. Agent 工具调用序列与策略 tool_sequence 有交集
5. 策略是建议非强制：Agent 可选择不完全遵循

### P3 — 完整闭环（跨会话）

| 项 | 内容 |
|----|------|
| **维度** | 提取 → 确认 → 新会话注入 → 相似执行（端到端闭环） |
| **流程** | 会话 1 分析反馈 → 自动提取 → API approve → 会话 2 分析满意度 → 策略注入 |
| **Data** | 会话 1 用 product_feedback.xlsx，会话 2 用 customer_survey.xlsx |
| **Expected** | 8 项验证：会话 1 正确、提取成功、approve 成功、持久化、会话 2 注入、工具相似、结果正确、usage_count 递增 |
| **Grader 铁律** | 跨会话策略未注入 → <= 2 分（闭环断裂）；会话 2 质量显著低于会话 1 → <= 2 分 |

**技术验证点**：
1. 跨会话状态传递：会话 1 的学习结果持久化后，会话 2 能读取
2. 状态机：DRAFT → submit_for_review → PENDING_REVIEW → approve → APPROVED
3. WebSocket `playbook_suggestion` 事件先于 API 调用
4. Mem0 sync 延迟：创建后需等待向量索引完成才能被搜索到
5. `record_usage()` 在注入后被调用，usage_count 递增

### P4 — 拒绝流程验证

| 项 | 内容 |
|----|------|
| **维度** | reject → 状态变更 → 后续不注入（负向隔离） |
| **场景** | Agent 学了个不好的策略，用户拒绝后不希望它再出现 |
| **流程** | 会话 1 完成 → 提取 → API reject → 会话 2 相似任务 → 验证无注入 |
| **Expected** | REJECTED 状态、会话 2 无 hint、approved 列表为空、任务仍能完成 |
| **Grader 铁律** | 被拒绝策略仍注入 → <= 2 分（拒绝机制失效） |

**技术验证点**：
1. `find_matching_async(only_approved=True)` 应过滤掉 REJECTED 状态的策略
2. 拒绝后 Agent 不受已拒策略影响，正常完成任务
3. `GET /api/v1/playbook?status=approved` 返回空列表

### P5 — 删除 + 清理验证

| 项 | 内容 |
|----|------|
| **维度** | DELETE → JSON 清除 → index 清除 → 后续不命中 |
| **场景** | 用户删除策略后，彻底从系统中移除 |
| **流程** | 预置 APPROVED 策略 → DELETE API → 验证文件删除 → 新查询无注入 |
| **Expected** | JSON 文件删除、index.json 清理、API 列表不含、后续无 hint |
| **Grader 铁律** | 删除后 JSON 仍存在 → <= 2 分；删除后仍注入 → <= 2 分 |

**技术验证点**：
1. `manager.delete()` 删除内存缓存 + JSON 文件 + index.json 更新
2. **已知风险**：`delete()` 未调用 Mem0 向量清理，P5 可能暴露此 bug
3. 如果 Mem0 残留导致删除后仍匹配，需修复 `delete()` 增加 `_remove_from_mem0()`

### P1-P5 核心差异总结

```
传统 AI 助手：
  → 每次对话从零开始，不记得上次怎么做的
  → 用户每次都要重复解释偏好和流程

Playbook 在线学习（小搭子）：
  → 成功完成任务 → 自动提取策略模式
  → 用户确认"记住" → 持久化为可复用策略
  → 下次相似任务 → 语义匹配 → 自动参考历史经验
  → 不好的策略可以拒绝/删除，不会再出现
  → 越用越聪明，但用户始终有控制权

这就是"在线学习"的真正验证——不是看能不能记住，
是看 (1) 能不能自动学 (2) 学了有没有用 (3) 学错了能不能改。
```

---

## 测试结果汇总模板

### 总体对比

| 维度 | 场景数 | 小搭子胜 | clawdbot胜 | 对等 |
|------|-------|---------|-----------|------|
| A. 效果 | 3 | | | |
| B. 特色 | 10 | | | |
| C. Token | 3 | | | |
| D. 场景 | 6 | | | |
| E. 电脑操作 | 4 | | | |
| F. 开发者体验 | 4 | | | |
| G. 垂直场景 | 5 | | | |
| H. 产品健壮性 | 4 | | | |
| I. RVR-B 回溯 | 4 | | | |
| P. Playbook 学习 | 5 | | | |

### Token 消耗实测

| 用例 | clawdbot 实测 | 小搭子 实测 | 节省比例 |
|------|-------------|----------|---------|
| C1 简单问答 | | | |
| C2 10轮对话 | | | |
| C3 复杂工具调用 | | | |
| A1 Excel 分析 | | | |
| F4 上下文极限 | | | |

### 月度成本估算

| 使用频率 | clawdbot 月费 | 小搭子 月费 | 节省 |
|---------|-------------|----------|------|
| 轻度（10次/天） | | | |
| 中度（50次/天） | | | |
| 重度（100次/天） | | | |

---

## 差异化定位总结

```
┌─────────────────────────────────────────────────────────────────┐
│                     AI Agent 市场定位                             │
│                                                                  │
│  clawdbot / OpenClaw                                            │
│  ├─ 定位：24/7 后台 AI 管家                                      │
│  ├─ 优势：消息通道丰富 / 后台自动化 / 生态成熟                     │
│  ├─ 劣势：配置复杂 / Token 消耗大 / 面向技术用户 / 云端依赖        │
│  └─ 用户：开发者 / 技术爱好者                                     │
│                                                                  │
│  小搭子                                                          │
│  ├─ 定位：桌面 AI 搭子（面对面交互）                               │
│  ├─ 优势：                                                       │
│  │   ├─ 零门槛安装 / 智能回溯 / 三层记忆 / Token 节省              │
│  │   ├─ 🔥 文件修改回滚（事务语义 + 崩溃恢复 + 用户可反悔）       │
│  │   ├─ 🔥 本地知识语义搜索（离线/隐私/零成本/30ms）              │
│  │   ├─ 🔥 长程桌面操作可靠性（断点续做 + 防重复 + 回滚安全网）   │
│  │   ├─ 🆕 开发者友好（Skill 热扩展 / 多模型兼容 / 实例隔离）    │
│  │   ├─ 🆕 垂直场景深度（写稿/表格/研究/办公/隐私 5 大搭子）     │
│  │   └─ 🆕 产品级健壮性（服务降级 / 大文件 / 多语言 / 中断恢复）  │
│  ├─ 劣势：无消息通道 / 无 24/7 后台 / 生态早期                     │
│  └─ 用户：内容创作者 / 职场人 / 研究者 / 小白用户 / 开源开发者      │
│                                                                  │
│  核心差异：不是"谁更强"，是"服务谁"                                │
│  clawdbot = 给技术用户的后台管家（云端驱动）                        │
│  小搭子 = 给所有人的桌面搭子（本地优先，你的数据只在你的电脑上）      │
└─────────────────────────────────────────────────────────────────┘
```

---

## 开源宣发测试优先级

面向开源发布，建议按以下优先级执行测试：

### P0 — 核心 Demo（开源首页 README 演示）

| 用例 | 金句 | 演示价值 |
|------|------|---------|
| G2 表格搭子 | "出错自己修" | 智能回溯 + 错误自愈，30 秒 GIF 可展示 |
| G1 写稿搭子 | "用久越懂你" | 跨项目风格隔离，记忆能力直观 |
| G5 隐私搭子 | "数据不出电脑" | 全离线运行，隐私卖点一目了然 |
| A1 Excel 分析 | "说一句它就干" | 格式混乱自动修复，最直接的效果对比 |

### P1 — 技术深度（开发者审视）

| 用例 | 技术卖点 | 开发者关注 |
|------|---------|----------|
| F1 Skill 开发 | 零代码扩展 | 生态贡献门槛 |
| F2 多模型切换 | 不绑定厂商 | 架构灵活度 |
| F4 上下文极限 | 上下文工程 | 技术壁垒 |
| H1 服务降级 | 生产级可靠性 | 部署信心 |

### P2 — 场景覆盖（用户说服力）

| 用例 | 目标用户 | 说服力 |
|------|---------|--------|
| G3 研究搭子 | 学生/研究者 | 论文搜索零幻觉 |
| G4 办公搭子 | 职场人 | 会议纪要 5 分钟 |
| H3 多语言 | 国际用户 | 中英混合无障碍 |
| H4 中断恢复 | 重度用户 | 崩溃不丢工作 |
