"""
é€šä¹‰åƒé—® LLM æœåŠ¡å®ç°

åŸºäº OpenAI å…¼å®¹æ¥å£å®ç°ï¼Œä¸ Claude æœåŠ¡ä¿æŒç›¸åŒçš„æ¥å£è§„èŒƒã€‚

æ”¯æŒçš„åŠŸèƒ½ï¼š
- åŸºç¡€å¯¹è¯ï¼ˆæµå¼/éæµå¼ï¼‰
- Function Callingï¼ˆå·¥å…·è°ƒç”¨ï¼‰
- æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆenable_thinkingï¼‰
- å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ï¼‰
- æ˜¾å¼ç¼“å­˜ï¼ˆcache_controlï¼‰
- ç»“æ„åŒ–è¾“å‡ºï¼ˆresponse_formatï¼‰

æ¨¡å‹å¯¹åº”å…³ç³»ï¼š
- qwen3-max â†” claude-sonnet-4-5ï¼ˆæ——èˆ°æ¨¡å‹ï¼‰
- qwen-plus â†” claude-haiku-4-5ï¼ˆå¿«é€Ÿæ¨¡å‹ï¼‰

å‚è€ƒæ–‡æ¡£ï¼š
- https://help.aliyun.com/zh/model-studio/qwen-api-reference
"""

import json
import os
from dataclasses import dataclass
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Union

import httpx
from openai import AsyncOpenAI

from infra.resilience import with_retry
from logger import get_logger

from .adaptor import OpenAIAdaptor
from .base import BaseLLMService, LLMConfig, LLMProvider, LLMResponse, Message, ToolType

logger = get_logger("llm.qwen")

# è¯¦ç»†æ—¥å¿—å¼€å…³
LLM_DEBUG_VERBOSE = os.getenv("LLM_DEBUG_VERBOSE", "").lower() in ("1", "true", "yes")


# ============================================================
# åƒé—®é…ç½®å’Œå¸¸é‡
# ============================================================

QWEN_MAX_TOKENS = 65536  # åƒé—®æ¨¡å‹å•æ¬¡å“åº”çš„ max_tokens ä¸Šé™ï¼ˆqwen3-max å®˜æ–¹æœ€å¤§å€¼ï¼‰


@dataclass
class QwenConfig(LLMConfig):
    """
    åƒé—®é…ç½®ç±»

    æ‰©å±• LLMConfigï¼Œæ·»åŠ åƒé—®ç‰¹æœ‰é…ç½®
    """

    # åœ°åŸŸé…ç½®
    region: str = "cn-beijing"  # cn-beijing, singapore, us-virginia, finance
    base_url: Optional[str] = None  # ğŸ†• è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆä¼˜å…ˆçº§é«˜äº regionï¼‰

    # åƒé—®ç‰¹æœ‰åŠŸèƒ½
    enable_thinking: bool = False  # æ·±åº¦æ€è€ƒæ¨¡å¼
    thinking_budget: Optional[int] = None  # æ€è€ƒé•¿åº¦é™åˆ¶

    # è§†è§‰æ¨¡å‹å‚æ•°
    vl_high_resolution_images: bool = False
    min_pixels: Optional[int] = None
    max_pixels: Optional[int] = None
    total_pixels: Optional[int] = None
    fps: Optional[float] = None

    # éŸ³é¢‘æ¨¡å‹å‚æ•°
    audio_voice: Optional[str] = None  # éŸ³è‰²
    audio_format: str = "wav"
    modalities: List[str] = None  # ["text"] æˆ– ["text", "audio"]

    # å…¶ä»–å‚æ•°
    seed: Optional[int] = None
    top_k: Optional[int] = None
    presence_penalty: float = 0.0
    response_format: Optional[Dict[str, Any]] = None  # ç»“æ„åŒ–è¾“å‡º


class QwenRegions:
    """åƒé—®æœåŠ¡åœ°åŸŸç«¯ç‚¹"""

    CN_BEIJING = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    SINGAPORE = "https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
    US_VIRGINIA = "https://dashscope-us.aliyuncs.com/compatible-mode/v1"
    FINANCE = "https://dashscope-finance.aliyuncs.com/compatible-mode/v1"

    MAPPING = {
        "cn-beijing": CN_BEIJING,
        "singapore": SINGAPORE,
        "us-virginia": US_VIRGINIA,
        "finance": FINANCE,
    }


class QwenModelCapability:
    """åƒé—®æ¨¡å‹èƒ½åŠ›æ£€æµ‹"""

    # æ”¯æŒæ·±åº¦æ€è€ƒçš„æ¨¡å‹
    THINKING_MODELS = {
        "qwen3-max-preview",
        "qwen3-max",
        "qwen-max",
        "qwen-plus",
        "qwen-turbo",
        "qwen-flash",
        "qwen3-vl-plus",
        "qwen3-vl-flash",
        "qwen3",
        "qwen3-235b-a22b-thinking",
    }

    # æ”¯æŒè§†è§‰çš„æ¨¡å‹
    VISION_MODELS = {
        "qwen-vl-max",
        "qwen-vl-plus",
        "qwen3-vl-plus",
        "qwen3-vl-flash",
        "qvq-72b-preview",
        "qvq-max",
    }

    # æ”¯æŒéŸ³é¢‘çš„æ¨¡å‹
    AUDIO_MODELS = {
        "qwen-omni-turbo",
        "qwen3-omni-flash",
        "qwen-audio-turbo",
    }

    # æ”¯æŒ Function Calling çš„æ¨¡å‹ï¼ˆå¤§éƒ¨åˆ†æ¨¡å‹éƒ½æ”¯æŒï¼‰
    TOOL_CALLING_MODELS = {
        "qwen-max",
        "qwen-plus",
        "qwen-turbo",
        "qwen-flash",
        "qwen3-max",
        "qwen3-plus",
        "qwen-vl-max",
        "qwen-vl-plus",
        "qwen3-vl-plus",
        "qwen3-vl-flash",
    }

    @staticmethod
    def supports_thinking(model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ·±åº¦æ€è€ƒ"""
        return any(m in model for m in QwenModelCapability.THINKING_MODELS)

    @staticmethod
    def supports_vision(model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰"""
        return any(m in model for m in QwenModelCapability.VISION_MODELS)

    @staticmethod
    def supports_audio(model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒéŸ³é¢‘"""
        return any(m in model for m in QwenModelCapability.AUDIO_MODELS)

    @staticmethod
    def supports_tools(model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨"""
        return any(m in model for m in QwenModelCapability.TOOL_CALLING_MODELS)


# ============================================================
# åƒé—® LLM æœåŠ¡
# ============================================================


class QwenLLMService(BaseLLMService):
    """
    é€šä¹‰åƒé—® LLM æœåŠ¡å®ç°

    åŸºäº OpenAI å…¼å®¹æ¥å£ï¼Œä¿æŒä¸ Claude æœåŠ¡ç›¸åŒçš„æ¥å£è§„èŒƒã€‚

    æ¨¡å‹å¯¹åº”å…³ç³»ï¼š
    - qwen3-max: å¯¹æ ‡ claude-sonnet-4-5ï¼ˆæ——èˆ°æ¨¡å‹ï¼Œé€‚ç”¨äºå¤æ‚æ¨ç†ï¼‰
    - qwen-plus: å¯¹æ ‡ claude-haiku-4-5ï¼ˆå¿«é€Ÿæ¨¡å‹ï¼Œé€‚ç”¨äºç®€å•ä»»åŠ¡ï¼‰

    æ”¯æŒçš„åŠŸèƒ½ï¼š
    - åŸºç¡€å¯¹è¯ï¼ˆæµå¼/éæµå¼ï¼‰
    - Function Callingï¼ˆå·¥å…·è°ƒç”¨ï¼‰
    - æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆenable_thinkingï¼‰
    - å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ï¼‰
    - æ˜¾å¼ç¼“å­˜ï¼ˆcache_controlï¼‰
    - ç»“æ„åŒ–è¾“å‡ºï¼ˆresponse_formatï¼‰

    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    config = QwenConfig(
        model="qwen3-max",  # æˆ– qwen-plus
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        enable_thinking=True
    )
    llm = QwenLLMService(config)

    response = await llm.create_message_async(
        messages=[Message(role="user", content="ä½ å¥½")],
        system="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"
    )
    ```
    """

    def __init__(self, config: Union[LLMConfig, QwenConfig]):
        """
        åˆå§‹åŒ–åƒé—®æœåŠ¡

        Args:
            config: åƒé—®é…ç½®ï¼ˆLLMConfig æˆ– QwenConfigï¼‰
        """
        # å¦‚æœä¼ å…¥çš„æ˜¯ LLMConfigï¼Œè½¬æ¢ä¸º QwenConfig
        if isinstance(config, LLMConfig) and not isinstance(config, QwenConfig):
            self.config = QwenConfig(
                provider=config.provider,
                model=config.model,
                api_key=config.api_key,
                max_tokens=config.max_tokens,
                temperature=config.temperature,
                enable_thinking=config.enable_thinking,
                thinking_budget=config.thinking_budget,
                enable_caching=config.enable_caching,
                timeout=getattr(config, "timeout", 120.0),
                max_retries=getattr(config, "max_retries", 3),
            )
        else:
            self.config = config

        # æ¶ˆæ¯é€‚é…å™¨ï¼ˆç»Ÿä¸€å¤„ç†æ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼‰
        self._adaptor = OpenAIAdaptor()

        # API Keyï¼ˆä¼˜å…ˆçº§ï¼šé…ç½® > ç¯å¢ƒå˜é‡ï¼‰
        api_key = self.config.api_key or os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            raise ValueError(
                "åƒé—® API Key æœªè®¾ç½®ã€‚è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æˆ–ä¼ å…¥ api_key å‚æ•°"
            )

        # æ‰“å° API Key ä¿¡æ¯ï¼ˆä»…æ˜¾ç¤ºå‰8ä½å’Œå4ä½ï¼‰
        masked_key = f"{api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else "***"
        logger.info(f"ğŸ”‘ Qwen API Key: {masked_key} (é•¿åº¦: {len(api_key)})")

        # è·å– API ç«¯ç‚¹ï¼ˆä¼˜å…ˆä½¿ç”¨ base_urlï¼Œå¦åˆ™æ ¹æ® region é€‰æ‹©ï¼‰
        base_url = getattr(self.config, "base_url", None)
        if base_url:
            logger.info(f"ğŸŒ åƒé—®ç«¯ç‚¹ï¼ˆè‡ªå®šä¹‰ï¼‰: {base_url}")
        else:
            region = getattr(self.config, "region", "singapore")
            base_url = QwenRegions.MAPPING.get(region, QwenRegions.SINGAPORE)
            logger.info(f"ğŸŒ åƒé—®åœ°åŸŸ: {region} ({base_url})")

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        timeout = getattr(self.config, "timeout", 120.0)
        max_retries = getattr(self.config, "max_retries", 3)

        self.client = AsyncOpenAI(
            api_key=api_key, base_url=base_url, timeout=timeout, max_retries=max_retries
        )

        # è‡ªå®šä¹‰å·¥å…·å­˜å‚¨
        self._custom_tools: List[Dict[str, Any]] = []

        logger.info(f"âœ… åƒé—®æœåŠ¡åˆå§‹åŒ–æˆåŠŸ: model={self.config.model}")

    # ============================================================
    # è‡ªå®šä¹‰å·¥å…·ç®¡ç†ï¼ˆä¸ Claude ä¿æŒä¸€è‡´ï¼‰
    # ============================================================

    def add_custom_tool(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:
        """
        æ·»åŠ è‡ªå®šä¹‰å·¥å…·

        Args:
            name: å·¥å…·åç§°
            description: å·¥å…·æè¿°
            input_schema: è¾“å…¥å‚æ•° schemaï¼ˆJSON Schema æ ¼å¼ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåå·¥å…·
        for i, tool in enumerate(self._custom_tools):
            if tool["name"] == name:
                # æ›´æ–°ç°æœ‰å·¥å…·
                self._custom_tools[i] = {
                    "name": name,
                    "description": description,
                    "input_schema": input_schema,
                }
                logger.debug(f"æ›´æ–°è‡ªå®šä¹‰å·¥å…·: {name}")
                return

        # æ·»åŠ æ–°å·¥å…·
        self._custom_tools.append(
            {"name": name, "description": description, "input_schema": input_schema}
        )
        logger.debug(f"æ³¨å†Œè‡ªå®šä¹‰å·¥å…·: {name}")

    def remove_custom_tool(self, name: str) -> bool:
        """
        ç§»é™¤è‡ªå®šä¹‰å·¥å…·

        Args:
            name: å·¥å…·åç§°

        Returns:
            æ˜¯å¦æˆåŠŸç§»é™¤
        """
        for i, tool in enumerate(self._custom_tools):
            if tool["name"] == name:
                self._custom_tools.pop(i)
                logger.debug(f"ç§»é™¤è‡ªå®šä¹‰å·¥å…·: {name}")
                return True
        return False

    def get_custom_tools(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰è‡ªå®šä¹‰å·¥å…·

        Returns:
            è‡ªå®šä¹‰å·¥å…·åˆ—è¡¨
        """
        return self._custom_tools.copy()

    def clear_custom_tools(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰è‡ªå®šä¹‰å·¥å…·"""
        self._custom_tools.clear()
        logger.debug("æ¸…ç©ºæ‰€æœ‰è‡ªå®šä¹‰å·¥å…·")

    def convert_to_tool_schema(self, capability: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°†èƒ½åŠ›å®šä¹‰è½¬æ¢ä¸ºåƒé—® API æ ¼å¼

        Args:
            capability: èƒ½åŠ›å®šä¹‰

        Returns:
            åƒé—® API æ ¼å¼çš„å·¥å…·å®šä¹‰
        """
        name = capability.get("name", "")
        input_schema = capability.get(
            "input_schema", {"type": "object", "properties": {}, "required": []}
        )
        description = capability.get("metadata", {}).get("description", f"Tool: {name}")

        # åƒé—®ä½¿ç”¨æ ‡å‡†çš„ OpenAI Function Calling æ ¼å¼
        return {
            "type": "function",
            "function": {"name": name, "description": description, "parameters": input_schema},
        }

    def _format_tools(self, tools: List[Union[ToolType, str, Dict]]) -> List[Dict[str, Any]]:
        """
        æ ¼å¼åŒ–å·¥å…·åˆ—è¡¨

        æ”¯æŒä¸‰ç§è¾“å…¥ï¼š
        1. ToolType æšä¸¾ï¼ˆä¼šè¢«å¿½ç•¥ï¼Œåƒé—®ä¸æ”¯æŒåŸç”Ÿå·¥å…·ï¼‰
        2. å­—ç¬¦ä¸²ï¼ˆå·¥å…·åç§°ï¼‰
        3. å®Œæ•´ schema å­—å…¸
        """
        formatted = []

        for idx, tool in enumerate(tools):
            try:
                if isinstance(tool, ToolType):
                    # åƒé—®æ²¡æœ‰åŸç”Ÿå·¥å…·ï¼Œè·³è¿‡
                    logger.warning(f"åƒé—®ä¸æ”¯æŒ ToolType æšä¸¾: {tool}ï¼Œå·²è·³è¿‡")
                    continue

                elif isinstance(tool, str):
                    # ä»è‡ªå®šä¹‰å·¥å…·ä¸­æŸ¥æ‰¾
                    found = False
                    for custom_tool in self._custom_tools:
                        if custom_tool.get("name") == tool:
                            formatted.append(self._convert_tool_to_openai_format(custom_tool))
                            found = True
                            break
                    if not found:
                        logger.warning(f"æœªæ‰¾åˆ°å·¥å…·: {tool}")

                elif isinstance(tool, dict):
                    formatted.append(self._convert_tool_to_openai_format(tool))

                else:
                    raise ValueError(f"Invalid tool format: {tool}")

                # éªŒè¯ JSON å¯åºåˆ—åŒ–
                if formatted:
                    json.dumps(formatted[-1])

            except Exception as e:
                logger.error(f"å¤„ç†å·¥å…· #{idx} æ—¶å‡ºé”™: {e}")
                raise

        return formatted

    def _convert_tool_to_openai_format(self, tool: Dict[str, Any]) -> Dict[str, Any]:
        """
        è½¬æ¢å·¥å…·ä¸º OpenAI Function Calling æ ¼å¼

        Args:
            tool: å·¥å…·å®šä¹‰ï¼ˆå¯èƒ½æ˜¯ Claude æ ¼å¼æˆ–è‡ªå®šä¹‰æ ¼å¼ï¼‰

        Returns:
            OpenAI æ ¼å¼çš„å·¥å…·å®šä¹‰
        """
        # å¦‚æœå·²ç»æ˜¯ OpenAI æ ¼å¼ï¼ˆåŒ…å« type: functionï¼‰ï¼Œç›´æ¥è¿”å›
        if tool.get("type") == "function":
            return tool

        # è½¬æ¢ Claude æ ¼å¼åˆ° OpenAI æ ¼å¼
        return {
            "type": "function",
            "function": {
                "name": tool.get("name", ""),
                "description": tool.get("description", ""),
                "parameters": tool.get("input_schema", {}),
            },
        }

    # ============================================================
    # æ ¸å¿ƒ API æ–¹æ³•
    # ============================================================

    @with_retry(
        max_retries=3,
        base_delay=1.0,
        retryable_errors=(
            # OpenAI SDK å¼‚å¸¸
            httpx.RemoteProtocolError,
            httpx.ConnectError,
            httpx.TimeoutException,
        ),
    )
    async def create_message_async(
        self,
        messages: List[Message],
        system: Optional[Union[str, List[Dict[str, Any]]]] = None,
        tools: Optional[List[Union[ToolType, str, Dict]]] = None,
        invocation_type: Optional[str] = None,
        override_thinking: Optional[bool] = None,
        is_probe: bool = False,
        **kwargs,
    ) -> LLMResponse:
        """
        åˆ›å»ºæ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤ºè¯ï¼ˆæ”¯æŒå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
            tools: å·¥å…·åˆ—è¡¨
            invocation_type: è°ƒç”¨æ–¹å¼ï¼ˆåƒé—®ä¸éœ€è¦ï¼‰
            override_thinking: åŠ¨æ€è¦†ç›–æ€è€ƒé…ç½®
            is_probe: æ˜¯å¦ä¸ºæ¢æµ‹è¯·æ±‚
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMResponse å“åº”å¯¹è±¡
        """
        # ä½¿ç”¨ adaptor è½¬æ¢æ¶ˆæ¯
        converted = self._adaptor.convert_messages_to_provider(messages)
        openai_messages = converted["messages"]

        # æ„å»ºè¯·æ±‚å‚æ•°
        # âš ï¸ åƒé—®é™åˆ¶: max_tokens ä¸èƒ½è¶…è¿‡ QWEN_MAX_TOKENS (65536)
        max_tokens = min(kwargs.get("max_tokens", self.config.max_tokens), QWEN_MAX_TOKENS)
        request_params = {
            "model": self.config.model,
            "messages": openai_messages,
            "temperature": kwargs.get("temperature", self.config.temperature),
            "max_tokens": max_tokens,
            "stream": False,
        }

        # System promptï¼ˆåƒé—®æ”¯æŒåˆ—è¡¨æ ¼å¼ç”¨äºæ˜¾å¼ç¼“å­˜ï¼‰
        if system:
            if isinstance(system, list):
                # è½¬æ¢ä¸ºåƒé—®æ ¼å¼ï¼ˆæ·»åŠ åˆ° messages å¼€å¤´ï¼‰
                system_message = self._build_system_message(system)
                request_params["messages"].insert(0, system_message)
            elif isinstance(system, dict):
                # dict æ ¼å¼ï¼ŒåŒ…è£…æˆ list å¤„ç†
                system_message = self._build_system_message([system])
                request_params["messages"].insert(0, system_message)
            else:
                # å­—ç¬¦ä¸²æ ¼å¼
                request_params["messages"].insert(0, {"role": "system", "content": str(system)})

        # åƒé—®ç‰¹æœ‰å‚æ•°ï¼ˆé€šè¿‡ extra_body ä¼ é€’ï¼‰
        extra_body = self._build_extra_body(override_thinking, kwargs)
        if extra_body:
            # OpenAI SDK çš„ extra_body å‚æ•°
            for key, value in extra_body.items():
                request_params[key] = value

        # Toolsï¼ˆFunction Callingï¼‰
        all_tools = []
        tool_names_seen = set()

        # æ·»åŠ ç”¨æˆ·æŒ‡å®šçš„å·¥å…·
        if tools:
            for tool in self._format_tools(tools):
                tool_name = tool.get("function", {}).get("name", "")
                if tool_name and tool_name not in tool_names_seen:
                    all_tools.append(tool)
                    tool_names_seen.add(tool_name)

        # æ·»åŠ è‡ªå®šä¹‰å·¥å…·
        for custom_tool in self._custom_tools:
            tool_name = custom_tool.get("name", "")
            if tool_name and tool_name not in tool_names_seen:
                all_tools.append(self._convert_tool_to_openai_format(custom_tool))
                tool_names_seen.add(tool_name)

        if all_tools:
            request_params["tools"] = all_tools
            request_params["tool_choice"] = kwargs.get("tool_choice", "auto")
            logger.debug(f"Tools: {[t['function']['name'] for t in all_tools]}")

        # è®°å½• max_tokens é™åˆ¶è­¦å‘Š
        original_max_tokens = kwargs.get("max_tokens", self.config.max_tokens)
        if original_max_tokens > QWEN_MAX_TOKENS:
            logger.warning(
                f"âš ï¸ max_tokens å·²é™åˆ¶: {original_max_tokens} â†’ {QWEN_MAX_TOKENS} " f"(åƒé—®ä¸Šé™)"
            )

        # è°ƒè¯•æ—¥å¿—
        logger.debug(f"ğŸ“¤ åƒé—®è¯·æ±‚: model={self.config.model}, messages={len(openai_messages)}")

        if LLM_DEBUG_VERBOSE:
            logger.info("=" * 80)
            logger.info("ğŸ” [DEBUG-ASYNC] å®Œæ•´ request_params:")
            logger.info(f"   model: {request_params.get('model')}")
            logger.info(f"   messages: {len(request_params.get('messages', []))}")
            for i, msg in enumerate(request_params.get("messages", [])):
                logger.info(
                    f"   [{i}] role={msg.get('role')}, content={str(msg.get('content'))[:200]}..."
                )
            logger.info("=" * 80)

        # API è°ƒç”¨
        try:
            response = await self.client.chat.completions.create(**request_params)
        except Exception as e:
            if not is_probe:
                logger.error(f"åƒé—® API è°ƒç”¨å¤±è´¥: {e}")
            raise

        # è½¬æ¢å“åº”
        return self._parse_response(response)

    async def create_message_stream(
        self,
        messages: List[Message],
        system: Optional[Union[str, List[Dict[str, Any]]]] = None,
        tools: Optional[List[Union[ToolType, str, Dict]]] = None,
        on_thinking: Optional[Callable[[str], None]] = None,
        on_content: Optional[Callable[[str], None]] = None,
        on_tool_call: Optional[Callable[[Dict], None]] = None,
        override_thinking: Optional[bool] = None,
        **kwargs,
    ) -> AsyncIterator[LLMResponse]:
        """
        åˆ›å»ºæ¶ˆæ¯ï¼ˆæµå¼ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·åˆ—è¡¨
            on_thinking: thinking å›è°ƒ
            on_content: content å›è°ƒ
            on_tool_call: tool_call å›è°ƒ
            override_thinking: åŠ¨æ€è¦†ç›– thinking é…ç½®
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            LLMResponse ç‰‡æ®µ
        """
        # ä½¿ç”¨ adaptor è½¬æ¢æ¶ˆæ¯
        converted = self._adaptor.convert_messages_to_provider(messages)
        openai_messages = converted["messages"]

        # æ„å»ºè¯·æ±‚å‚æ•°
        # âš ï¸ åƒé—®é™åˆ¶: max_tokens ä¸èƒ½è¶…è¿‡ QWEN_MAX_TOKENS (65536)
        max_tokens = min(kwargs.get("max_tokens", self.config.max_tokens), QWEN_MAX_TOKENS)
        request_params = {
            "model": self.config.model,
            "messages": openai_messages,
            "temperature": kwargs.get("temperature", self.config.temperature),
            "max_tokens": max_tokens,
            "stream": True,
            "stream_options": {"include_usage": True},
        }

        # System prompt
        if system:
            if isinstance(system, list):
                system_message = self._build_system_message(system)
                request_params["messages"].insert(0, system_message)
            elif isinstance(system, dict):
                # dict æ ¼å¼ï¼ŒåŒ…è£…æˆ list å¤„ç†
                system_message = self._build_system_message([system])
                request_params["messages"].insert(0, system_message)
            else:
                # å­—ç¬¦ä¸²æ ¼å¼
                request_params["messages"].insert(0, {"role": "system", "content": str(system)})

        # åƒé—®ç‰¹æœ‰å‚æ•°ï¼ˆé€šè¿‡ extra_body ä¼ é€’ï¼‰
        # âš ï¸ æ³¨æ„ï¼šä»¥ä¸‹å‚æ•°ä¸º Qwen éæ ‡å‡†å‚æ•°ï¼Œéœ€è¦æ”¾åœ¨ extra_body ä¸­ï¼š
        # - enable_thinking: æ€è€ƒæ¨¡å¼
        # - top_k: é‡‡æ ·å‚æ•°
        # - vl_high_resolution_images: é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†
        # - thinking_budget: æ€è€ƒè¿‡ç¨‹çš„æœ€å¤§ Token æ•°
        extra_body = self._build_extra_body(override_thinking, kwargs)
        if extra_body:
            request_params["extra_body"] = extra_body

        # Tools
        all_tools = []
        tool_names_seen = set()

        if tools:
            for tool in self._format_tools(tools):
                tool_name = tool.get("function", {}).get("name", "")
                if tool_name and tool_name not in tool_names_seen:
                    all_tools.append(tool)
                    tool_names_seen.add(tool_name)

        for custom_tool in self._custom_tools:
            tool_name = custom_tool.get("name", "")
            if tool_name and tool_name not in tool_names_seen:
                all_tools.append(self._convert_tool_to_openai_format(custom_tool))
                tool_names_seen.add(tool_name)

        if all_tools:
            request_params["tools"] = all_tools
            request_params["tool_choice"] = kwargs.get("tool_choice", "auto")

        # è®°å½• max_tokens é™åˆ¶è­¦å‘Š
        original_max_tokens = kwargs.get("max_tokens", self.config.max_tokens)
        if original_max_tokens > QWEN_MAX_TOKENS:
            logger.warning(
                f"âš ï¸ max_tokens å·²é™åˆ¶: {original_max_tokens} â†’ {QWEN_MAX_TOKENS} " f"(åƒé—®ä¸Šé™)"
            )

        logger.info(f"ğŸ“¤ åƒé—®æµå¼è¯·æ±‚: model={self.config.model}, messages={len(openai_messages)}")

        # ç´¯ç§¯å˜é‡
        accumulated_thinking = ""
        accumulated_content = ""
        tool_calls = []
        stop_reason = None
        usage = {}

        try:
            stream = await self.client.chat.completions.create(**request_params)

            async for chunk in stream:
                if not chunk.choices:
                    # æœ€åä¸€ä¸ª chunkï¼ˆåŒ…å« usageï¼‰
                    if chunk.usage:
                        usage = {
                            "input_tokens": chunk.usage.prompt_tokens,
                            "output_tokens": chunk.usage.completion_tokens,
                            "thinking_tokens": 0,
                        }

                        # ä¼°ç®— thinking tokens
                        if accumulated_thinking:
                            usage["thinking_tokens"] = self.count_tokens(accumulated_thinking)

                        # Token ä½¿ç”¨é‡æ—¥å¿—
                        logger.info(
                            f"ğŸ“Š Token ä½¿ç”¨: input={usage['input_tokens']:,}, "
                            f"output={usage['output_tokens']:,}, "
                            f"thinking={usage['thinking_tokens']:,}"
                        )
                    continue

                choice = chunk.choices[0]
                delta = choice.delta

                # å¤„ç†æ€ç»´é“¾å†…å®¹ï¼ˆåƒé—®é€šè¿‡ reasoning_content è¿”å›ï¼‰
                if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                    accumulated_thinking += delta.reasoning_content
                    if on_thinking:
                        on_thinking(delta.reasoning_content)
                    yield LLMResponse(
                        content="",
                        thinking=delta.reasoning_content,
                        model=self.config.model,  # ğŸ†• æµå¼ä¸­é—´å—ä¹Ÿéœ€è¦ model
                        is_stream=True,
                    )

                # å¤„ç†æ™®é€šå†…å®¹
                if delta.content:
                    accumulated_content += delta.content
                    if on_content:
                        on_content(delta.content)
                    yield LLMResponse(
                        content=delta.content,
                        model=self.config.model,  # ğŸ†• æµå¼ä¸­é—´å—ä¹Ÿéœ€è¦ model
                        is_stream=True,
                    )

                # å¤„ç†å·¥å…·è°ƒç”¨
                if delta.tool_calls:
                    for tool_call in delta.tool_calls:
                        # ğŸ” è°ƒè¯•æ—¥å¿— - ä»…åœ¨æœ‰ id/name æ—¶è®°å½• INFOï¼Œå¦åˆ™ DEBUG
                        if tool_call.id or (tool_call.function and tool_call.function.name):
                            logger.info(
                                f"ğŸ” [DEBUG-STREAM] æ”¶åˆ°å·¥å…·è°ƒç”¨ delta: index={tool_call.index}, id={tool_call.id}, name={tool_call.function.name if tool_call.function else 'None'}"
                            )
                        else:
                            # åªç´¯ç§¯ argumentsï¼Œä½¿ç”¨ DEBUG çº§åˆ«é¿å…æ—¥å¿—è¿‡å¤š
                            logger.debug(
                                f"ğŸ” [DEBUG-STREAM] ç´¯ç§¯ arguments chunk: index={tool_call.index}"
                            )

                        # âœ… ç´¯ç§¯å·¥å…·è°ƒç”¨åˆ°åˆ—è¡¨ä¸­
                        # OpenAI æµå¼ API çš„å·¥å…·è°ƒç”¨å¯èƒ½åˆ†å¤šä¸ª chunk è¿”å›ï¼Œéœ€è¦æŒ‰ index ç´¯ç§¯
                        index = tool_call.index

                        # ç¡®ä¿ tool_calls åˆ—è¡¨è¶³å¤Ÿé•¿
                        while len(tool_calls) <= index:
                            tool_calls.append(
                                {"id": "", "name": "", "arguments": "", "type": "function"}
                            )

                        # ç´¯ç§¯å­—æ®µ
                        if tool_call.id:
                            tool_calls[index]["id"] = tool_call.id

                            # ğŸ†• Tool Use Start äº‹ä»¶
                            yield LLMResponse(
                                content="",
                                model=self.config.model,
                                is_stream=True,
                                tool_use_start={
                                    "type": "tool_use",
                                    "id": tool_call.id,
                                    "name": tool_call.function.name if tool_call.function else "",
                                },
                            )

                        if tool_call.function:
                            if tool_call.function.name:
                                tool_calls[index]["name"] = tool_call.function.name
                            if tool_call.function.arguments:
                                tool_calls[index]["arguments"] += tool_call.function.arguments

                                # ğŸ†• Input Delta äº‹ä»¶
                                yield LLMResponse(
                                    content="",
                                    model=self.config.model,
                                    is_stream=True,
                                    input_delta=tool_call.function.arguments,
                                )

                        # å›è°ƒ
                        if on_tool_call:
                            on_tool_call(
                                {
                                    "id": tool_call.id,
                                    "name": tool_call.function.name if tool_call.function else "",
                                    "arguments": (
                                        tool_call.function.arguments if tool_call.function else ""
                                    ),
                                }
                            )

                # åœæ­¢åŸå› 
                if choice.finish_reason:
                    stop_reason = choice.finish_reason

            # âœ… å¤„ç†ç´¯ç§¯çš„å·¥å…·è°ƒç”¨ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            formatted_tool_calls = []
            if tool_calls:
                logger.info(f"ğŸ” [DEBUG-STREAM] ç´¯ç§¯çš„å·¥å…·è°ƒç”¨æ•°é‡: {len(tool_calls)}")
                for tc in tool_calls:
                    if tc.get("name"):  # åªæ·»åŠ æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨
                        try:
                            # è§£æ arguments å­—ç¬¦ä¸²ä¸º JSON
                            # ğŸ”§ ä¿®å¤ï¼šQwen å¯èƒ½è¿”å›åŒ…å«æ§åˆ¶å­—ç¬¦çš„ JSONï¼Œéœ€è¦ä½¿ç”¨ strict=False
                            input_dict = (
                                json.loads(tc["arguments"], strict=False) if tc["arguments"] else {}
                            )

                            # ğŸ†• å‚æ•°è§„èŒƒåŒ–ï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯åŸºæœ¬ç±»å‹ï¼Œæ‰å¹³åŒ–åµŒå¥—ç»“æ„
                            normalized_input = self._normalize_tool_input(input_dict)

                            formatted_tool_calls.append(
                                {
                                    "id": tc["id"],
                                    "name": tc["name"],
                                    "input": normalized_input,
                                    "type": "tool_use",  # ğŸ”„ ç»Ÿä¸€è½¬æ¢ä¸º Claude æ ¼å¼ï¼ˆé€‚é…å™¨å±‚èŒè´£ï¼‰
                                }
                            )
                            logger.info(
                                f"ğŸ” [DEBUG-STREAM] æ ¼å¼åŒ–å·¥å…·è°ƒç”¨: id={tc['id']}, name={tc['name']}"
                            )
                            logger.info(
                                f"ğŸ” [DEBUG-STREAM] å·¥å…·è°ƒç”¨å‚æ•°: {json.dumps(normalized_input, ensure_ascii=False)[:500]}"
                            )
                        except json.JSONDecodeError as e:
                            logger.error(f"âŒ å·¥å…·è°ƒç”¨å‚æ•°è§£æå¤±è´¥: {e}")
                            logger.error(
                                f"   åŸå§‹ argumentsï¼ˆå‰200å­—ç¬¦ï¼‰: {tc['arguments'][:200] if tc.get('arguments') else 'None'}"
                            )

            # æ„å»º raw_content
            raw_content = []
            if accumulated_thinking:
                raw_content.append({"type": "thinking", "thinking": accumulated_thinking})
            if accumulated_content:
                raw_content.append({"type": "text", "text": accumulated_content})
            # æ·»åŠ å·¥å…·è°ƒç”¨åˆ° raw_content
            for tc in formatted_tool_calls:
                raw_content.append(
                    {"type": "tool_use", "id": tc["id"], "name": tc["name"], "input": tc["input"]}
                )

            logger.info(f"ğŸ“¥ åƒé—®å“åº”: stop_reason={stop_reason or 'stop'}")

            # ğŸ”„ ç»Ÿä¸€è½¬æ¢ stop_reasonï¼ˆQwen -> Claude æ ¼å¼ï¼‰
            # OpenAI/Qwen: "tool_calls" -> Claude: "tool_use"
            if stop_reason == "tool_calls" or (formatted_tool_calls and stop_reason == "stop"):
                stop_reason = "tool_use"
                logger.debug("ğŸ”„ è½¬æ¢ stop_reason: tool_calls -> tool_use")

            # è¿”å›æœ€ç»ˆå“åº”
            yield LLMResponse(
                content=accumulated_content,
                thinking=accumulated_thinking if accumulated_thinking else None,
                tool_calls=formatted_tool_calls if formatted_tool_calls else None,
                stop_reason=stop_reason or "stop",  # âœ… å·²è½¬æ¢ä¸º Claude æ ¼å¼
                usage=usage if usage else None,
                model=self.config.model,  # ğŸ†• å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°
                raw_content=raw_content,
                is_stream=False,
            )

        except Exception as e:
            logger.error(f"åƒé—®æµå¼ä¼ è¾“é”™è¯¯: {e}")
            raise

    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®— token æ•°é‡

        TODO: ä½¿ç”¨ Qwen å®˜æ–¹ tokenizer ç²¾ç¡®è®¡ç®—
        - é˜¿é‡Œäº‘å¯èƒ½æä¾›å®˜æ–¹ tokenizer
        - æˆ–ä½¿ç”¨ HuggingFace çš„ Qwen tokenizer

        å½“å‰ä½¿ç”¨çˆ¶ç±»çš„ tiktoken å®ç°ï¼ˆè¿‘ä¼¼ï¼‰ã€‚

        Args:
            text: è¦è®¡ç®—çš„æ–‡æœ¬

        Returns:
            token æ•°é‡
        """
        # TODO: å®ç° Qwen å®˜æ–¹ token è®¡ç®—
        return super().count_tokens(text)

    # ============================================================
    # è¾…åŠ©æ–¹æ³•
    # ============================================================

    def _build_system_message(self, system_blocks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ˆæ”¯æŒæ˜¾å¼ç¼“å­˜ï¼‰

        Args:
            system_blocks: ç³»ç»Ÿæç¤ºè¯å—åˆ—è¡¨

        Returns:
            ç³»ç»Ÿæ¶ˆæ¯
        """
        # åƒé—®çš„æ˜¾å¼ç¼“å­˜é€šè¿‡ cache_control å®ç°
        # æ ¼å¼ä¸ Claude ç±»ä¼¼ï¼Œä½†åªæ”¯æŒ ephemeral ç±»å‹
        content_blocks = []

        for block in system_blocks:
            if isinstance(block, dict):
                content_blocks.append(block)
            else:
                content_blocks.append({"type": "text", "text": str(block)})

        # å¦‚æœåªæœ‰ä¸€ä¸ªçº¯æ–‡æœ¬å—ï¼Œç®€åŒ–æ ¼å¼
        if len(content_blocks) == 1 and content_blocks[0].get("type") == "text":
            return {"role": "system", "content": content_blocks[0].get("text", "")}

        return {"role": "system", "content": content_blocks}

    def _build_extra_body(
        self, override_thinking: Optional[bool], kwargs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ„å»ºåƒé—®ç‰¹æœ‰å‚æ•°

        Args:
            override_thinking: åŠ¨æ€è¦†ç›–æ€è€ƒé…ç½®
            kwargs: å…¶ä»–å‚æ•°

        Returns:
            extra_body å­—å…¸
        """
        extra = {}

        # æ·±åº¦æ€è€ƒ
        effective_thinking = (
            override_thinking
            if override_thinking is not None
            else getattr(self.config, "enable_thinking", False)
        )
        if effective_thinking and QwenModelCapability.supports_thinking(self.config.model):
            extra["enable_thinking"] = True
            thinking_budget = getattr(self.config, "thinking_budget", None)
            if thinking_budget:
                extra["thinking_budget"] = thinking_budget

        # è§†è§‰æ¨¡å‹å‚æ•°
        if QwenModelCapability.supports_vision(self.config.model):
            if getattr(self.config, "vl_high_resolution_images", False):
                extra["vl_high_resolution_images"] = True

        # å…¶ä»–å‚æ•°
        seed = getattr(self.config, "seed", None)
        if seed is not None:
            extra["seed"] = seed

        top_k = getattr(self.config, "top_k", None)
        if top_k is not None:
            extra["top_k"] = top_k

        presence_penalty = getattr(self.config, "presence_penalty", 0.0)
        if presence_penalty != 0.0:
            extra["presence_penalty"] = presence_penalty

        # ç»“æ„åŒ–è¾“å‡º
        response_format = getattr(self.config, "response_format", None) or kwargs.get(
            "response_format"
        )
        if response_format:
            extra["response_format"] = response_format

        return extra

    def _normalize_tool_input(self, input_dict: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§„èŒƒåŒ–å·¥å…·è¾“å…¥å‚æ•°ï¼Œç¡®ä¿æ‰€æœ‰åµŒå¥—çš„å­—å…¸/å¯¹è±¡éƒ½è¢«æ‰å¹³åŒ–ä¸ºåŸºæœ¬ç±»å‹

        Qwen å¯èƒ½ä¼šå°†æŸäº›å‚æ•°ä½œä¸ºåµŒå¥—å¯¹è±¡ä¼ é€’ï¼Œä¾‹å¦‚ï¼š
        {
            "api_name": {"name": "wenshu_api", "config": {...}},
            "url": "..."
        }

        éœ€è¦æ‰å¹³åŒ–ä¸ºï¼š
        {
            "api_name": "wenshu_api",
            "url": "..."
        }

        ğŸ¯ è®¾è®¡åŸåˆ™ï¼šæ‰€æœ‰ Qwen ç‰¹æœ‰çš„å‚æ•°é€‚é…éƒ½åœ¨æ­¤å¤„ç†ï¼Œ
        ä¸šåŠ¡å±‚çš„å·¥å…·ä»£ç ï¼ˆå¦‚ api_callingï¼‰æ— éœ€å…³å¿ƒæ¨¡å‹å·®å¼‚ã€‚

        Args:
            input_dict: åŸå§‹è¾“å…¥å‚æ•°

        Returns:
            è§„èŒƒåŒ–åçš„å‚æ•°
        """
        normalized = {}

        for key, value in input_dict.items():
            if isinstance(value, dict):
                # ğŸ”§ ç­–ç•¥ 1: ä¼˜å…ˆæå– "name" å­—æ®µï¼ˆå¸¸è§äº api_name ç­‰æ ‡è¯†ç¬¦å‚æ•°ï¼‰
                if "name" in value:
                    normalized[key] = value["name"]
                    if len(value) > 1:
                        logger.debug(
                            f"ğŸ”§ Qwen å‚æ•°è§„èŒƒåŒ–: {key} -> ä»å¤šå­—æ®µ dict æå– 'name': {value['name']} (å¿½ç•¥å…¶ä»–å­—æ®µ: {list(value.keys())})"
                        )
                    else:
                        logger.debug(
                            f"ğŸ”§ Qwen å‚æ•°è§„èŒƒåŒ–: {key} -> ä» dict æå– 'name': {value['name']}"
                        )

                # ğŸ”§ ç­–ç•¥ 2: æå– "value" å­—æ®µï¼ˆå¸¸è§äºå€¼ç±»å‹å‚æ•°ï¼‰
                elif "value" in value:
                    normalized[key] = value["value"]
                    logger.debug(
                        f"ğŸ”§ Qwen å‚æ•°è§„èŒƒåŒ–: {key} -> ä» dict æå– 'value': {value['value']}"
                    )

                # ğŸ”§ ç­–ç•¥ 3: ä¿ç•™åˆæ³•åµŒå¥—ç»“æ„
                # å·¥å…· schema ä¸­å®šä¹‰ä¸º object ç±»å‹çš„å‚æ•°ï¼Œåº”ä¿ç•™åŸå§‹ dict
                # åŒ…æ‹¬ï¼šç²¾ç¡®åŒ¹é…çš„å¸¸è§å­—æ®µ + åç¼€åŒ¹é…ï¼ˆ_config/_action/_params ç­‰ï¼‰
                elif key in (
                    "body", "parameters", "headers", "data", "json", "config",
                    "action", "trigger_config",
                ) or key.endswith((
                    "_config", "_action", "_params", "_data", "_options",
                    "_settings", "_metadata",
                )):
                    # è¿™äº›å­—æ®µé€šå¸¸åº”è¯¥ä¿ç•™ä¸ºå­—å…¸ï¼ˆæ˜¯åˆæ³•çš„åµŒå¥—ç»“æ„ï¼‰
                    normalized[key] = value
                    logger.debug(f"ğŸ”§ Qwen å‚æ•°è§„èŒƒåŒ–: {key} -> ä¿ç•™åŸå§‹ dictï¼ˆåˆæ³•åµŒå¥—ç»“æ„ï¼‰")

                # ğŸ”§ ç­–ç•¥ 4: å…¶ä»–å­—å…¸å‚æ•°ï¼Œé»˜è®¤ä¿ç•™åŸå§‹ç»“æ„
                # é¿å…é”™è¯¯æ‰å¹³åŒ–å·¥å…· schema ä¸­çš„åˆæ³•åµŒå¥—å¯¹è±¡
                else:
                    normalized[key] = value
                    if len(value) == 1:
                        logger.debug(
                            f"ğŸ”§ Qwen å‚æ•°è§„èŒƒåŒ–: {key} -> ä¿ç•™å•å­—æ®µ dictï¼ˆå­—æ®µ: {list(value.keys())}ï¼‰"
                        )
                    else:
                        logger.debug(
                            f"ğŸ”§ Qwen å‚æ•°è§„èŒƒåŒ–: {key} -> ä¿ç•™å¤šå­—æ®µ dictï¼ˆå­—æ®µ: {list(value.keys())}ï¼‰"
                        )

            elif isinstance(value, list):
                # é€’å½’å¤„ç†åˆ—è¡¨ä¸­çš„å…ƒç´ 
                normalized[key] = [
                    self._normalize_tool_input(item) if isinstance(item, dict) else item
                    for item in value
                ]
            else:
                # åŸºæœ¬ç±»å‹ç›´æ¥ä½¿ç”¨
                normalized[key] = value

        # ======== V2.1 å¢å¼ºï¼šå¤„ç† Qwen ç‰¹æ®Šè°ƒç”¨æ¨¡å¼ ========

        # ğŸ”§ ç­–ç•¥ 6: è‡ªåŠ¨å°† body è½¬æ¢ä¸º parametersï¼ˆé’ˆå¯¹ç®€åŒ– API è°ƒç”¨ï¼‰
        # Qwen æœ‰æ—¶ä¼šä½¿ç”¨ body è€Œä¸æ˜¯ parametersï¼Œä½†ç®€åŒ–è°ƒç”¨çš„æ ‡å‡†æ ¼å¼æ˜¯ parameters
        if "body" in normalized and "parameters" not in normalized and "api_name" in normalized:
            logger.info(
                f"ğŸ”„ Qwen å‚æ•°è§„èŒƒåŒ–: æ£€æµ‹åˆ°æ—§æ ¼å¼ 'body'ï¼Œè‡ªåŠ¨è½¬æ¢ä¸º 'parameters' (api_name={normalized.get('api_name')})"
            )
            normalized["parameters"] = normalized.pop("body")

        # ğŸ”§ ç­–ç•¥ 7: è¿‡æ»¤æ¡†æ¶æ³¨å…¥å­—æ®µï¼ˆLLM ä¸åº”è¯¥ä¼ é€’è¿™äº›ï¼‰
        # è¿™äº›å­—æ®µåº”è¯¥ç”±æ¡†æ¶åœ¨ ToolContext ä¸­æä¾›ï¼Œè€Œä¸æ˜¯ç”± LLM ä¼ é€’
        FRAMEWORK_FIELDS = {"user_id", "session_id", "conversation_id", "task_id"}
        filtered_fields = []
        for field in FRAMEWORK_FIELDS:
            if field in normalized:
                filtered_fields.append(field)
                del normalized[field]

        if filtered_fields:
            logger.info(
                f"ğŸ§¹ Qwen å‚æ•°è§„èŒƒåŒ–: è¿‡æ»¤æ¡†æ¶æ³¨å…¥å­—æ®µ {filtered_fields}ï¼ˆè¿™äº›å­—æ®µåº”ç”±æ¡†æ¶æä¾›ï¼Œä¸åº”ç”± LLM ä¼ é€’ï¼‰"
            )

        # ğŸ”§ ç­–ç•¥ 8: ç§»é™¤åº•å±‚ HTTP å‚æ•°ï¼ˆå¦‚æœä½¿ç”¨ç®€åŒ–è°ƒç”¨ï¼‰
        # å¦‚æœåŒæ—¶å­˜åœ¨ api_nameï¼ˆè¡¨ç¤ºç®€åŒ–è°ƒç”¨ï¼‰ï¼Œåˆ™ä¸åº”è¯¥æœ‰ path/method ç­‰åº•å±‚å‚æ•°
        if "api_name" in normalized and "parameters" in normalized:
            REMOVED_FIELDS = []
            for field in ["path", "url", "method"]:
                if field in normalized:
                    REMOVED_FIELDS.append(field)
                    del normalized[field]

            if REMOVED_FIELDS:
                logger.info(
                    f"ğŸ§¹ Qwen å‚æ•°è§„èŒƒåŒ–: ç§»é™¤åº•å±‚å‚æ•° {REMOVED_FIELDS}ï¼ˆç®€åŒ–è°ƒç”¨ä¸åº”åŒ…å«è¿™äº›å­—æ®µï¼‰"
                )

        # ğŸ”§ ç­–ç•¥ 9: API ç‰¹å®šçš„å‚æ•°åæ˜ å°„ï¼ˆè§£å†³ Qwen è¯­ä¹‰æ¨ç†é—®é¢˜ï¼‰
        # Qwen å€¾å‘äºæ ¹æ®è¯­ä¹‰æ¨ç†å‚æ•°åï¼Œè€Œéä¸¥æ ¼éµå®ˆ schema å®šä¹‰
        # ä¾‹å¦‚ï¼š"åˆ†æå­¦ç”Ÿæˆç»©" â†’ ä½¿ç”¨ "analysis_type" è€Œé schema è¦æ±‚çš„ "question"
        #
        # âš ï¸ é—®é¢˜æ ¹æºï¼šQwen çš„è®­ç»ƒç›®æ ‡æ˜¯"è¯­ä¹‰ä¼˜å…ˆ"ï¼ŒClaude æ˜¯"Schema First"
        # è¯¦è§ï¼šqwen_å‚æ•°åæ¨ç†ä¼˜å…ˆçº§åˆ†æ.md
        API_PARAM_MAPPING = {
            "wenshu_api": {
                # Qwen å¸¸ç”¨çš„è¯­ä¹‰æ¨ç†å‚æ•°å â†’ API å®é™…è¦æ±‚çš„å‚æ•°å
                "analysis_type": "question",  # "åˆ†æç±»å‹" â†’ "é—®é¢˜"
                "query": "question",  # "æŸ¥è¯¢" â†’ "é—®é¢˜"
                "prompt": "question",  # "æç¤º" â†’ "é—®é¢˜"
                "åˆ†æç±»å‹": "question",  # ä¸­æ–‡è¯­ä¹‰å
                "åˆ†æå†…å®¹": "question",  # ä¸­æ–‡è¯­ä¹‰å
                "é—®é¢˜": "question",  # ä¸­æ–‡ç›´è¯‘
                "content": "question",  # "å†…å®¹" â†’ "é—®é¢˜"
                # file_url é€šå¸¸æ­£ç¡®ï¼Œä¿ç•™æ˜ å°„ä½œä¸ºæ–‡æ¡£
                "file_url": "file_url",
            },
            # ğŸ”§ å¯åœ¨æ­¤ä¸ºå…¶ä»– API æ·»åŠ æ˜ å°„è§„åˆ™
            # "other_api": {
            #     "wrong_param": "correct_param",
            # }
        }

        if "api_name" in normalized and "parameters" in normalized:
            api_name = normalized["api_name"]
            if api_name in API_PARAM_MAPPING:
                mapping = API_PARAM_MAPPING[api_name]
                params = normalized["parameters"]
                fixed_count = 0
                fixed_details = []

                for wrong_name, correct_name in mapping.items():
                    # åªæœ‰å½“é”™è¯¯å‚æ•°åå­˜åœ¨ã€æ­£ç¡®å‚æ•°åä¸å­˜åœ¨ã€ä¸”ä¸¤è€…ä¸åŒæ—¶æ‰ä¿®å¤
                    if (
                        wrong_name in params
                        and correct_name not in params
                        and wrong_name != correct_name
                    ):
                        # æ‰§è¡Œå‚æ•°åä¿®å¤
                        params[correct_name] = params.pop(wrong_name)
                        fixed_count += 1
                        fixed_details.append(f"{wrong_name}â†’{correct_name}")
                        logger.info(
                            f"ğŸ”§ Qwen å‚æ•°åä¿®å¤: {wrong_name} â†’ {correct_name} "
                            f"(api={api_name}, é¿å…ç¬¬ä¸€æ¬¡è°ƒç”¨å¤±è´¥)"
                        )

                if fixed_count > 0:
                    logger.info(
                        f"âœ… æˆåŠŸä¿®å¤ {fixed_count} ä¸ªå‚æ•°å: [{', '.join(fixed_details)}]ï¼Œ"
                        f"é¿å…äº†é¢å¤–çš„ LLM è°ƒç”¨ï¼ˆèŠ‚çœçº¦ $0.015 + 8ç§’ï¼‰"
                    )

        # ğŸ”§ ç­–ç•¥ 10: API å¿…éœ€å‚æ•°æ™ºèƒ½è¡¥å…¨ï¼ˆè§£å†³ Qwen å‚æ•°ç¼ºå¤±é—®é¢˜ï¼‰
        # Qwen æœ‰æ—¶ä¼šé—æ¼å¿…éœ€å‚æ•°ï¼Œè®¤ä¸ºæŸäº›å‚æ•°"æ˜¾è€Œæ˜“è§"å¯ä»¥çœç•¥
        # ä¾‹å¦‚ï¼šè°ƒç”¨ wenshu_api åªä¼  file_urlï¼Œè®¤ä¸º"åˆ†ææ–‡ä»¶"çš„æ„å›¾å·²ç»å¾ˆæ˜ç¡®
        if "api_name" in normalized and "parameters" in normalized:
            api_name = normalized["api_name"]
            params = normalized["parameters"]

            # å®šä¹‰ API å¿…éœ€å‚æ•°çš„æ™ºèƒ½è¡¥å…¨è§„åˆ™
            # æ ¼å¼ï¼š{api_name: {required_param: default_value_generator}}
            API_REQUIRED_PARAMS = {
                "wenshu_api": {
                    "question": lambda p: (
                        # ç­–ç•¥ 1: å¦‚æœæœ‰ file_urlï¼Œç”Ÿæˆæ–‡ä»¶åˆ†ææç¤º
                        "è¯·åˆ†æè¿™ä¸ªæ–‡ä»¶çš„å†…å®¹"
                        if "file_url" in p
                        # ç­–ç•¥ 2: é»˜è®¤é€šç”¨åˆ†ææç¤º
                        else "è¯·åˆ†ææ•°æ®"
                    )
                },
                # ğŸ”§ å¯åœ¨æ­¤ä¸ºå…¶ä»– API æ·»åŠ å¿…éœ€å‚æ•°è¡¥å…¨è§„åˆ™
            }

            if api_name in API_REQUIRED_PARAMS:
                rules = API_REQUIRED_PARAMS[api_name]
                è¡¥å…¨_count = 0
                è¡¥å…¨_details = []

                for required_param, generator in rules.items():
                    # åªæœ‰å½“å¿…éœ€å‚æ•°ç¼ºå¤±æ—¶æ‰è¡¥å…¨
                    if required_param not in params:
                        # ç”Ÿæˆé»˜è®¤å€¼ï¼ˆæ”¯æŒåŠ¨æ€ç”Ÿæˆå™¨å‡½æ•°ï¼‰
                        default_value = generator(params) if callable(generator) else generator
                        params[required_param] = default_value
                        è¡¥å…¨_count += 1
                        è¡¥å…¨_details.append(f"{required_param}='{default_value}'")
                        logger.info(
                            f"ğŸ”§ Qwen å‚æ•°è¡¥å…¨: æ·»åŠ ç¼ºå¤±çš„å¿…éœ€å‚æ•° {required_param}='{default_value}' "
                            f"(api={api_name}, é¿å… HTTP 422 é”™è¯¯)"
                        )

                if è¡¥å…¨_count > 0:
                    logger.info(
                        f"âœ… æˆåŠŸè¡¥å…¨ {è¡¥å…¨_count} ä¸ªå¿…éœ€å‚æ•°: [{', '.join(è¡¥å…¨_details)}]ï¼Œ"
                        f"é¿å…äº†ç¬¬ä¸€æ¬¡è°ƒç”¨å¤±è´¥ï¼ˆèŠ‚çœçº¦ $0.015 + 8ç§’ï¼‰"
                    )

        return normalized

    def _parse_response(self, response) -> LLMResponse:
        """
        è§£æåƒé—®å“åº”ä¸ºç»Ÿä¸€æ ¼å¼

        Args:
            response: OpenAI æ ¼å¼çš„å“åº”

        Returns:
            LLMResponse
        """
        choice = response.choices[0]
        message = choice.message

        # æå–å†…å®¹
        content_text = message.content or ""
        thinking_text = getattr(message, "reasoning_content", None)

        # æå–å·¥å…·è°ƒç”¨
        tool_calls = []
        # ğŸ” è°ƒè¯•æ—¥å¿—
        logger.info(f"ğŸ” [DEBUG] message.tool_calls: {message.tool_calls}")
        if message.tool_calls:
            logger.info(f"ğŸ” [DEBUG] message.tool_calls æ•°é‡: {len(message.tool_calls)}")
            for tc in message.tool_calls:
                logger.info(
                    f"ğŸ” [DEBUG] å·¥å…·è°ƒç”¨: id={tc.id}, name={tc.function.name}, args={tc.function.arguments[:100] if tc.function.arguments else 'None'}"
                )

                # è§£æå‚æ•°å¹¶è§„èŒƒåŒ–
                input_dict = json.loads(tc.function.arguments) if tc.function.arguments else {}
                normalized_input = self._normalize_tool_input(input_dict)

                tool_calls.append(
                    {
                        "id": tc.id,
                        "name": tc.function.name,
                        "input": normalized_input,
                        "type": "tool_use",  # ğŸ”„ ç»Ÿä¸€è½¬æ¢ä¸º Claude æ ¼å¼ï¼ˆé€‚é…å™¨å±‚èŒè´£ï¼‰
                    }
                )
        else:
            # ğŸ”§ é™çº§ä¸º DEBUGï¼Œé¿å…å¥åº·æ¢æµ‹æ—¶çš„è¯¯æŠ¥
            logger.debug(f"message.tool_calls ä¸ºç©ºï¼ˆstop_reason={choice.finish_reason}ï¼‰")

        # Usage ä¿¡æ¯
        usage = {}
        if response.usage:
            usage = {
                "input_tokens": response.usage.prompt_tokens,
                "output_tokens": response.usage.completion_tokens,
                "thinking_tokens": 0,
            }

            # ä¼°ç®— thinking tokens
            if thinking_text:
                usage["thinking_tokens"] = self.count_tokens(thinking_text)

            # Token ä½¿ç”¨é‡æ—¥å¿—
            logger.info(
                f"ğŸ“Š Token ä½¿ç”¨: input={usage['input_tokens']:,}, "
                f"output={usage['output_tokens']:,}, "
                f"thinking={usage['thinking_tokens']:,}"
            )

        # ğŸ”„ ç»Ÿä¸€è½¬æ¢ stop_reasonï¼ˆQwen -> Claude æ ¼å¼ï¼‰
        # OpenAI/Qwen: "tool_calls" -> Claude: "tool_use"
        stop_reason = choice.finish_reason
        if stop_reason == "tool_calls" or (tool_calls and stop_reason == "stop"):
            stop_reason = "tool_use"
            logger.debug("ğŸ”„ è½¬æ¢ stop_reason: tool_calls -> tool_use")

        # æ„å»º raw_content
        raw_content = []
        if thinking_text:
            raw_content.append({"type": "thinking", "thinking": thinking_text})
        if content_text:
            raw_content.append({"type": "text", "text": content_text})
        for tc in tool_calls:
            raw_content.append(
                {"type": "tool_use", "id": tc["id"], "name": tc["name"], "input": tc["input"]}
            )

        return LLMResponse(
            content=content_text,
            thinking=thinking_text,
            tool_calls=tool_calls if tool_calls else None,
            stop_reason=stop_reason,  # âœ… å·²è½¬æ¢ä¸º Claude æ ¼å¼
            usage=usage,
            model=self.config.model,  # ğŸ†• å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°
            raw_content=raw_content,
        )


# ============================================================
# å·¥å‚å‡½æ•°
# ============================================================


def create_qwen_service(
    model: str = "qwen3-max",
    api_key: Optional[str] = None,
    region: str = "cn-beijing",
    base_url: Optional[str] = None,
    enable_thinking: bool = False,
    **kwargs,
) -> QwenLLMService:
    """
    åˆ›å»ºåƒé—®æœåŠ¡çš„ä¾¿æ·å‡½æ•°

    Args:
        model: æ¨¡å‹åç§°ï¼ˆqwen3-max æˆ– qwen-plusï¼‰
        api_key: API å¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        region: åœ°åŸŸï¼ˆcn-beijing, singapore, us-virginia, financeï¼‰
        base_url: è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆä¼˜å…ˆçº§é«˜äº regionï¼‰
        enable_thinking: å¯ç”¨æ·±åº¦æ€è€ƒ
        **kwargs: å…¶ä»–é…ç½®å‚æ•°

    Returns:
        QwenLLMService å®ä¾‹

    ç¤ºä¾‹ï¼š
        # qwen3-max: å¯¹æ ‡ claude-sonnet-4-5ï¼ˆæ——èˆ°æ¨¡å‹ï¼‰
        llm = create_qwen_service(
            model="qwen3-max",
            enable_thinking=True
        )

        # qwen-plus: å¯¹æ ‡ claude-haiku-4-5ï¼ˆå¿«é€Ÿæ¨¡å‹ï¼‰
        llm = create_qwen_service(
            model="qwen-plus",
            enable_thinking=False
        )

        # è‡ªå®šä¹‰ç«¯ç‚¹ï¼ˆä½¿ç”¨ä»£ç†ï¼‰
        llm = create_qwen_service(
            model="qwen3-max-2026-01-23",
            base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
        )
    """
    if api_key is None:
        api_key = os.getenv("DASHSCOPE_API_KEY")

    config = QwenConfig(
        provider=LLMProvider.QWEN,
        model=model,
        api_key=api_key,
        region=region,
        base_url=base_url,
        enable_thinking=enable_thinking,
        **kwargs,
    )

    return QwenLLMService(config)


# ============================================================
# æ³¨å†Œåˆ° LLMRegistry
# ============================================================


def _register_qwen():
    """å»¶è¿Ÿæ³¨å†Œ Qwen Providerï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰"""
    from .registry import LLMRegistry

    LLMRegistry.register(
        name="qwen",
        service_class=QwenLLMService,
        adaptor_class=OpenAIAdaptor,  # åƒé—®ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        default_model="qwen3-max",
        api_key_env="DASHSCOPE_API_KEY",
        config_class=QwenConfig,
        display_name="é€šä¹‰åƒé—®",
        description="é˜¿é‡Œäº‘é€šä¹‰åƒé—®ç³»åˆ—æ¨¡å‹",
        supported_features=[
            "streaming",
            "tool_calling",
            "thinking",
            "vision",
            "audio",
        ],
    )


# æ¨¡å—åŠ è½½æ—¶æ³¨å†Œ
_register_qwen()
