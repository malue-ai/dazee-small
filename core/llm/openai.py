"""
OpenAI LLM æœåŠ¡å®ç°

åŸºäº OpenAI SDK å®ç°ï¼Œæ”¯æŒ OpenAI å®˜æ–¹ API åŠå…¼å®¹æ¥å£ã€‚

æ”¯æŒçš„åŠŸèƒ½ï¼š
- åŸºç¡€å¯¹è¯ï¼ˆæµå¼/éæµå¼ï¼‰
- Function Callingï¼ˆå·¥å…·è°ƒç”¨ï¼‰
- ç»“æ„åŒ–è¾“å‡ºï¼ˆresponse_formatï¼‰

å‚è€ƒæ–‡æ¡£ï¼š
- https://platform.openai.com/docs/api-reference/chat
"""

import json
import os
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Union

import httpx
from openai import AsyncOpenAI

from infra.resilience import with_retry
from logger import get_logger

from .adaptor import OpenAIAdaptor
from .base import (
    BaseLLMService,
    LLMConfig,
    LLMResponse,
    Message,
    ToolType,
)

logger = get_logger("llm.openai")

# è¯¦ç»†æ—¥å¿—å¼€å…³
LLM_DEBUG_VERBOSE = os.getenv("LLM_DEBUG_VERBOSE", "").lower() in ("1", "true", "yes")

# æ”¯æŒéŸ³é¢‘è¾“å…¥/è¾“å‡ºçš„æ¨¡å‹
OPENAI_AUDIO_MODELS = {
    "gpt-audio",
    "gpt-4o-audio-preview",
    "gpt-4o-audio-preview-2024-12-17",
}


def _is_audio_model(model: str) -> bool:
    """Check if the model supports audio input/output."""
    return any(m in model for m in OPENAI_AUDIO_MODELS)


# ============================================================
# OpenAI LLM æœåŠ¡
# ============================================================


class OpenAILLMService(BaseLLMService):
    """
    OpenAI LLM æœåŠ¡å®ç°

    æ”¯æŒ OpenAI å®˜æ–¹ API åŠå…¼å®¹æ¥å£ï¼ˆå¦‚ DeepSeekï¼‰ã€‚

    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    config = LLMConfig(
        provider=LLMProvider.OPENAI,
        model="gpt-4o",
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    llm = OpenAILLMService(config)

    response = await llm.create_message_async(
        messages=[Message(role="user", content="ä½ å¥½")],
        system="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"
    )
    ```
    """

    def __init__(self, config: LLMConfig):
        """
        åˆå§‹åŒ– OpenAI æœåŠ¡

        Args:
            config: LLM é…ç½®
        """
        self.config = config

        # æ¶ˆæ¯é€‚é…å™¨
        self._adaptor = OpenAIAdaptor()

        # API Key
        api_key = self.config.api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "OpenAI API Key æœªè®¾ç½®ã€‚è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡æˆ–ä¼ å…¥ api_key å‚æ•°"
            )

        # API ç«¯ç‚¹
        base_url = self.config.base_url or "https://api.openai.com/v1"

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        timeout = getattr(self.config, "timeout", 120.0)
        max_retries = getattr(self.config, "max_retries", 3)

        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=timeout,
            max_retries=max_retries,
        )

        # è‡ªå®šä¹‰å·¥å…·å­˜å‚¨
        self._custom_tools: List[Dict[str, Any]] = []

        logger.info(f"âœ… OpenAI æœåŠ¡åˆå§‹åŒ–æˆåŠŸ: model={self.config.model}, base_url={base_url}")

    # ============================================================
    # è‡ªå®šä¹‰å·¥å…·ç®¡ç†
    # ============================================================

    def add_custom_tool(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:
        """
        æ·»åŠ è‡ªå®šä¹‰å·¥å…·

        Args:
            name: å·¥å…·åç§°
            description: å·¥å…·æè¿°
            input_schema: è¾“å…¥å‚æ•° schema
        """
        for i, tool in enumerate(self._custom_tools):
            if tool["name"] == name:
                self._custom_tools[i] = {
                    "name": name,
                    "description": description,
                    "input_schema": input_schema,
                }
                logger.debug(f"æ›´æ–°è‡ªå®šä¹‰å·¥å…·: {name}")
                return

        self._custom_tools.append(
            {"name": name, "description": description, "input_schema": input_schema}
        )
        logger.debug(f"æ³¨å†Œè‡ªå®šä¹‰å·¥å…·: {name}")

    def remove_custom_tool(self, name: str) -> bool:
        """ç§»é™¤è‡ªå®šä¹‰å·¥å…·"""
        for i, tool in enumerate(self._custom_tools):
            if tool["name"] == name:
                self._custom_tools.pop(i)
                logger.debug(f"ç§»é™¤è‡ªå®šä¹‰å·¥å…·: {name}")
                return True
        return False

    def get_custom_tools(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰è‡ªå®šä¹‰å·¥å…·"""
        return self._custom_tools.copy()

    def clear_custom_tools(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰è‡ªå®šä¹‰å·¥å…·"""
        self._custom_tools.clear()
        logger.debug("æ¸…ç©ºæ‰€æœ‰è‡ªå®šä¹‰å·¥å…·")

    def _format_tools(self, tools: List[Union[ToolType, str, Dict]]) -> List[Dict[str, Any]]:
        """
        æ ¼å¼åŒ–å·¥å…·åˆ—è¡¨ä¸º OpenAI æ ¼å¼
        """
        formatted = []

        for tool in tools:
            if isinstance(tool, ToolType):
                # OpenAI æ²¡æœ‰åŸç”Ÿå·¥å…·ï¼Œè·³è¿‡
                logger.warning(f"OpenAI ä¸æ”¯æŒ ToolType æšä¸¾: {tool}ï¼Œå·²è·³è¿‡")
                continue

            elif isinstance(tool, str):
                # ä»è‡ªå®šä¹‰å·¥å…·ä¸­æŸ¥æ‰¾
                for custom_tool in self._custom_tools:
                    if custom_tool.get("name") == tool:
                        formatted.append(self._convert_tool_to_openai_format(custom_tool))
                        break
                else:
                    logger.warning(f"æœªæ‰¾åˆ°å·¥å…·: {tool}")

            elif isinstance(tool, dict):
                formatted.append(self._convert_tool_to_openai_format(tool))

        return formatted

    def _convert_tool_to_openai_format(self, tool: Dict[str, Any]) -> Dict[str, Any]:
        """
        è½¬æ¢å·¥å…·ä¸º OpenAI Function Calling æ ¼å¼
        """
        if tool.get("type") == "function":
            return tool

        return {
            "type": "function",
            "function": {
                "name": tool.get("name", ""),
                "description": tool.get("description", ""),
                "parameters": tool.get("input_schema", {}),
            },
        }

    @staticmethod
    def _normalize_tool_choice(tool_choice: Any) -> Any:
        """Convert Claude-style tool_choice to OpenAI-compatible format.

        Claude format:  {"type": "tool", "name": "func_name"}
        OpenAI format:  {"type": "function", "function": {"name": "func_name"}}
        """
        if isinstance(tool_choice, str):
            return tool_choice
        if isinstance(tool_choice, dict) and tool_choice.get("type") == "tool":
            return {
                "type": "function",
                "function": {"name": tool_choice["name"]},
            }
        return tool_choice

    # ============================================================
    # æ ¸å¿ƒ API æ–¹æ³•
    # ============================================================

    @with_retry(
        max_retries=3,
        base_delay=1.0,
        retryable_errors=(
            httpx.RemoteProtocolError,
            httpx.ConnectError,
            httpx.TimeoutException,
        ),
    )
    async def create_message_async(
        self,
        messages: List[Message],
        system: Optional[Union[str, List[Dict[str, Any]]]] = None,
        tools: Optional[List[Union[ToolType, str, Dict]]] = None,
        is_probe: bool = False,
        **kwargs,
    ) -> LLMResponse:
        """
        åˆ›å»ºæ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·åˆ—è¡¨
            is_probe: æ˜¯å¦ä¸ºæ¢æµ‹è¯·æ±‚
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMResponse å“åº”å¯¹è±¡
        """
        # ä½¿ç”¨ adaptor è½¬æ¢æ¶ˆæ¯
        converted = self._adaptor.convert_messages_to_provider(messages)
        openai_messages = converted["messages"]

        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": self.config.model,
            "messages": openai_messages,
            "temperature": kwargs.get("temperature", self.config.temperature),
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "stream": False,
        }

        # éŸ³é¢‘æ¨¡å‹å‚æ•°
        if _is_audio_model(self.config.model):
            request_params["modalities"] = ["text", "audio"]
            request_params["audio"] = {
                "voice": kwargs.get("audio_voice", "alloy"),
                "format": kwargs.get("audio_format", "wav"),
            }

        # System prompt
        if system:
            if isinstance(system, list):
                system_text = "\n".join(
                    block.get("text", "") for block in system if block.get("type") == "text"
                )
                request_params["messages"].insert(0, {"role": "system", "content": system_text})
            elif isinstance(system, dict):
                system_text = (
                    system.get("text", "") if system.get("type") == "text" else str(system)
                )
                request_params["messages"].insert(0, {"role": "system", "content": system_text})
            else:
                request_params["messages"].insert(0, {"role": "system", "content": str(system)})

        # Tools
        all_tools = []
        tool_names_seen = set()

        if tools:
            for tool in self._format_tools(tools):
                tool_name = tool.get("function", {}).get("name", "")
                if tool_name and tool_name not in tool_names_seen:
                    all_tools.append(tool)
                    tool_names_seen.add(tool_name)

        for custom_tool in self._custom_tools:
            tool_name = custom_tool.get("name", "")
            if tool_name and tool_name not in tool_names_seen:
                all_tools.append(self._convert_tool_to_openai_format(custom_tool))
                tool_names_seen.add(tool_name)

        if all_tools:
            request_params["tools"] = all_tools
            request_params["tool_choice"] = self._normalize_tool_choice(
                kwargs.get("tool_choice", "auto")
            )

        logger.debug(f"ğŸ“¤ OpenAI è¯·æ±‚: model={self.config.model}, messages={len(openai_messages)}")

        if LLM_DEBUG_VERBOSE:
            logger.info("=" * 80)
            logger.info("ğŸ” [DEBUG-ASYNC] å®Œæ•´ request_params:")
            logger.info(f"   model: {request_params.get('model')}")
            logger.info(f"   messages: {len(request_params.get('messages', []))}")
            logger.info("=" * 80)

        # API è°ƒç”¨
        try:
            response = await self.client.chat.completions.create(**request_params)
        except Exception as e:
            if not is_probe:
                logger.error(f"OpenAI API è°ƒç”¨å¤±è´¥: {e}")
            raise

        # è½¬æ¢å“åº”ï¼ˆå«éŸ³é¢‘è¾“å‡ºå¤„ç†ï¼‰
        return self._parse_response(response)

    async def create_message_stream(
        self,
        messages: List[Message],
        system: Optional[Union[str, List[Dict[str, Any]]]] = None,
        tools: Optional[List[Union[ToolType, str, Dict]]] = None,
        on_thinking: Optional[Callable[[str], None]] = None,
        on_content: Optional[Callable[[str], None]] = None,
        on_tool_call: Optional[Callable[[Dict], None]] = None,
        **kwargs,
    ) -> AsyncIterator[LLMResponse]:
        """
        åˆ›å»ºæ¶ˆæ¯ï¼ˆæµå¼ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·åˆ—è¡¨
            on_thinking: thinking å›è°ƒï¼ˆæ¨ç†æ¨¡å‹é€šè¿‡ reasoning_content è¿”å›ï¼‰
            on_content: content å›è°ƒ
            on_tool_call: tool_call å›è°ƒ
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            LLMResponse ç‰‡æ®µ
        """
        # ä½¿ç”¨ adaptor è½¬æ¢æ¶ˆæ¯
        converted = self._adaptor.convert_messages_to_provider(messages)
        openai_messages = converted["messages"]

        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": self.config.model,
            "messages": openai_messages,
            "temperature": kwargs.get("temperature", self.config.temperature),
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "stream": True,
            "stream_options": {"include_usage": True},
        }

        # éŸ³é¢‘æ¨¡å‹å‚æ•°
        if _is_audio_model(self.config.model):
            request_params["modalities"] = ["text", "audio"]
            request_params["audio"] = {
                "voice": kwargs.get("audio_voice", "alloy"),
                "format": kwargs.get("audio_format", "wav"),
            }

        # System prompt
        if system:
            if isinstance(system, list):
                system_text = "\n".join(
                    block.get("text", "") for block in system if block.get("type") == "text"
                )
                request_params["messages"].insert(0, {"role": "system", "content": system_text})
            elif isinstance(system, dict):
                system_text = (
                    system.get("text", "") if system.get("type") == "text" else str(system)
                )
                request_params["messages"].insert(0, {"role": "system", "content": system_text})
            else:
                # å­—ç¬¦ä¸²æ ¼å¼
                request_params["messages"].insert(0, {"role": "system", "content": str(system)})

        # Tools
        all_tools = []
        tool_names_seen = set()

        if tools:
            for tool in self._format_tools(tools):
                tool_name = tool.get("function", {}).get("name", "")
                if tool_name and tool_name not in tool_names_seen:
                    all_tools.append(tool)
                    tool_names_seen.add(tool_name)

        for custom_tool in self._custom_tools:
            tool_name = custom_tool.get("name", "")
            if tool_name and tool_name not in tool_names_seen:
                all_tools.append(self._convert_tool_to_openai_format(custom_tool))
                tool_names_seen.add(tool_name)

        if all_tools:
            request_params["tools"] = all_tools
            request_params["tool_choice"] = self._normalize_tool_choice(
                kwargs.get("tool_choice", "auto")
            )

        logger.info(
            f"ğŸ“¤ OpenAI æµå¼è¯·æ±‚: model={self.config.model}, messages={len(openai_messages)}"
        )

        # ç´¯ç§¯å˜é‡
        accumulated_content = ""
        accumulated_thinking = ""
        accumulated_audio_data = ""
        tool_calls = []
        stop_reason = None
        usage = {}

        try:
            stream = await self.client.chat.completions.create(**request_params)

            async for chunk in stream:
                if not chunk.choices:
                    # æœ€åä¸€ä¸ª chunkï¼ˆåŒ…å« usageï¼‰
                    if chunk.usage:
                        usage = {
                            "input_tokens": chunk.usage.prompt_tokens,
                            "output_tokens": chunk.usage.completion_tokens,
                        }
                        # æå– reasoning tokensï¼ˆå¦‚æœæœ‰ï¼‰
                        if hasattr(chunk.usage, "completion_tokens_details"):
                            details = chunk.usage.completion_tokens_details
                            reasoning_tokens = getattr(details, "reasoning_tokens", 0) if details else 0
                            if reasoning_tokens:
                                usage["thinking_tokens"] = reasoning_tokens
                        logger.info(
                            f"ğŸ“Š Token ä½¿ç”¨: input={usage['input_tokens']:,}, "
                            f"output={usage['output_tokens']:,}"
                        )
                    continue

                choice = chunk.choices[0]
                delta = choice.delta

                # å¤„ç†æ€è€ƒå†…å®¹ï¼ˆOpenAI æ¨ç†æ¨¡å‹é€šè¿‡ reasoning_content è¿”å›ï¼‰
                if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                    accumulated_thinking += delta.reasoning_content
                    if on_thinking:
                        on_thinking(delta.reasoning_content)
                    yield LLMResponse(
                        content="",
                        thinking=delta.reasoning_content,
                        model=self.config.model,
                        is_stream=True,
                    )

                # å¤„ç†æ™®é€šå†…å®¹
                if delta.content:
                    accumulated_content += delta.content
                    if on_content:
                        on_content(delta.content)
                    yield LLMResponse(
                        content=delta.content, model=self.config.model, is_stream=True
                    )

                # å¤„ç†éŸ³é¢‘è¾“å‡ºï¼ˆOpenAI éŸ³é¢‘æ¨¡å‹é€šè¿‡ delta.audio è¿”å›æµå¼éŸ³é¢‘ï¼‰
                if hasattr(delta, "audio") and delta.audio:
                    audio_chunk = getattr(delta.audio, "data", None)
                    if audio_chunk:
                        accumulated_audio_data += audio_chunk

                    audio_transcript = getattr(delta.audio, "transcript", None)
                    if audio_transcript:
                        accumulated_content += audio_transcript
                        if on_content:
                            on_content(audio_transcript)
                        yield LLMResponse(
                            content=audio_transcript, model=self.config.model, is_stream=True
                        )

                # å¤„ç†å·¥å…·è°ƒç”¨
                if delta.tool_calls:
                    for tool_call in delta.tool_calls:
                        index = tool_call.index

                        # ç¡®ä¿ tool_calls åˆ—è¡¨è¶³å¤Ÿé•¿
                        while len(tool_calls) <= index:
                            tool_calls.append(
                                {"id": "", "name": "", "arguments": "", "type": "function"}
                            )

                        # ç´¯ç§¯å­—æ®µ
                        if tool_call.id:
                            tool_calls[index]["id"] = tool_call.id

                            # ğŸ†• Tool Use Start äº‹ä»¶ï¼ˆæµå¼ï¼‰
                            yield LLMResponse(
                                content="",
                                model=self.config.model,
                                is_stream=True,
                                tool_use_start={
                                    "type": "tool_use",
                                    "id": tool_call.id,
                                    "name": tool_call.function.name if tool_call.function else "",
                                },
                            )

                        if tool_call.function:
                            if tool_call.function.name:
                                tool_calls[index]["name"] = tool_call.function.name
                            if tool_call.function.arguments:
                                tool_calls[index]["arguments"] += tool_call.function.arguments

                                # ğŸ†• Input Delta äº‹ä»¶ï¼ˆæµå¼ï¼‰
                                yield LLMResponse(
                                    content="",
                                    model=self.config.model,
                                    is_stream=True,
                                    input_delta=tool_call.function.arguments,
                                )

                        # å›è°ƒ
                        if on_tool_call:
                            on_tool_call(
                                {
                                    "id": tool_call.id,
                                    "name": tool_call.function.name if tool_call.function else "",
                                    "arguments": (
                                        tool_call.function.arguments if tool_call.function else ""
                                    ),
                                }
                            )

                # åœæ­¢åŸå› 
                if choice.finish_reason:
                    stop_reason = choice.finish_reason

            # å¤„ç†ç´¯ç§¯çš„å·¥å…·è°ƒç”¨
            formatted_tool_calls = []
            for tc in tool_calls:
                if tc.get("name"):
                    try:
                        input_dict = (
                            json.loads(tc["arguments"], strict=False) if tc["arguments"] else {}
                        )
                        formatted_tool_calls.append(
                            {
                                "id": tc["id"],
                                "name": tc["name"],
                                "input": input_dict,
                                "type": "tool_use",
                            }
                        )
                    except json.JSONDecodeError as e:
                        logger.error(f"âŒ å·¥å…·è°ƒç”¨å‚æ•°è§£æå¤±è´¥: {e}")

            # æ„å»º raw_content
            raw_content = []
            if accumulated_thinking:
                raw_content.append({"type": "thinking", "thinking": accumulated_thinking})
            if accumulated_content:
                raw_content.append({"type": "text", "text": accumulated_content})
            for tc in formatted_tool_calls:
                raw_content.append(
                    {"type": "tool_use", "id": tc["id"], "name": tc["name"], "input": tc["input"]}
                )

            logger.info(f"ğŸ“¥ OpenAI å“åº”: stop_reason={stop_reason or 'stop'}")

            if stop_reason == "tool_calls" or (formatted_tool_calls and stop_reason == "stop"):
                stop_reason = "tool_use"

            # æ„å»ºéŸ³é¢‘è¾“å‡ºæ•°æ®
            audio_data = None
            if accumulated_audio_data:
                audio_data = {
                    "data": accumulated_audio_data,
                    "transcript": accumulated_content,
                    "format": "wav",
                }
                logger.info(
                    f"ğŸµ OpenAI éŸ³é¢‘è¾“å‡º: data_len={len(accumulated_audio_data)}"
                )

            # è¿”å›æœ€ç»ˆå“åº”
            yield LLMResponse(
                content=accumulated_content,
                thinking=accumulated_thinking if accumulated_thinking else None,
                tool_calls=formatted_tool_calls if formatted_tool_calls else None,
                stop_reason=stop_reason or "stop",
                usage=usage if usage else None,
                model=self.config.model,
                raw_content=raw_content,
                audio_data=audio_data,
                is_stream=False,
            )

        except Exception as e:
            logger.error(f"OpenAI æµå¼ä¼ è¾“é”™è¯¯: {e}")
            raise

    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®— token æ•°é‡

        TODO: OpenAI å¯ä»¥ä½¿ç”¨ tiktoken ç²¾ç¡®è®¡ç®—ï¼ˆå·²åœ¨çˆ¶ç±»å®ç°ï¼‰
        - tiktoken æ˜¯ OpenAI å®˜æ–¹çš„ tokenizer
        - cl100k_base é€‚ç”¨äº GPT-4 ç³»åˆ—

        å½“å‰ä½¿ç”¨çˆ¶ç±»çš„ tiktoken å®ç°ã€‚

        Args:
            text: è¦è®¡ç®—çš„æ–‡æœ¬

        Returns:
            token æ•°é‡
        """
        # OpenAI ç›´æ¥ä½¿ç”¨ tiktokenï¼ˆçˆ¶ç±»å®ç°ï¼‰å³å¯
        return super().count_tokens(text)

    def _parse_response(self, response) -> LLMResponse:
        """
        è§£æ OpenAI å“åº”ä¸ºç»Ÿä¸€æ ¼å¼
        """
        choice = response.choices[0]
        message = choice.message

        content_text = message.content or ""

        # æå–æ€è€ƒå†…å®¹ï¼ˆOpenAI æ¨ç†æ¨¡å‹é€šè¿‡ reasoning_content è¿”å›ï¼‰
        thinking_text = getattr(message, "reasoning_content", None)

        # æå–éŸ³é¢‘è¾“å‡ºï¼ˆéŸ³é¢‘æ¨¡å‹é€šè¿‡ message.audio è¿”å›ï¼‰
        audio_data = None
        if hasattr(message, "audio") and message.audio:
            audio_data = {
                "data": getattr(message.audio, "data", ""),
                "transcript": getattr(message.audio, "transcript", ""),
                "format": getattr(message.audio, "format", "wav"),
                "id": getattr(message.audio, "id", ""),
                "expires_at": getattr(message.audio, "expires_at", None),
            }
            if audio_data["transcript"] and not content_text:
                content_text = audio_data["transcript"]
            logger.info(
                f"ğŸµ æ”¶åˆ°éŸ³é¢‘è¾“å‡º: format={audio_data['format']}, "
                f"transcript_len={len(audio_data.get('transcript', ''))}"
            )

        # æå–å·¥å…·è°ƒç”¨
        tool_calls = []
        if message.tool_calls:
            for tc in message.tool_calls:
                input_dict = json.loads(tc.function.arguments) if tc.function.arguments else {}
                tool_calls.append(
                    {"id": tc.id, "name": tc.function.name, "input": input_dict, "type": "tool_use"}
                )

        # Usage ä¿¡æ¯
        usage = {}
        if response.usage:
            usage = {
                "input_tokens": response.usage.prompt_tokens,
                "output_tokens": response.usage.completion_tokens,
            }
            if hasattr(response.usage, "completion_tokens_details"):
                details = response.usage.completion_tokens_details
                reasoning_tokens = getattr(details, "reasoning_tokens", 0) if details else 0
                if reasoning_tokens:
                    usage["thinking_tokens"] = reasoning_tokens
            logger.info(
                f"ğŸ“Š Token ä½¿ç”¨: input={usage['input_tokens']:,}, "
                f"output={usage['output_tokens']:,}"
            )

        stop_reason = choice.finish_reason
        if stop_reason == "tool_calls" or (tool_calls and stop_reason == "stop"):
            stop_reason = "tool_use"

        # æ„å»º raw_content
        raw_content = []
        if thinking_text:
            raw_content.append({"type": "thinking", "thinking": thinking_text})
        if content_text:
            raw_content.append({"type": "text", "text": content_text})
        for tc in tool_calls:
            raw_content.append(
                {"type": "tool_use", "id": tc["id"], "name": tc["name"], "input": tc["input"]}
            )

        llm_response = LLMResponse(
            content=content_text,
            thinking=thinking_text,
            tool_calls=tool_calls if tool_calls else None,
            stop_reason=stop_reason,
            usage=usage,
            model=self.config.model,
            raw_content=raw_content,
        )

        if audio_data:
            llm_response.audio_data = audio_data

        return llm_response


# ============================================================
# æ³¨å†Œåˆ° LLMRegistry
# ============================================================


def _register_openai():
    """å»¶è¿Ÿæ³¨å†Œ OpenAI Providerï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰"""
    from .registry import LLMRegistry

    LLMRegistry.register(
        name="openai",
        service_class=OpenAILLMService,
        adaptor_class=OpenAIAdaptor,
        default_model="gpt-4o",
        api_key_env="OPENAI_API_KEY",
        display_name="OpenAI",
        description="OpenAI GPT ç³»åˆ—æ¨¡å‹",
        supported_features=[
            "streaming",
            "tool_calling",
            "function_calling",
            "thinking",
        ],
    )


# æ¨¡å—åŠ è½½æ—¶æ³¨å†Œ
_register_openai()
