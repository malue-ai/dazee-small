"""
å…¨å±€æ¨¡å‹æ³¨å†Œä¸­å¿ƒ

æä¾›æŒ‰æ¨¡å‹ç»´åº¦çš„æ³¨å†Œå’Œç®¡ç†ï¼Œä¸ LLMRegistryï¼ˆProvider ç»´åº¦ï¼‰é…åˆä½¿ç”¨ã€‚
æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹çš„ YAML æŒä¹…åŒ–ï¼ˆconfig/custom_models.yamlï¼‰ã€‚

åŒå±‚æ¶æ„ï¼š
- Provider å±‚ï¼ˆLLMRegistryï¼‰ï¼šå®šä¹‰"å¦‚ä½•è°ƒç”¨"ï¼ˆæœåŠ¡ç±» + é€‚é…å™¨ï¼‰
- Model å±‚ï¼ˆModelRegistryï¼‰ï¼šå®šä¹‰"å…·ä½“é…ç½®"ï¼ˆendpointã€èƒ½åŠ›ã€ç±»å‹ï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
```python
from core.llm.model_registry import ModelRegistry, ModelType

# é€šè¿‡æ¨¡å‹ååˆ›å»ºæœåŠ¡
llm = ModelRegistry.create_service("gpt-4o")
embedder = ModelRegistry.create_service("bge-m3")

# æŸ¥è¯¢å¯ç”¨æ¨¡å‹
llm_models = ModelRegistry.list_models(model_type=ModelType.LLM)
embedding_models = ModelRegistry.list_models(model_type=ModelType.EMBEDDING)
```
"""

import os
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

import aiofiles
import yaml

from logger import get_logger

if TYPE_CHECKING:
    from .base import BaseLLMService

logger = get_logger("llm.model_registry")


# ============================================================
# æšä¸¾å®šä¹‰
# ============================================================


class ModelType(Enum):
    """
    æ¨¡å‹ç±»å‹

    ç”¨äºåŒºåˆ†ä¸åŒç”¨é€”çš„æ¨¡å‹ï¼Œä¾¿äºæŸ¥è¯¢å’Œç®¡ç†ã€‚
    """

    LLM = "llm"  # çº¯æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹
    VLM = "vlm"  # è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆæ”¯æŒå›¾åƒè¾“å…¥ï¼‰
    EMBEDDING = "embedding"  # åµŒå…¥æ¨¡å‹
    RERANK = "rerank"  # é‡æ’åºæ¨¡å‹
    TTS = "tts"  # æ–‡æœ¬è½¬è¯­éŸ³
    STT = "stt"  # è¯­éŸ³è½¬æ–‡æœ¬
    AUDIO = "audio"  # éŸ³é¢‘ç†è§£/ç”Ÿæˆ


class AdapterType(Enum):
    """
    é€‚é…å™¨ç±»å‹

    å†³å®šæ¶ˆæ¯æ ¼å¼è½¬æ¢æ–¹å¼ï¼Œä¸åŒ Provider å¯èƒ½ä½¿ç”¨ç›¸åŒçš„é€‚é…å™¨ã€‚
    ä¾‹å¦‚ï¼šQwen ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼Œæ‰€ä»¥ adapter=OPENAIã€‚
    """

    OPENAI = "openai"  # OpenAI å…¼å®¹æ ¼å¼ï¼ˆOpenAIã€Qwenã€DeepSeek ç­‰ï¼‰
    CLAUDE = "claude"  # Anthropic Claude æ ¼å¼
    GEMINI = "gemini"  # Google Gemini æ ¼å¼


# ============================================================
# æ•°æ®ç±»
# ============================================================


@dataclass
class ModelPricing:
    """
    æ¨¡å‹å®šä»·ä¿¡æ¯ï¼ˆç¾å…ƒ / ç™¾ä¸‡ tokenï¼‰

    ç”¨äºå®æ—¶è´¹ç”¨ä¼°ç®—å’Œ HITL è´¹ç”¨é¢„è­¦ã€‚
    None è¡¨ç¤ºå…è´¹æˆ–æœªçŸ¥ï¼ˆå¦‚ç§æœ‰åŒ–éƒ¨ç½²ï¼‰ã€‚
    """

    input_per_million: Optional[float] = None  # è¾“å…¥ $/M tokens
    output_per_million: Optional[float] = None  # è¾“å‡º $/M tokens
    cache_read_per_million: Optional[float] = None  # ç¼“å­˜è¯»å– $/M tokens
    cache_write_per_million: Optional[float] = None  # ç¼“å­˜å†™å…¥ $/M tokens

    @property
    def is_free(self) -> bool:
        """Whether pricing is zero or unknown (private deployment)."""
        return (
            not self.input_per_million
            and not self.output_per_million
        )

    def estimate_cost(
        self,
        input_tokens: int = 0,
        output_tokens: int = 0,
        cache_read_tokens: int = 0,
        cache_write_tokens: int = 0,
    ) -> Optional[float]:
        """
        Estimate cost in USD based on token counts.

        Returns:
            Estimated cost in USD, or None if pricing is unknown.
        """
        if self.is_free:
            return None

        cost = 0.0
        if self.input_per_million:
            cost += input_tokens * self.input_per_million / 1_000_000
        if self.output_per_million:
            cost += output_tokens * self.output_per_million / 1_000_000
        if self.cache_read_per_million:
            cost += cache_read_tokens * self.cache_read_per_million / 1_000_000
        if self.cache_write_per_million:
            cost += cache_write_tokens * self.cache_write_per_million / 1_000_000
        return cost


@dataclass
class ModelCapabilities:
    """
    æ¨¡å‹èƒ½åŠ›é…ç½®

    æè¿°æ¨¡å‹æ”¯æŒçš„åŠŸèƒ½å’Œé™åˆ¶ã€‚
    """

    supports_tools: bool = True  # æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
    supports_vision: bool = False  # æ˜¯å¦æ”¯æŒå›¾åƒè¾“å…¥
    supports_thinking: bool = False  # æ˜¯å¦æ”¯æŒæ·±åº¦æ€è€ƒï¼ˆClaude/Qwenï¼‰
    supports_audio: bool = False  # æ˜¯å¦æ”¯æŒéŸ³é¢‘è¾“å…¥
    supports_streaming: bool = True  # æ˜¯å¦æ”¯æŒæµå¼è¾“å‡º
    max_tokens: int = 4096  # æœ€å¤§è¾“å‡º token æ•°
    max_input_tokens: Optional[int] = None  # æœ€å¤§è¾“å…¥ token æ•°ï¼ˆNone è¡¨ç¤ºæœªçŸ¥ï¼‰


@dataclass
class ModelConfig:
    """
    æ¨¡å‹é…ç½®

    å®šä¹‰å•ä¸ªæ¨¡å‹çš„å®Œæ•´é…ç½®ä¿¡æ¯ã€‚

    Attributes:
        model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ "gpt-4o", "qwen3-max"ï¼‰
        model_type: æ¨¡å‹ç±»å‹ï¼ˆLLM, VLM, EMBEDDING ç­‰ï¼‰
        adapter: é€‚é…å™¨ç±»å‹ï¼ˆå†³å®šæ¶ˆæ¯æ ¼å¼ï¼‰
        base_url: API ç«¯ç‚¹
        api_key_env: API Key ç¯å¢ƒå˜é‡å
        provider: Provider åç§°ï¼ˆå¼•ç”¨ LLMRegistryï¼‰
        display_name: æ˜¾ç¤ºåç§°ï¼ˆå¯é€‰ï¼‰
        description: æ¨¡å‹æè¿°ï¼ˆå¯é€‰ï¼‰
        capabilities: æ¨¡å‹èƒ½åŠ›é…ç½®
        extra_config: é¢å¤–é…ç½®ï¼ˆProvider ç‰¹æœ‰å‚æ•°ï¼‰
    """

    model_name: str
    model_type: ModelType
    adapter: AdapterType
    base_url: str
    api_key_env: str
    provider: str
    display_name: Optional[str] = None
    description: Optional[str] = None
    capabilities: ModelCapabilities = field(default_factory=ModelCapabilities)
    pricing: ModelPricing = field(default_factory=ModelPricing)
    extra_config: Dict[str, Any] = field(default_factory=dict)
    is_custom: bool = False  # True = user-registered via API, persisted to YAML


@dataclass
class ActivatedModelEntry:
    """
    Activated model entry.

    Tracks a model that the user has configured with an API key.
    """

    model_name: str
    api_key: str  # actual API key value
    base_url: Optional[str] = None  # override catalog default
    activated_at: str = ""  # ISO timestamp

    # For custom models not in the catalog
    provider: Optional[str] = None
    model_type: Optional[str] = None
    adapter: Optional[str] = None
    display_name: Optional[str] = None
    description: Optional[str] = None
    capabilities: Optional[Dict[str, Any]] = None
    pricing: Optional[Dict[str, Any]] = None


# ============================================================
# ModelRegistry
# ============================================================


class ModelRegistry:
    """
    å…¨å±€æ¨¡å‹æ³¨å†Œä¸­å¿ƒ

    åŒå±‚æ¶æ„ï¼š
    1. æ”¯æŒç›®å½•ï¼ˆ_modelsï¼‰ï¼šç³»ç»ŸçŸ¥é“çš„æ‰€æœ‰æ¨¡å‹ï¼ˆpreset + customï¼‰ï¼Œå®šä¹‰èƒ½åŠ›å’Œé…ç½®
    2. æ¿€æ´»åˆ—è¡¨ï¼ˆ_activatedï¼‰ï¼šç”¨æˆ·é…ç½®äº† API Key çš„æ¨¡å‹ï¼Œå®é™…å¯ç”¨

    ä¸ LLMRegistry çš„å…³ç³»ï¼š
    - LLMRegistry æ³¨å†Œ Providerï¼ˆæœåŠ¡ç±» + é€‚é…å™¨ï¼‰
    - ModelRegistry æ³¨å†Œ Modelï¼ˆå…·ä½“é…ç½®ï¼‰ï¼Œå¼•ç”¨ Provider
    - åˆ›å»ºæœåŠ¡æ—¶ï¼šModelRegistry è·å–é…ç½® â†’ è°ƒç”¨ LLMRegistry åˆ›å»ºæœåŠ¡
    """

    _models: Dict[str, ModelConfig] = {}
    _activated: Dict[str, ActivatedModelEntry] = {}
    _initialized: bool = False

    @classmethod
    def register(cls, config: ModelConfig) -> None:
        """
        æ³¨å†Œæ¨¡å‹

        Args:
            config: æ¨¡å‹é…ç½®
        """
        model_name = config.model_name.lower()

        if model_name in cls._models:
            logger.warning(f"âš ï¸ æ¨¡å‹ '{config.model_name}' å·²æ³¨å†Œï¼Œå°†è¢«è¦†ç›–")

        cls._models[model_name] = config
        logger.debug(
            f"âœ… æ³¨å†Œæ¨¡å‹: {config.model_name} "
            f"(type={config.model_type.value}, provider={config.provider})"
        )

    @classmethod
    def get(cls, model_name: str) -> Optional[ModelConfig]:
        """
        è·å–æ¨¡å‹é…ç½®

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            æ¨¡å‹é…ç½®ï¼Œä¸å­˜åœ¨åˆ™è¿”å› None
        """
        cls._ensure_initialized()
        return cls._models.get(model_name.lower())

    @classmethod
    def create_service(
        cls, model_name: str, api_key: Optional[str] = None, **kwargs
    ) -> "BaseLLMService":
        """
        é€šè¿‡æ¨¡å‹ååˆ›å»º LLM æœåŠ¡

        Args:
            model_name: æ¨¡å‹åç§°
            api_key: API Keyï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            **kwargs: å…¶ä»–é…ç½®å‚æ•°ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰

        Returns:
            LLM æœåŠ¡å®ä¾‹

        Raises:
            ValueError: æ¨¡å‹æœªæ³¨å†Œ
        """
        cls._ensure_initialized()

        config = cls.get(model_name)
        if not config:
            available = ", ".join(cls._models.keys())
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹: '{model_name}'ã€‚" f"å¯ç”¨çš„æ¨¡å‹: {available}")

        # è·å– API Key
        effective_api_key = api_key or os.getenv(config.api_key_env)

        # é€šè¿‡ LLMRegistry åˆ›å»ºæœåŠ¡
        from .registry import LLMRegistry

        # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æ¿€æ´»æ—¶ä¿å­˜çš„è‡ªå®šä¹‰ base_urlï¼Œå¦åˆ™ä½¿ç”¨ç›®å½•é»˜è®¤å€¼
        effective_base_url = config.base_url
        entry = cls.get_activated_entry(model_name)
        if entry and entry.base_url:
            effective_base_url = entry.base_url

        # åˆå¹¶é…ç½®
        service_kwargs = {
            "base_url": effective_base_url,
            "max_tokens": config.capabilities.max_tokens,
            **config.extra_config,
            **kwargs,
        }

        # å¤„ç†ç‰¹æ®Šèƒ½åŠ›
        if config.capabilities.supports_thinking:
            service_kwargs.setdefault("enable_thinking", True)

        return LLMRegistry.create_service(
            provider=config.provider,
            model=config.model_name,
            api_key=effective_api_key,
            **service_kwargs,
        )

    @classmethod
    def list_models(
        cls,
        model_type: Optional[ModelType] = None,
        provider: Optional[str] = None,
        adapter: Optional[AdapterType] = None,
    ) -> List[ModelConfig]:
        """
        åˆ—å‡ºæ¨¡å‹

        Args:
            model_type: æŒ‰ç±»å‹è¿‡æ»¤
            provider: æŒ‰ Provider è¿‡æ»¤
            adapter: æŒ‰é€‚é…å™¨è¿‡æ»¤

        Returns:
            ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹é…ç½®åˆ—è¡¨
        """
        cls._ensure_initialized()

        models = list(cls._models.values())

        if model_type:
            models = [m for m in models if m.model_type == model_type]

        if provider:
            models = [m for m in models if m.provider.lower() == provider.lower()]

        if adapter:
            models = [m for m in models if m.adapter == adapter]

        return models

    @classmethod
    def list_model_names(
        cls,
        model_type: Optional[ModelType] = None,
    ) -> List[str]:
        """
        åˆ—å‡ºæ¨¡å‹åç§°

        Args:
            model_type: æŒ‰ç±»å‹è¿‡æ»¤

        Returns:
            æ¨¡å‹åç§°åˆ—è¡¨
        """
        models = cls.list_models(model_type=model_type)
        return [m.model_name for m in models]

    @classmethod
    def is_registered(cls, model_name: str) -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²æ³¨å†Œ

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            æ˜¯å¦å·²æ³¨å†Œ
        """
        cls._ensure_initialized()
        return model_name.lower() in cls._models

    @classmethod
    def get_model_info(cls, model_name: str) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        config = cls.get(model_name)
        if not config:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹: '{model_name}'")

        return {
            "model_name": config.model_name,
            "model_type": config.model_type.value,
            "adapter": config.adapter.value,
            "provider": config.provider,
            "base_url": config.base_url,
            "api_key_env": config.api_key_env,
            "display_name": config.display_name or config.model_name,
            "description": config.description,
            "capabilities": {
                "supports_tools": config.capabilities.supports_tools,
                "supports_vision": config.capabilities.supports_vision,
                "supports_thinking": config.capabilities.supports_thinking,
                "supports_audio": config.capabilities.supports_audio,
                "supports_streaming": config.capabilities.supports_streaming,
                "max_tokens": config.capabilities.max_tokens,
                "max_input_tokens": config.capabilities.max_input_tokens,
            },
            "pricing": {
                "input_per_million": config.pricing.input_per_million,
                "output_per_million": config.pricing.output_per_million,
                "cache_read_per_million": config.pricing.cache_read_per_million,
                "cache_write_per_million": config.pricing.cache_write_per_million,
                "is_free": config.pricing.is_free,
            },
        }

    # ============================================================
    # æ¿€æ´»å±‚ï¼ˆActivated Modelsï¼‰
    # ============================================================

    @classmethod
    def activate_model(
        cls,
        model_name: str,
        api_key: str,
        base_url: Optional[str] = None,
        *,
        provider: Optional[str] = None,
        model_type: Optional[str] = None,
        adapter: Optional[str] = None,
        display_name: Optional[str] = None,
        description: Optional[str] = None,
        capabilities: Optional[Dict[str, Any]] = None,
        pricing: Optional[Dict[str, Any]] = None,
    ) -> ActivatedModelEntry:
        """
        Activate a model by providing an API key.

        If the model exists in the catalog, only api_key (and optional
        base_url override) are needed. For custom models not in the catalog,
        provider/adapter/model_type are required and the model is also
        added to the catalog.

        Args:
            model_name: Model identifier.
            api_key: Actual API key value.
            base_url: Override the catalog default URL.
            provider: Required for custom models.
            model_type: Required for custom models.
            adapter: Required for custom models.

        Returns:
            The activated model entry.
        """
        from datetime import datetime

        cls._ensure_initialized()
        key = model_name.lower()

        catalog_config = cls._models.get(key)

        # If not in catalog, register as custom model first
        if not catalog_config:
            if not provider:
                raise ValueError(
                    f"æ¨¡å‹ '{model_name}' ä¸åœ¨æ”¯æŒç›®å½•ä¸­ï¼Œ"
                    f"å¿…é¡»æä¾› provider å­—æ®µ"
                )
            # Build catalog entry from activation params
            caps = ModelCapabilities(
                **(capabilities or {})
            ) if capabilities else ModelCapabilities()
            price = ModelPricing(
                **(pricing or {})
            ) if pricing else ModelPricing()

            catalog_config = ModelConfig(
                model_name=model_name,
                model_type=ModelType(model_type or "llm"),
                adapter=AdapterType(adapter or "openai"),
                base_url=base_url or "",
                api_key_env=f"{provider.upper()}_API_KEY",
                provider=provider,
                display_name=display_name,
                description=description,
                capabilities=caps,
                pricing=price,
                is_custom=True,
            )
            cls._models[key] = catalog_config
            logger.info(
                f"ğŸ“ è‡ªå®šä¹‰æ¨¡å‹å·²åŠ å…¥ç›®å½•: {model_name} "
                f"(provider={provider})"
            )

        # Set API key in environment
        os.environ[catalog_config.api_key_env] = api_key

        # Create activated entry
        entry = ActivatedModelEntry(
            model_name=model_name,
            api_key=api_key,
            base_url=base_url,
            activated_at=datetime.now().isoformat(),
            provider=catalog_config.provider,
            model_type=catalog_config.model_type.value,
            adapter=catalog_config.adapter.value,
            display_name=display_name,
            description=description,
            capabilities=capabilities,
            pricing=pricing,
        )
        cls._activated[key] = entry

        logger.info(f"âœ… æ¨¡å‹å·²æ¿€æ´»: {model_name} (provider={catalog_config.provider})")
        return entry

    @classmethod
    def activate_provider_models(
        cls,
        provider: str,
        api_key: str,
        base_url: Optional[str] = None,
    ) -> List[ActivatedModelEntry]:
        """
        Batch-activate all catalog models for a given provider.

        Args:
            provider: Provider name (e.g. "qwen", "claude", "openai").
            api_key: Actual API key value (shared by all models of this provider).
            base_url: Optional base URL override.

        Returns:
            List of newly activated model entries.
        """
        from datetime import datetime

        cls._ensure_initialized()

        catalog_models = cls.list_models(provider=provider)
        if not catalog_models:
            logger.warning(f"âš ï¸ Provider '{provider}' åœ¨ç›®å½•ä¸­æ²¡æœ‰æ¨¡å‹")
            return []

        # Set API key env once (all models of a provider share the same env var)
        env_var = catalog_models[0].api_key_env
        os.environ[env_var] = api_key

        activated = []
        now = datetime.now().isoformat()

        for config in catalog_models:
            key = config.model_name.lower()

            entry = ActivatedModelEntry(
                model_name=config.model_name,
                api_key=api_key,
                base_url=base_url,
                activated_at=now,
                provider=config.provider,
                model_type=config.model_type.value,
                adapter=config.adapter.value,
            )
            cls._activated[key] = entry
            activated.append(entry)

        logger.info(
            f"âœ… æ‰¹é‡æ¿€æ´» {len(activated)} ä¸ªæ¨¡å‹ (provider={provider})"
        )
        return activated

    @classmethod
    def deactivate_model(cls, model_name: str) -> bool:
        """
        Deactivate a model (remove API key config).

        Returns:
            True if model was deactivated, False if not found.
        """
        cls._ensure_initialized()
        key = model_name.lower()

        entry = cls._activated.pop(key, None)
        if not entry:
            return False

        # Clear env var
        catalog_config = cls._models.get(key)
        if catalog_config:
            os.environ.pop(catalog_config.api_key_env, None)

        logger.info(f"ğŸ—‘ï¸ æ¨¡å‹å·²åœç”¨: {model_name}")
        return True

    @classmethod
    def is_activated(cls, model_name: str) -> bool:
        """Check if a model is activated."""
        cls._ensure_initialized()
        return model_name.lower() in cls._activated

    @classmethod
    def list_activated(
        cls,
        provider: Optional[str] = None,
    ) -> List[ModelConfig]:
        """
        List activated models (with catalog info merged).

        Returns ModelConfig objects for each activated model.
        """
        cls._ensure_initialized()

        results = []
        for key, entry in cls._activated.items():
            config = cls._models.get(key)
            if not config:
                continue
            if provider and config.provider.lower() != provider.lower():
                continue
            results.append(config)
        return results

    @classmethod
    def get_activated_entry(cls, model_name: str) -> Optional[ActivatedModelEntry]:
        """Get activated entry for a model."""
        cls._ensure_initialized()
        return cls._activated.get(model_name.lower())

    # ============================================================
    # åˆå§‹åŒ– & é‡ç½®
    # ============================================================

    @classmethod
    def _ensure_initialized(cls) -> None:
        """
        ç¡®ä¿ Registry å·²åˆå§‹åŒ–ï¼ˆåŠ è½½é¢„ç½®æ¨¡å‹ + è‡ªå®šä¹‰æ¨¡å‹ + æ¿€æ´»æ¨¡å‹ï¼‰
        """
        if cls._initialized:
            return

        # åŠ è½½é¢„ç½®æ¨¡å‹
        _register_preset_models()

        # åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹ï¼ˆä» YAML æŒä¹…åŒ–æ–‡ä»¶ï¼‰
        custom_count = cls._load_custom_models()
        if custom_count > 0:
            logger.info(f"ğŸ“¦ å·²åŠ è½½ {custom_count} ä¸ªç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹")

        # åŠ è½½ç”¨æˆ·æ¿€æ´»çš„æ¨¡å‹ï¼ˆä» YAML æŒä¹…åŒ–æ–‡ä»¶ï¼‰
        activated_count = cls._load_activated_models()
        if activated_count > 0:
            logger.info(f"ğŸ”‘ å·²åŠ è½½ {activated_count} ä¸ªå·²æ¿€æ´»æ¨¡å‹")

        cls._initialized = True
        logger.info(
            f"âœ… ModelRegistry åˆå§‹åŒ–å®Œæˆï¼Œ"
            f"ç›®å½• {len(cls._models)} ä¸ªæ¨¡å‹ï¼Œ"
            f"å·²æ¿€æ´» {len(cls._activated)} ä¸ª"
        )

    @classmethod
    def reset(cls) -> None:
        """
        é‡ç½® Registryï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
        """
        cls._models.clear()
        cls._activated.clear()
        cls._initialized = False

    # ============================================================
    # æŒä¹…åŒ–ï¼ˆYAMLï¼‰
    # ============================================================

    @classmethod
    def _get_custom_models_path(cls) -> Path:
        """
        Get path to the custom models YAML file.

        Uses the user data directory to keep persistence across restarts.
        """
        from utils.app_paths import get_user_data_dir

        config_dir = get_user_data_dir() / "config"
        config_dir.mkdir(parents=True, exist_ok=True)
        return config_dir / "custom_models.yaml"

    @classmethod
    def _model_config_to_dict(cls, config: ModelConfig) -> Dict[str, Any]:
        """Serialize a ModelConfig to a plain dict for YAML output."""
        return {
            "model_name": config.model_name,
            "model_type": config.model_type.value,
            "adapter": config.adapter.value,
            "base_url": config.base_url,
            "api_key_env": config.api_key_env,
            "provider": config.provider,
            "display_name": config.display_name,
            "description": config.description,
            "capabilities": {
                "supports_tools": config.capabilities.supports_tools,
                "supports_vision": config.capabilities.supports_vision,
                "supports_thinking": config.capabilities.supports_thinking,
                "supports_audio": config.capabilities.supports_audio,
                "supports_streaming": config.capabilities.supports_streaming,
                "max_tokens": config.capabilities.max_tokens,
                "max_input_tokens": config.capabilities.max_input_tokens,
            },
            "pricing": {
                "input_per_million": config.pricing.input_per_million,
                "output_per_million": config.pricing.output_per_million,
                "cache_read_per_million": config.pricing.cache_read_per_million,
                "cache_write_per_million": config.pricing.cache_write_per_million,
            },
            "extra_config": config.extra_config or {},
        }

    @classmethod
    def _dict_to_model_config(cls, data: Dict[str, Any]) -> ModelConfig:
        """Deserialize a plain dict from YAML into a ModelConfig."""
        caps_data = data.get("capabilities", {})
        pricing_data = data.get("pricing", {})

        return ModelConfig(
            model_name=data["model_name"],
            model_type=ModelType(data.get("model_type", "llm")),
            adapter=AdapterType(data.get("adapter", "openai")),
            base_url=data["base_url"],
            api_key_env=data["api_key_env"],
            provider=data["provider"],
            display_name=data.get("display_name"),
            description=data.get("description"),
            capabilities=ModelCapabilities(
                supports_tools=caps_data.get("supports_tools", True),
                supports_vision=caps_data.get("supports_vision", False),
                supports_thinking=caps_data.get("supports_thinking", False),
                supports_audio=caps_data.get("supports_audio", False),
                supports_streaming=caps_data.get("supports_streaming", True),
                max_tokens=caps_data.get("max_tokens", 4096),
                max_input_tokens=caps_data.get("max_input_tokens"),
            ),
            pricing=ModelPricing(
                input_per_million=pricing_data.get("input_per_million"),
                output_per_million=pricing_data.get("output_per_million"),
                cache_read_per_million=pricing_data.get("cache_read_per_million"),
                cache_write_per_million=pricing_data.get("cache_write_per_million"),
            ),
            extra_config=data.get("extra_config", {}),
            is_custom=True,
        )

    @classmethod
    def _load_custom_models(cls) -> int:
        """
        Load custom models from YAML file (synchronous, called during init).

        Returns:
            Number of custom models loaded.
        """
        path = cls._get_custom_models_path()
        if not path.exists():
            return 0

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)

            if not data or not isinstance(data.get("models"), list):
                return 0

            count = 0
            for model_data in data["models"]:
                try:
                    config = cls._dict_to_model_config(model_data)
                    cls._models[config.model_name.lower()] = config
                    count += 1
                except Exception as e:
                    logger.warning(
                        f"âš ï¸ åŠ è½½è‡ªå®šä¹‰æ¨¡å‹å¤±è´¥ "
                        f"(name={model_data.get('model_name', '?')}): {e}"
                    )
            return count

        except Exception as e:
            logger.error(f"âŒ è¯»å–è‡ªå®šä¹‰æ¨¡å‹æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            return 0

    @classmethod
    async def save_custom_models(cls) -> None:
        """
        Persist all custom models (is_custom=True) to YAML file.

        Called after register/unregister operations.
        """
        custom_models = [
            cls._model_config_to_dict(m)
            for m in cls._models.values()
            if m.is_custom
        ]

        path = cls._get_custom_models_path()
        content = yaml.dump(
            {"models": custom_models},
            allow_unicode=True,
            default_flow_style=False,
            sort_keys=False,
        )

        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(content)

        logger.info(
            f"ğŸ’¾ å·²ä¿å­˜ {len(custom_models)} ä¸ªè‡ªå®šä¹‰æ¨¡å‹åˆ° {path}"
        )

    # ============================================================
    # æ¿€æ´»æ¨¡å‹æŒä¹…åŒ–ï¼ˆYAMLï¼‰
    # ============================================================

    @classmethod
    def _get_activated_models_path(cls) -> Path:
        """Get path to activated_models.yaml."""
        from utils.app_paths import get_user_data_dir

        config_dir = get_user_data_dir() / "config"
        config_dir.mkdir(parents=True, exist_ok=True)
        return config_dir / "activated_models.yaml"

    @classmethod
    def _load_activated_models(cls) -> int:
        """
        Load activated models from YAML (synchronous, called at init).

        Also sets API keys in environment variables.

        Returns:
            Number of activated models loaded.
        """
        path = cls._get_activated_models_path()
        if not path.exists():
            return 0

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)

            if not data or not isinstance(data.get("models"), list):
                return 0

            count = 0
            for item in data["models"]:
                try:
                    name = item["model_name"]
                    api_key = item.get("api_key", "")
                    key = name.lower()

                    entry = ActivatedModelEntry(
                        model_name=name,
                        api_key=api_key,
                        base_url=item.get("base_url"),
                        activated_at=item.get("activated_at", ""),
                        provider=item.get("provider"),
                        model_type=item.get("model_type"),
                        adapter=item.get("adapter"),
                        display_name=item.get("display_name"),
                        description=item.get("description"),
                        capabilities=item.get("capabilities"),
                        pricing=item.get("pricing"),
                    )
                    cls._activated[key] = entry

                    # If model not in catalog, register as custom
                    if key not in cls._models and entry.provider:
                        caps_data = entry.capabilities or {}
                        pricing_data = entry.pricing or {}
                        config = ModelConfig(
                            model_name=name,
                            model_type=ModelType(entry.model_type or "llm"),
                            adapter=AdapterType(entry.adapter or "openai"),
                            base_url=entry.base_url or "",
                            api_key_env=f"{entry.provider.upper()}_API_KEY",
                            provider=entry.provider,
                            display_name=entry.display_name,
                            description=entry.description,
                            capabilities=ModelCapabilities(**caps_data) if caps_data else ModelCapabilities(),
                            pricing=ModelPricing(**pricing_data) if pricing_data else ModelPricing(),
                            is_custom=True,
                        )
                        cls._models[key] = config

                    # Set API key in environment
                    catalog = cls._models.get(key)
                    if catalog and api_key:
                        os.environ[catalog.api_key_env] = api_key

                    count += 1
                except Exception as e:
                    logger.warning(
                        f"âš ï¸ åŠ è½½æ¿€æ´»æ¨¡å‹å¤±è´¥ "
                        f"(name={item.get('model_name', '?')}): {e}"
                    )
            return count

        except Exception as e:
            logger.error(f"âŒ è¯»å–æ¿€æ´»æ¨¡å‹æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            return 0

    @classmethod
    async def save_activated_models(cls) -> None:
        """
        Persist all activated models to YAML.

        Called after activate/deactivate operations.
        API keys are stored in the file (local-only, gitignored).
        """
        entries = []
        for entry in cls._activated.values():
            item: Dict[str, Any] = {
                "model_name": entry.model_name,
                "api_key": entry.api_key,
                "activated_at": entry.activated_at,
            }
            if entry.base_url:
                item["base_url"] = entry.base_url
            # For custom models, store full config
            if entry.provider:
                item["provider"] = entry.provider
            if entry.model_type:
                item["model_type"] = entry.model_type
            if entry.adapter:
                item["adapter"] = entry.adapter
            if entry.display_name:
                item["display_name"] = entry.display_name
            if entry.description:
                item["description"] = entry.description
            if entry.capabilities:
                item["capabilities"] = entry.capabilities
            if entry.pricing:
                item["pricing"] = entry.pricing
            entries.append(item)

        path = cls._get_activated_models_path()
        content = yaml.dump(
            {"models": entries},
            allow_unicode=True,
            default_flow_style=False,
            sort_keys=False,
        )

        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(content)

        logger.info(f"ğŸ’¾ å·²ä¿å­˜ {len(entries)} ä¸ªæ¿€æ´»æ¨¡å‹åˆ° {path}")


# ============================================================
# é¢„ç½®æ¨¡å‹æ³¨å†Œ
# ============================================================


def _register_preset_models() -> None:
    """
    æ³¨å†Œé¢„ç½®æ¨¡å‹

    åœ¨ ModelRegistry åˆå§‹åŒ–æ—¶è‡ªåŠ¨è°ƒç”¨ã€‚
    """
    # ==================== OpenAI ç³»åˆ— ====================

    _OPENAI_COMMON: Dict[str, Any] = dict(
        adapter=AdapterType.OPENAI,
        base_url="https://api.openai.com/v1",
        api_key_env="OPENAI_API_KEY",
        provider="openai",
    )

    # --- GPT-5 family ---

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-5.2",
            model_type=ModelType.VLM,
            display_name="GPT-5.2",
            description="OpenAI æœ€æ–°æ——èˆ°ï¼Œç¼–ç å’Œæ™ºèƒ½ä½“ä»»åŠ¡æœ€å¼º",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=100000, max_input_tokens=1000000,
            ),
            pricing=ModelPricing(input_per_million=1.75, output_per_million=14.0,
                                 cache_read_per_million=0.175),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-5.1",
            model_type=ModelType.VLM,
            display_name="GPT-5.1",
            description="GPT-5 ç³»åˆ—é«˜æ€§èƒ½æ¨ç†æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=100000, max_input_tokens=1000000,
            ),
            pricing=ModelPricing(input_per_million=1.25, output_per_million=10.0,
                                 cache_read_per_million=0.125),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-5",
            model_type=ModelType.VLM,
            display_name="GPT-5",
            description="GPT-5 ç³»åˆ—æ¨ç†æ¨¡å‹ï¼Œç¼–ç å’Œæ™ºèƒ½ä½“ä»»åŠ¡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=100000, max_input_tokens=1000000,
            ),
            pricing=ModelPricing(input_per_million=1.25, output_per_million=10.0,
                                 cache_read_per_million=0.125),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-5-mini",
            model_type=ModelType.VLM,
            display_name="GPT-5 Mini",
            description="GPT-5 é«˜æ€§ä»·æ¯”ç‰ˆæœ¬",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=100000, max_input_tokens=1000000,
            ),
            pricing=ModelPricing(input_per_million=0.25, output_per_million=2.0,
                                 cache_read_per_million=0.025),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-5-nano",
            model_type=ModelType.VLM,
            display_name="GPT-5 Nano",
            description="GPT-5 æœ€ä½æˆæœ¬ç‰ˆæœ¬",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=16384, max_input_tokens=1000000,
            ),
            pricing=ModelPricing(input_per_million=0.05, output_per_million=0.40,
                                 cache_read_per_million=0.005),
            **_OPENAI_COMMON,
        )
    )

    # --- GPT-4.1 family ---

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-4.1",
            model_type=ModelType.VLM,
            display_name="GPT-4.1",
            description="æœ€å¼ºéæ¨ç†æ¨¡å‹ï¼Œ1M ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=32768, max_input_tokens=1047576,
            ),
            pricing=ModelPricing(input_per_million=2.0, output_per_million=8.0,
                                 cache_read_per_million=0.50),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-4.1-mini",
            model_type=ModelType.VLM,
            display_name="GPT-4.1 Mini",
            description="GPT-4.1 è½»é‡ç‰ˆï¼Œ1M ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=32768, max_input_tokens=1047576,
            ),
            pricing=ModelPricing(input_per_million=0.40, output_per_million=1.60,
                                 cache_read_per_million=0.10),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-4.1-nano",
            model_type=ModelType.VLM,
            display_name="GPT-4.1 Nano",
            description="GPT-4.1 æœ€ä½æˆæœ¬ç‰ˆï¼Œ1M ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=16384, max_input_tokens=1047576,
            ),
            pricing=ModelPricing(input_per_million=0.10, output_per_million=0.40,
                                 cache_read_per_million=0.025),
            **_OPENAI_COMMON,
        )
    )

    # --- o-series (reasoning) ---

    ModelRegistry.register(
        ModelConfig(
            model_name="o3",
            model_type=ModelType.VLM,
            display_name="o3",
            description="OpenAI æ¨ç†æ¨¡å‹ï¼Œç¼–ç /æ•°å­¦/ç§‘å­¦æœ€å¼º",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=100000, max_input_tokens=200000,
            ),
            pricing=ModelPricing(input_per_million=2.0, output_per_million=8.0,
                                 cache_read_per_million=0.50),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="o4-mini",
            model_type=ModelType.VLM,
            display_name="o4-mini",
            description="é«˜æ€§ä»·æ¯”æ¨ç†æ¨¡å‹ï¼Œæ•°å­¦/ç¼–ç /è§†è§‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=100000, max_input_tokens=200000,
            ),
            pricing=ModelPricing(input_per_million=1.10, output_per_million=4.40,
                                 cache_read_per_million=0.275),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="o3-mini",
            model_type=ModelType.LLM,
            display_name="o3-mini",
            description="o3 è½»é‡ç‰ˆæ¨ç†æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=65536, max_input_tokens=200000,
            ),
            pricing=ModelPricing(input_per_million=1.10, output_per_million=4.40,
                                 cache_read_per_million=0.55),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="o1",
            model_type=ModelType.VLM,
            display_name="o1",
            description="OpenAI é¦–ä»£æ¨ç†æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=100000, max_input_tokens=200000,
            ),
            pricing=ModelPricing(input_per_million=15.0, output_per_million=60.0,
                                 cache_read_per_million=7.50),
            **_OPENAI_COMMON,
        )
    )

    # --- GPT-4o family ---

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-4o",
            model_type=ModelType.VLM,
            display_name="GPT-4o",
            description="å¿«é€Ÿæ™ºèƒ½å¤šæ¨¡æ€æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=16384, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=2.5, output_per_million=10.0,
                                 cache_read_per_million=1.25),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-4o-mini",
            model_type=ModelType.VLM,
            display_name="GPT-4o Mini",
            description="GPT-4o è½»é‡ç‰ˆæœ¬",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=16384, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.15, output_per_million=0.60,
                                 cache_read_per_million=0.075),
            **_OPENAI_COMMON,
        )
    )

    # --- GPT Audio (Omni) ---

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-audio",
            model_type=ModelType.AUDIO,
            display_name="GPT Audio",
            description="OpenAI å…¨æ¨¡æ€éŸ³é¢‘æ¨¡å‹ï¼Œæ”¯æŒéŸ³é¢‘è¾“å…¥/è¾“å‡º",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_audio=True,
                max_tokens=16384, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=2.5, output_per_million=10.0),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-4o-audio-preview",
            model_type=ModelType.AUDIO,
            display_name="GPT-4o Audio Preview",
            description="GPT-4o éŸ³é¢‘é¢„è§ˆç‰ˆï¼Œæ”¯æŒéŸ³é¢‘è¾“å…¥/è¾“å‡º",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_audio=True,
                max_tokens=16384, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=2.5, output_per_million=10.0),
            **_OPENAI_COMMON,
        )
    )

    # --- Legacy ---

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-4-turbo",
            model_type=ModelType.VLM,
            display_name="GPT-4 Turbo",
            description="GPT-4 Turbo with Visionï¼ˆæ—§ç‰ˆï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=4096, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=10.0, output_per_million=30.0),
            **_OPENAI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gpt-3.5-turbo",
            model_type=ModelType.LLM,
            display_name="GPT-3.5 Turbo",
            description="æ—§ç‰ˆå¿«é€Ÿæ¨¡å‹ï¼ˆLegacyï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=False,
                max_tokens=4096, max_input_tokens=16385,
            ),
            pricing=ModelPricing(input_per_million=0.50, output_per_million=1.50),
            **_OPENAI_COMMON,
        )
    )

    # ==================== Claude ç³»åˆ— ====================

    _CLAUDE_COMMON: Dict[str, Any] = dict(
        adapter=AdapterType.CLAUDE,
        base_url="https://api.anthropic.com",
        api_key_env="ANTHROPIC_API_KEY",
        provider="claude",
    )

    # --- Claude 4.5 / 4.6 (latest) ---

    ModelRegistry.register(
        ModelConfig(
            model_name="claude-sonnet-4-5-20250929",
            model_type=ModelType.VLM,
            display_name="Claude Sonnet 4.5",
            description="Anthropic æœ€å¼ºæ™ºèƒ½ä½“ä¸ç¼–ç æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=64000, max_input_tokens=200000,
            ),
            pricing=ModelPricing(
                input_per_million=3.0, output_per_million=15.0,
                cache_read_per_million=0.30, cache_write_per_million=3.75,
            ),
            **_CLAUDE_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="claude-opus-4-6",
            model_type=ModelType.VLM,
            display_name="Claude Opus 4.6",
            description="Anthropic æœ€å¼ºæ¨¡å‹ï¼ˆ2026.02ï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=64000, max_input_tokens=200000,
            ),
            pricing=ModelPricing(
                input_per_million=15.0, output_per_million=75.0,
                cache_read_per_million=1.50, cache_write_per_million=18.75,
            ),
            **_CLAUDE_COMMON,
        )
    )

    # --- Claude 4 ---

    ModelRegistry.register(
        ModelConfig(
            model_name="claude-sonnet-4-20250514",
            model_type=ModelType.VLM,
            display_name="Claude Sonnet 4",
            description="Claude 4 ç³»åˆ—å‡è¡¡æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=64000, max_input_tokens=200000,
            ),
            pricing=ModelPricing(
                input_per_million=3.0, output_per_million=15.0,
                cache_read_per_million=0.30, cache_write_per_million=3.75,
            ),
            **_CLAUDE_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="claude-opus-4-20250514",
            model_type=ModelType.VLM,
            display_name="Claude Opus 4",
            description="Claude 4 ç³»åˆ—æ——èˆ°æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=32000, max_input_tokens=200000,
            ),
            pricing=ModelPricing(
                input_per_million=15.0, output_per_million=75.0,
                cache_read_per_million=1.50, cache_write_per_million=18.75,
            ),
            **_CLAUDE_COMMON,
        )
    )

    # --- Claude 3.5 ---

    ModelRegistry.register(
        ModelConfig(
            model_name="claude-3-5-sonnet-20241022",
            model_type=ModelType.VLM,
            display_name="Claude 3.5 Sonnet",
            description="Claude 3.5 ç³»åˆ—å‡è¡¡æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=8192, max_input_tokens=200000,
            ),
            pricing=ModelPricing(
                input_per_million=3.0, output_per_million=15.0,
                cache_read_per_million=0.30, cache_write_per_million=3.75,
            ),
            **_CLAUDE_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="claude-3-5-haiku-20241022",
            model_type=ModelType.VLM,
            display_name="Claude 3.5 Haiku",
            description="Claude 3.5 å¿«é€Ÿæ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=8192, max_input_tokens=200000,
            ),
            pricing=ModelPricing(
                input_per_million=0.80, output_per_million=4.0,
                cache_read_per_million=0.08, cache_write_per_million=1.0,
            ),
            **_CLAUDE_COMMON,
        )
    )

    # --- Claude 3 (Legacy) ---

    ModelRegistry.register(
        ModelConfig(
            model_name="claude-3-opus-20240229",
            model_type=ModelType.VLM,
            display_name="Claude 3 Opus",
            description="Claude 3 ç³»åˆ—æ——èˆ°æ¨¡å‹ï¼ˆLegacyï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=4096, max_input_tokens=200000,
            ),
            pricing=ModelPricing(
                input_per_million=15.0, output_per_million=75.0,
                cache_read_per_million=1.50, cache_write_per_million=18.75,
            ),
            **_CLAUDE_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="claude-3-haiku-20240307",
            model_type=ModelType.VLM,
            display_name="Claude 3 Haiku",
            description="Claude 3 å¿«é€Ÿæ¨¡å‹ï¼ˆLegacyï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=4096, max_input_tokens=200000,
            ),
            pricing=ModelPricing(
                input_per_million=0.25, output_per_million=1.25,
                cache_read_per_million=0.03, cache_write_per_million=0.30,
            ),
            **_CLAUDE_COMMON,
        )
    )

    # ==================== Qwen ç³»åˆ— ====================

    _QWEN_COMMON: Dict[str, Any] = dict(
        adapter=AdapterType.OPENAI,
        base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
        api_key_env="DASHSCOPE_API_KEY",
        provider="qwen",
    )

    # --- Qwen3 æ——èˆ° ---

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-max",
            model_type=ModelType.LLM,
            display_name="Qwen3-Max",
            description="é˜¿é‡Œäº‘æ——èˆ°æ¨¡å‹ï¼Œ262K ä¸Šä¸‹æ–‡ï¼ŒCoding 76.7 / GPQA 76.4",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=65536, max_input_tokens=258048,
            ),
            pricing=ModelPricing(input_per_million=1.20, output_per_million=6.00,
                                 cache_read_per_million=0.24),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-max-thinking",
            model_type=ModelType.LLM,
            display_name="Qwen3-Max-Thinking",
            description="é˜¿é‡Œäº‘æœ€å¼ºæ¨ç†æ¨¡å‹ (2026.01)ï¼Œå¯¹æ ‡ GPT-5.2-Thinking / Claude-Opus-4.5",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=65536, max_input_tokens=258048,
            ),
            pricing=ModelPricing(input_per_million=1.20, output_per_million=6.00,
                                 cache_read_per_million=0.24),
            **_QWEN_COMMON,
        )
    )

    # --- Qwen3 Coder ç³»åˆ— ---

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-coder-plus",
            model_type=ModelType.LLM,
            display_name="Qwen3-Coder-Plus",
            description="Qwen3 ä»£ç ä¸“ç²¾æ¨¡å‹ï¼Œ128K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=65536, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=1.00, output_per_million=5.00,
                                 cache_read_per_million=0.10),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-coder-flash",
            model_type=ModelType.LLM,
            display_name="Qwen3-Coder-Flash",
            description="Qwen3 ä»£ç å¿«é€Ÿæ¨¡å‹ï¼Œ128K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=65536, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.30, output_per_million=1.50,
                                 cache_read_per_million=0.08),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-coder-next",
            model_type=ModelType.LLM,
            display_name="Qwen3-Coder-Next",
            description="Qwen3 æœ€æ–°ä»£ç æ¨¡å‹ (2026.02)ï¼Œ262K ä¸Šä¸‹æ–‡ï¼ŒGPQA 73.7",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=65536, max_input_tokens=262144,
            ),
            pricing=ModelPricing(input_per_million=0.07, output_per_million=0.30,
                                 cache_read_per_million=0.035),
            **_QWEN_COMMON,
        )
    )

    # --- Qwen3 Next ç³»åˆ— ---

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-next-80b-a3b-instruct",
            model_type=ModelType.LLM,
            display_name="Qwen3-Next-80B Instruct",
            description="Qwen3 Next é«˜æ€§èƒ½æ¨¡å‹ï¼Œ262K ä¸Šä¸‹æ–‡ï¼ŒCoding 68.4",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=False,
                max_tokens=65536, max_input_tokens=262144,
            ),
            pricing=ModelPricing(input_per_million=0.09, output_per_million=1.10),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-next-80b-a3b-thinking",
            model_type=ModelType.LLM,
            display_name="Qwen3-Next-80B Thinking",
            description="Qwen3 Next æ·±åº¦æ¨ç†æ¨¡å‹ï¼Œ128K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=65536, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.15, output_per_million=1.20),
            **_QWEN_COMMON,
        )
    )

    # --- Qwen3 VLï¼ˆè§†è§‰ï¼‰ç³»åˆ— ---

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-vl-plus",
            model_type=ModelType.VLM,
            display_name="Qwen3-VL-Plus",
            description="Qwen3 è§†è§‰æ——èˆ°ï¼Œ262K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=65536, max_input_tokens=262144,
            ),
            pricing=ModelPricing(input_per_million=0.50, output_per_million=1.50),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-vl-235b-a22b-instruct",
            model_type=ModelType.VLM,
            display_name="Qwen3-VL-235B Instruct",
            description="Qwen3 æ——èˆ°è§†è§‰æ¨¡å‹ï¼Œ262K ä¸Šä¸‹æ–‡ï¼ŒCoding 59.4",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=65536, max_input_tokens=262144,
            ),
            pricing=ModelPricing(input_per_million=0.20, output_per_million=0.88,
                                 cache_read_per_million=0.11),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-vl-235b-a22b-thinking",
            model_type=ModelType.VLM,
            display_name="Qwen3-VL-235B Thinking",
            description="Qwen3 æ——èˆ°è§†è§‰æ¨ç†æ¨¡å‹ï¼Œ262K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=65536, max_input_tokens=262144,
            ),
            pricing=ModelPricing(input_per_million=0.45, output_per_million=3.50),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-vl-32b-instruct",
            model_type=ModelType.VLM,
            display_name="Qwen3-VL-32B Instruct",
            description="Qwen3 è§†è§‰æ¨¡å‹ 32Bï¼Œ262K ä¸Šä¸‹æ–‡ï¼ŒCoding 51.4",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=65536, max_input_tokens=262144,
            ),
            pricing=ModelPricing(input_per_million=0.50, output_per_million=1.50),
            **_QWEN_COMMON,
        )
    )

    # --- Qwen ç»å…¸ç³»åˆ—ï¼ˆQwen-Plus / Turbo / Max / QwQï¼‰---

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen-plus",
            model_type=ModelType.LLM,
            display_name="Qwen-Plus",
            description="é˜¿é‡Œäº‘é€šç”¨æ¨¡å‹ï¼Œ131K ä¸Šä¸‹æ–‡ï¼Œæ€§ä»·æ¯”é«˜",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=33000, max_input_tokens=131072,
            ),
            pricing=ModelPricing(input_per_million=0.40, output_per_million=1.20,
                                 cache_read_per_million=0.16),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen-turbo",
            model_type=ModelType.LLM,
            display_name="Qwen-Turbo",
            description="é˜¿é‡Œäº‘ä½æˆæœ¬å¿«é€Ÿæ¨¡å‹ï¼Œ1M è¶…é•¿ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=8192, max_input_tokens=1000000,
            ),
            pricing=ModelPricing(input_per_million=0.05, output_per_million=0.20,
                                 cache_read_per_million=0.02),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen-max",
            model_type=ModelType.LLM,
            display_name="Qwen-Max",
            description="Qwen2.5 æ——èˆ°æ¨¡å‹ï¼Œ32K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=False,
                max_tokens=16384, max_input_tokens=32768,
            ),
            pricing=ModelPricing(input_per_million=1.60, output_per_million=6.40,
                                 cache_read_per_million=0.64),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwq-plus",
            model_type=ModelType.LLM,
            display_name="QwQ-Plus",
            description="Qwen æ¨ç†æ¨¡å‹ï¼ŒCoding 63.1 / GPQA 59.3",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=33000, max_input_tokens=131072,
            ),
            pricing=ModelPricing(input_per_million=0.40, output_per_million=1.20),
            **_QWEN_COMMON,
        )
    )

    # --- Qwen Omniï¼ˆå…¨æ¨¡æ€éŸ³é¢‘ï¼‰ç³»åˆ— ---

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen-omni-turbo",
            model_type=ModelType.AUDIO,
            display_name="Qwen-Omni-Turbo",
            description="é˜¿é‡Œäº‘å…¨æ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒéŸ³é¢‘/è§†è§‰è¾“å…¥è¾“å‡º",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_audio=True,
                max_tokens=33000, max_input_tokens=131072,
            ),
            pricing=ModelPricing(input_per_million=0.80, output_per_million=3.20),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen3-omni-flash",
            model_type=ModelType.AUDIO,
            display_name="Qwen3-Omni-Flash",
            description="Qwen3 å…¨æ¨¡æ€å¿«é€Ÿæ¨¡å‹ï¼Œæ”¯æŒéŸ³é¢‘/è§†è§‰è¾“å…¥è¾“å‡º",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_audio=True,
                max_tokens=33000, max_input_tokens=131072,
            ),
            pricing=ModelPricing(input_per_million=0.30, output_per_million=1.50),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen-audio-turbo",
            model_type=ModelType.AUDIO,
            display_name="Qwen-Audio-Turbo",
            description="é˜¿é‡Œäº‘éŸ³é¢‘ç†è§£æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=False, supports_vision=False, supports_audio=True,
                max_tokens=8192, max_input_tokens=131072,
            ),
            pricing=ModelPricing(input_per_million=0.40, output_per_million=1.20),
            **_QWEN_COMMON,
        )
    )

    # --- Qwen VL ç»å…¸ ---

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen-vl-max",
            model_type=ModelType.VLM,
            display_name="Qwen-VL-Max",
            description="é˜¿é‡Œäº‘è§†è§‰è¯­è¨€æ——èˆ°ï¼Œ131K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=33000, max_input_tokens=131072,
            ),
            pricing=ModelPricing(input_per_million=0.80, output_per_million=3.20),
            **_QWEN_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="qwen-vl-plus",
            model_type=ModelType.VLM,
            display_name="Qwen-VL-Plus",
            description="é˜¿é‡Œäº‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆè½»é‡ï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=8192, max_input_tokens=131072,
            ),
            pricing=ModelPricing(input_per_million=0.21, output_per_million=0.63),
            **_QWEN_COMMON,
        )
    )

    # ==================== GLMï¼ˆæ™ºè°±AIï¼‰ç³»åˆ— ====================

    _GLM_COMMON: Dict[str, Any] = dict(
        adapter=AdapterType.OPENAI,
        base_url="https://open.bigmodel.cn/api/paas/v4",
        api_key_env="ZHIPUAI_API_KEY",
        provider="glm",
    )

    # --- GLM-5 ---

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-5",
            model_type=ModelType.LLM,
            display_name="GLM-5",
            description="744B MoE æ——èˆ°ï¼ˆæ¿€æ´» 40Bï¼‰ï¼Œ200K ä¸Šä¸‹æ–‡ï¼ŒCoding/Agent å¼€æº SOTA",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=128000, max_input_tokens=200000,
            ),
            pricing=ModelPricing(input_per_million=0.50, output_per_million=2.00),
            **_GLM_COMMON,
        )
    )

    # --- GLM-4.7 ---

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-4.7",
            model_type=ModelType.LLM,
            display_name="GLM-4.7",
            description="æ™ºè°±ä¸Šä¸€ä»£æ——èˆ°æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=96000, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.50, output_per_million=2.00),
            **_GLM_COMMON,
        )
    )

    # --- GLM-4.6 ---

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-4.6",
            model_type=ModelType.LLM,
            display_name="GLM-4.6",
            description="æ™ºè°±ä¸Šä¸€ä»£æ——èˆ°æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=96000, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.50, output_per_million=2.00),
            **_GLM_COMMON,
        )
    )

    # --- GLM-4.5 ç³»åˆ— ---

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-4.5",
            model_type=ModelType.LLM,
            display_name="GLM-4.5",
            description="355B MoE æ——èˆ°ï¼Œ128K ä¸Šä¸‹æ–‡ï¼ŒAgent/Coding æœ€å¼º",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=96000, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.20, output_per_million=1.10),
            **_GLM_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-4.5-air",
            model_type=ModelType.LLM,
            display_name="GLM-4.5-Air",
            description="106B MoE è½»é‡æ——èˆ°ï¼Œé«˜æ€§ä»·æ¯”",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=96000, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.08, output_per_million=0.40),
            **_GLM_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-4.5-x",
            model_type=ModelType.LLM,
            display_name="GLM-4.5-X",
            description="é«˜æ€§èƒ½åŠ é€Ÿç‰ˆï¼Œè¶…å¿«å“åº”",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=96000, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.50, output_per_million=2.00),
            **_GLM_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-4.5-airx",
            model_type=ModelType.LLM,
            display_name="GLM-4.5-AirX",
            description="è½»é‡åŠ é€Ÿç‰ˆï¼Œä½å»¶è¿Ÿé«˜å¹¶å‘",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=96000, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.10, output_per_million=0.50),
            **_GLM_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-4.5-flash",
            model_type=ModelType.LLM,
            display_name="GLM-4.5-Flash",
            description="å…è´¹å¿«é€Ÿæ¨¡å‹ï¼ŒCoding/Agent/Reasoning",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=96000, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.0, output_per_million=0.0),
            **_GLM_COMMON,
        )
    )

    # --- GLM è§†è§‰æ¨¡å‹ ---

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-4.6v",
            model_type=ModelType.VLM,
            display_name="GLM-4.6V",
            description="æ™ºè°±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆGLM-4.6 è§†è§‰ç‰ˆï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                supports_streaming=True,
                max_tokens=96000, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.50, output_per_million=2.00),
            **_GLM_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="glm-4.5v",
            model_type=ModelType.VLM,
            display_name="GLM-4.5V",
            description="æ™ºè°±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆGLM-4.5 è§†è§‰ç‰ˆï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                supports_streaming=True,
                max_tokens=96000, max_input_tokens=128000,
            ),
            pricing=ModelPricing(input_per_million=0.20, output_per_million=1.10),
            **_GLM_COMMON,
        )
    )

    # ==================== Gemini ç³»åˆ— ====================

    _GEMINI_COMMON: Dict[str, Any] = dict(
        adapter=AdapterType.GEMINI,
        base_url="https://generativelanguage.googleapis.com/v1beta",
        api_key_env="GOOGLE_API_KEY",
        provider="gemini",
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gemini-2.5-pro",
            model_type=ModelType.VLM,
            display_name="Gemini 2.5 Pro",
            description="Google æ——èˆ°æ¨ç†æ¨¡å‹ï¼Œæ”¯æŒå¤šæ¨¡æ€",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=65536, max_input_tokens=1048576,
            ),
            pricing=ModelPricing(input_per_million=1.25, output_per_million=10.0),
            **_GEMINI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gemini-2.5-flash",
            model_type=ModelType.VLM,
            display_name="Gemini 2.5 Flash",
            description="Google é«˜æ€§ä»·æ¯”å¿«é€Ÿæ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=65536, max_input_tokens=1048576,
            ),
            pricing=ModelPricing(input_per_million=0.15, output_per_million=0.60),
            **_GEMINI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="gemini-2.5-flash-native-audio",
            model_type=ModelType.AUDIO,
            display_name="Gemini 2.5 Flash Native Audio",
            description="Google å…¨æ¨¡æ€éŸ³é¢‘æ¨¡å‹ï¼Œæ”¯æŒéŸ³é¢‘/è§†é¢‘è¾“å…¥è¾“å‡º",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_audio=True,
                max_tokens=65536, max_input_tokens=1048576,
            ),
            pricing=ModelPricing(input_per_million=0.15, output_per_million=0.60),
            **_GEMINI_COMMON,
        )
    )

    # ==================== DeepSeek ç³»åˆ— ====================

    _DEEPSEEK_COMMON: Dict[str, Any] = dict(
        adapter=AdapterType.OPENAI,
        base_url="https://api.deepseek.com/v1",
        api_key_env="DEEPSEEK_API_KEY",
        provider="deepseek",
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="deepseek-chat",
            model_type=ModelType.LLM,
            display_name="DeepSeek Chat (V3)",
            description="DeepSeek å¯¹è¯æ¨¡å‹ï¼Œ64K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=False,
                max_tokens=8192, max_input_tokens=64000,
            ),
            pricing=ModelPricing(
                input_per_million=0.27, output_per_million=1.10,
                cache_read_per_million=0.07,
            ),
            **_DEEPSEEK_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="deepseek-reasoner",
            model_type=ModelType.LLM,
            display_name="DeepSeek Reasoner (R1)",
            description="DeepSeek æ¨ç†æ¨¡å‹ï¼Œæ”¯æŒ CoT",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=64000, max_input_tokens=64000,
            ),
            pricing=ModelPricing(
                input_per_million=0.55, output_per_million=2.19,
                cache_read_per_million=0.14,
            ),
            **_DEEPSEEK_COMMON,
        )
    )

    # ==================== Kimi (Moonshot) ç³»åˆ— ====================

    _KIMI_COMMON: Dict[str, Any] = dict(
        adapter=AdapterType.OPENAI,
        base_url="https://api.moonshot.cn/v1",
        api_key_env="MOONSHOT_API_KEY",
        provider="kimi",
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="kimi-k2.5",
            model_type=ModelType.VLM,
            display_name="Kimi K2.5",
            description="Kimi è¿„ä»Šæœ€å…¨èƒ½æ¨¡å‹ï¼ŒåŸç”Ÿå¤šæ¨¡æ€ï¼Œæ”¯æŒè§†è§‰ä¸æ–‡æœ¬ã€æ€è€ƒä¸éæ€è€ƒï¼Œ256K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=True,
                max_tokens=33000, max_input_tokens=262144,
            ),
            pricing=ModelPricing(
                input_per_million=0.56, output_per_million=2.92,
                cache_read_per_million=0.10,
            ),
            **_KIMI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="kimi-k2-0905-preview",
            model_type=ModelType.LLM,
            display_name="Kimi K2 0905",
            description="Kimi K2 å‡çº§ç‰ˆï¼Œæ›´å¼º Agentic Coding ä¸ä¸Šä¸‹æ–‡ç†è§£ï¼Œ256K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=16384, max_input_tokens=262144,
            ),
            pricing=ModelPricing(
                input_per_million=0.56, output_per_million=2.22,
                cache_read_per_million=0.14,
            ),
            **_KIMI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="kimi-k2-0711-preview",
            model_type=ModelType.LLM,
            display_name="Kimi K2 0711",
            description="Kimi K2 åŸºç¡€æ¨¡å‹ï¼ŒMoE 1T å‚æ•°ï¼Œ128K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=16384, max_input_tokens=131072,
            ),
            pricing=ModelPricing(
                input_per_million=0.56, output_per_million=2.22,
                cache_read_per_million=0.14,
            ),
            **_KIMI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="kimi-k2-turbo-preview",
            model_type=ModelType.LLM,
            display_name="Kimi K2 Turbo",
            description="Kimi K2 é«˜é€Ÿç‰ˆï¼Œè¾“å‡ºæœ€é«˜ 100 tok/sï¼Œå¯¹æ ‡æœ€æ–° K2ï¼Œ256K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=16384, max_input_tokens=262144,
            ),
            pricing=ModelPricing(
                input_per_million=1.11, output_per_million=8.06,
                cache_read_per_million=0.14,
            ),
            **_KIMI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="kimi-k2-thinking",
            model_type=ModelType.LLM,
            display_name="Kimi K2 Thinking",
            description="Kimi K2 æ·±åº¦æ¨ç†æ¨¡å‹ï¼Œé€šç”¨ Agentic + æ¨ç†èƒ½åŠ›ï¼Œ256K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=16384, max_input_tokens=262144,
            ),
            pricing=ModelPricing(
                input_per_million=0.56, output_per_million=2.22,
                cache_read_per_million=0.14,
            ),
            **_KIMI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="kimi-k2-thinking-turbo",
            model_type=ModelType.LLM,
            display_name="Kimi K2 Thinking Turbo",
            description="Kimi K2 æ·±åº¦æ¨ç†é«˜é€Ÿç‰ˆï¼Œé€‚åˆéœ€è¦æ·±åº¦æ¨ç†ä¸”è¿½æ±‚é«˜é€Ÿçš„åœºæ™¯ï¼Œ256K ä¸Šä¸‹æ–‡",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                max_tokens=16384, max_input_tokens=262144,
            ),
            pricing=ModelPricing(
                input_per_million=1.11, output_per_million=8.06,
                cache_read_per_million=0.14,
            ),
            **_KIMI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="moonshot-v1-128k-vision-preview",
            model_type=ModelType.VLM,
            display_name="Moonshot v1 128K Vision",
            description="Moonshot AI å¤šæ¨¡æ€è§†è§‰æ¨¡å‹ï¼Œæ”¯æŒå›¾ç‰‡ç†è§£ï¼ˆ128Kï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=True, supports_thinking=False,
                max_tokens=8192, max_input_tokens=131072,
            ),
            pricing=ModelPricing(input_per_million=1.39, output_per_million=4.17),
            **_KIMI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="moonshot-v1-128k",
            model_type=ModelType.LLM,
            display_name="Moonshot v1 128K",
            description="Moonshot AI é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆ128Kï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=False,
                max_tokens=8192, max_input_tokens=131072,
            ),
            pricing=ModelPricing(input_per_million=1.39, output_per_million=4.17),
            **_KIMI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="moonshot-v1-32k",
            model_type=ModelType.LLM,
            display_name="Moonshot v1 32K",
            description="Moonshot AI æ ‡å‡†æ¨¡å‹ï¼ˆ32Kï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=False,
                max_tokens=8192, max_input_tokens=32768,
            ),
            pricing=ModelPricing(input_per_million=0.69, output_per_million=2.78),
            **_KIMI_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="moonshot-v1-8k",
            model_type=ModelType.LLM,
            display_name="Moonshot v1 8K",
            description="Moonshot AI è½»é‡æ¨¡å‹ï¼ˆ8Kï¼‰",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=False,
                max_tokens=4096, max_input_tokens=8192,
            ),
            pricing=ModelPricing(input_per_million=0.28, output_per_million=1.39),
            **_KIMI_COMMON,
        )
    )

    # ==================== MiniMax ç³»åˆ—ï¼ˆAnthropic API å…¼å®¹ï¼‰ ====================

    _MINIMAX_COMMON: Dict[str, Any] = dict(
        adapter=AdapterType.CLAUDE,
        base_url="https://api.minimaxi.com/anthropic",
        api_key_env="MINIMAX_API_KEY",
        provider="minimax",
    )

    # --- MiniMax M2.5 ç³»åˆ— ---

    ModelRegistry.register(
        ModelConfig(
            model_name="MiniMax-M2.5",
            model_type=ModelType.LLM,
            display_name="MiniMax M2.5",
            description="MiniMax æœ€æ–°æ——èˆ°æ¨¡å‹ï¼Œç¼–ç ä¸æ™ºèƒ½ä½“ä»»åŠ¡ SOTAï¼Œ~60 tps",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=32768, max_input_tokens=204800,
            ),
            pricing=ModelPricing(input_per_million=0.30, output_per_million=1.10),
            **_MINIMAX_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="MiniMax-M2.5-highspeed",
            model_type=ModelType.LLM,
            display_name="MiniMax M2.5 Highspeed",
            description="MiniMax M2.5 æé€Ÿç‰ˆï¼Œæ€§èƒ½ä¸å˜ï¼Œ~100 tps",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=32768, max_input_tokens=204800,
            ),
            pricing=ModelPricing(input_per_million=0.30, output_per_million=1.10),
            **_MINIMAX_COMMON,
        )
    )

    # --- MiniMax M2 ç³»åˆ— ---

    ModelRegistry.register(
        ModelConfig(
            model_name="MiniMax-M2.1",
            model_type=ModelType.LLM,
            display_name="MiniMax M2.1",
            description="MiniMax æ——èˆ°æ¨¡å‹ï¼Œå¤šè¯­è¨€ç¼–ç¨‹ï¼Œ~60 tps",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=32768,
            ),
            pricing=ModelPricing(input_per_million=0.15, output_per_million=1.10),
            **_MINIMAX_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="MiniMax-M2.1-lightning",
            model_type=ModelType.LLM,
            display_name="MiniMax M2.1 Lightning",
            description="MiniMax æé€Ÿç‰ˆï¼Œ~100 tps",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=32768,
            ),
            pricing=ModelPricing(input_per_million=0.10, output_per_million=0.70),
            **_MINIMAX_COMMON,
        )
    )

    ModelRegistry.register(
        ModelConfig(
            model_name="MiniMax-M2",
            model_type=ModelType.LLM,
            display_name="MiniMax M2",
            description="MiniMax Agent/Coding ä¸“ç²¾æ¨¡å‹",
            capabilities=ModelCapabilities(
                supports_tools=True, supports_vision=False, supports_thinking=True,
                supports_streaming=True,
                max_tokens=32768,
            ),
            pricing=ModelPricing(input_per_million=0.15, output_per_million=1.10),
            **_MINIMAX_COMMON,
        )
    )

    logger.debug(f"ğŸ“¦ é¢„ç½®æ¨¡å‹æ³¨å†Œå®Œæˆ: {len(ModelRegistry._models)} ä¸ªæ¨¡å‹")
