"""
Claude LLM æœåŠ¡å®ç°

å°è£… Claude çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼š
- Extended Thinking
- Prompt Caching
- Memory Tool
- Bash Tool / Text Editor
- Web Search
- Streaming
- Tool Search
- Code Execution
- Skills API (Custom Skills)
- Files API (æ–‡ä»¶ä¸Šä¼ /ä¸‹è½½)
- Citations (å¼•ç”¨)

å‚è€ƒï¼š
- https://platform.claude.com/docs/en/build-with-claude/overview
- https://platform.claude.com/docs/en/api/overview
- https://docs.claude.com/en/docs/build-with-claude/skills
- https://docs.claude.com/en/docs/build-with-claude/citations
"""

import json
import os
import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Union

import aiofiles
import anthropic
import httpx

from core.tool.registry_config import get_frequent_tools  # ğŸ†• ä»ç»Ÿä¸€é…ç½®è¯»å–
from infra.resilience import with_retry  # ğŸ†• V7.3: ä½¿ç”¨ç»Ÿä¸€çš„é‡è¯•æœºåˆ¶
from logger import get_logger
from utils.message_utils import messages_to_dict_list

from .adaptor import ClaudeAdaptor
from .base import BaseLLMService, LLMConfig, LLMProvider, LLMResponse, Message, ToolType

# ============================================================
# Files API æ•°æ®ç»“æ„
# ============================================================


@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯"""

    file_id: str
    filename: str
    size_bytes: int
    mime_type: str
    created_at: str
    downloadable: bool = True


logger = get_logger("llm.claude")

# è¯¦ç»†æ—¥å¿—å¼€å…³ï¼šè®¾ç½® LLM_DEBUG_VERBOSE=1 å¯æ‰“å°å®Œæ•´è¯·æ±‚/å“åº”
LLM_DEBUG_VERBOSE = os.getenv("LLM_DEBUG_VERBOSE", "").lower() in ("1", "true", "yes")


class ClaudeLLMService(BaseLLMService):
    """
    Claude LLM æœåŠ¡å®ç°

    æ”¯æŒçš„åŠŸèƒ½ï¼š
    - Extended Thinking: æ·±åº¦æ¨ç†èƒ½åŠ›
    - Prompt Caching: å‡å°‘é‡å¤ token æ¶ˆè€—
    - Client Tools: computer_use
    - Context Editing: è‡ªåŠ¨æ¸…ç†é•¿ä¸Šä¸‹æ–‡

    æ³¨ï¼šæ‰€æœ‰æœåŠ¡å™¨å·¥å…·å·²ç§»é™¤ï¼Œæœç´¢é€šè¿‡ Skills æä¾›

    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    config = LLMConfig(
        provider=LLMProvider.CLAUDE,
        model="claude-sonnet-4-5-20250929",
        api_key=os.getenv("ANTHROPIC_API_KEY"),
        enable_thinking=True
    )
    llm = ClaudeLLMService(config)

    response = await llm.create_message_async(
        messages=[Message(role="user", content="Hello")],
        system="You are helpful"
    )
    ```
    """

    # Claude åŸç”Ÿå·¥å…·çš„ API æ ¼å¼æ˜ å°„
    # æ³¨ï¼šæ‰€æœ‰æœåŠ¡å™¨å·¥å…·å·²ç§»é™¤ï¼ˆweb_search, code_execution, tool_search, memoryï¼‰
    # æœç´¢é€šè¿‡ Skills æä¾›
    NATIVE_TOOLS = {
        # Client-side Tools
        "computer": {
            "type": "computer_20250124",
            "name": "computer",
            "display_width_px": 1024,
            "display_height_px": 768,
        },
    }

    def __init__(self, config: LLMConfig):
        """
        åˆå§‹åŒ– Claude æœåŠ¡

        Args:
            config: LLM é…ç½®
        """
        self.config = config

        # æ¶ˆæ¯é€‚é…å™¨ï¼ˆç»Ÿä¸€å¤„ç†æ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼‰
        self._adaptor = ClaudeAdaptor()

        # ğŸ†• V5.0: ä½¿ç”¨é…ç½®ä¸­çš„è¶…æ—¶å’Œé‡è¯•è®¾ç½®
        timeout = getattr(config, "timeout", 120.0)
        max_retries = getattr(config, "max_retries", 3)

        # ğŸ” DEBUG: æ‰“å° API Key ä¿¡æ¯ï¼ˆä»…æ˜¾ç¤ºå‰8ä½å’Œå4ä½ï¼‰
        api_key = config.api_key or ""
        if api_key:
            masked_key = f"{api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else "***"
            logger.info(f"ğŸ”‘ Claude API Key: {masked_key} (é•¿åº¦: {len(api_key)})")
        else:
            logger.warning("âš ï¸ Claude API Key ä¸ºç©ºï¼")

        # ğŸ†• æ”¯æŒè‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆå¦‚ä¸‡ç•Œæ–¹èˆŸï¼‰
        # ä¼˜å…ˆçº§ï¼šconfig.base_url > ANTHROPIC_BASE_URL ç¯å¢ƒå˜é‡ > Noneï¼ˆä½¿ç”¨å®˜æ–¹é»˜è®¤ï¼‰
        base_url = getattr(config, "base_url", None) or os.getenv("ANTHROPIC_BASE_URL") or None

        # ğŸ”§ å¦‚æœ base_url æ˜¯å®˜æ–¹é»˜è®¤åœ°å€ï¼Œå°†å…¶è®¾ç½®ä¸º Noneï¼ˆè®© SDK ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        # è¿™æ ·å¯ä»¥é¿å… SDK çš„è®¤è¯æ£€æŸ¥é—®é¢˜
        if base_url == "https://api.anthropic.com":
            base_url = None

        # ğŸ”§ ä¸‡ç•Œæ–¹èˆŸéœ€è¦ Authorization: Bearer è®¤è¯ï¼Œè€Œä¸æ˜¯ x-api-key
        # æ£€æµ‹æ˜¯å¦ä½¿ç”¨ä¸‡ç•Œæ–¹èˆŸç«¯ç‚¹
        is_wanjie = base_url and "wanjiedata.com" in base_url

        if base_url:
            logger.info(f"ğŸŒ ä½¿ç”¨è‡ªå®šä¹‰ API ç«¯ç‚¹: {base_url}")
            if is_wanjie:
                logger.info("ğŸ”‘ æ£€æµ‹åˆ°ä¸‡ç•Œæ–¹èˆŸï¼Œä½¿ç”¨ Bearer Token è®¤è¯")

        # å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆå¢åŠ  timeout å’Œé‡è¯•é…ç½®ï¼‰
        # æ³¨æ„ï¼šå¯¹äºæµå¼å“åº”ï¼Œtimeout æ˜¯é¦–ä¸ªå“åº”çš„è¶…æ—¶ï¼Œä¸æ˜¯æ•´ä½“è¶…æ—¶
        if is_wanjie:
            # ä¸‡ç•Œæ–¹èˆŸï¼šä½¿ç”¨ auth_tokenï¼ˆBearer è®¤è¯ï¼‰
            self.async_client = anthropic.AsyncAnthropic(
                auth_token=config.api_key,
                base_url=base_url,
                timeout=timeout,
                max_retries=max_retries,
            )
            self.sync_client = anthropic.Anthropic(
                auth_token=config.api_key,
                base_url=base_url,
                timeout=timeout,
                max_retries=max_retries,
            )
        else:
            # å®˜æ–¹ APIï¼šä½¿ç”¨ api_keyï¼ˆx-api-key è®¤è¯ï¼‰
            # å½“ base_url ä¸º None æ—¶ï¼ŒSDK ä¼šä½¿ç”¨é»˜è®¤çš„å®˜æ–¹ç«¯ç‚¹
            self.async_client = anthropic.AsyncAnthropic(
                api_key=config.api_key, base_url=base_url, timeout=timeout, max_retries=max_retries
            )
            self.sync_client = anthropic.Anthropic(
                api_key=config.api_key, base_url=base_url, timeout=timeout, max_retries=max_retries
            )

        # Beta åŠŸèƒ½é…ç½®
        self._betas: List[str] = []

        # è°ƒç”¨æ–¹å¼é…ç½®
        self._programmatic_mode = False

        # Context Editing é…ç½®
        self._context_editing_enabled = False
        self._context_editing_config: Dict[str, Any] = {}

        # å·¥å…·æ³¨å†Œè¡¨ï¼ˆç”¨äºè‡ªå®šä¹‰å·¥å…·ï¼‰
        self._tool_registry: Dict[str, Dict[str, Any]] = {}

        # è‡ªå®šä¹‰å·¥å…·å­˜å‚¨
        self._custom_tools: List[Dict[str, Any]] = []

        # Citations é…ç½®
        self._citations_enabled = False

    # ============================================================
    # Beta Headers ç®¡ç†
    # ============================================================

    def _add_beta(self, beta_header: str) -> None:
        """æ·»åŠ  Beta Header"""
        if beta_header not in self._betas:
            self._betas.append(beta_header)

    def _remove_beta(self, beta_header: str) -> None:
        """ç§»é™¤ Beta Header"""
        if beta_header in self._betas:
            self._betas.remove(beta_header)

    # ============================================================
    # åŠŸèƒ½å¼€å…³
    # ============================================================

    def enable_context_editing(
        self, mode: str = "progressive", clear_threshold: int = 150000, retain_tool_uses: int = 10
    ):
        """
        å¯ç”¨ Context Editing

        Args:
            mode: æ¸…ç†æ¨¡å¼ ("progressive" | "aggressive")
            clear_threshold: è§¦å‘æ¸…ç†çš„ token é˜ˆå€¼
            retain_tool_uses: ä¿ç•™æœ€è¿‘ N ä¸ªå·¥å…·è°ƒç”¨
        """
        self._context_editing_enabled = True
        self._context_editing_config = {
            "mode": mode,
            "clear_threshold": clear_threshold,
            "retain_tool_uses": retain_tool_uses,
        }
        self._add_beta("context-management-2025-06-27")

    def disable_context_editing(self) -> None:
        """ç¦ç”¨ Context Editing"""
        self._context_editing_enabled = False
        self._context_editing_config = {}
        self._remove_beta("context-management-2025-06-27")

    def enable_programmatic_tool_calling(self) -> None:
        """å¯ç”¨ Programmatic Tool Calling æ¨¡å¼"""
        self._programmatic_mode = True

    def disable_programmatic_tool_calling(self) -> None:
        """ç¦ç”¨ Programmatic Tool Calling æ¨¡å¼"""
        self._programmatic_mode = False

    # ============================================================
    # è‡ªå®šä¹‰å·¥å…·ç®¡ç†
    # ============================================================

    def add_custom_tool(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:
        """
        æ·»åŠ è‡ªå®šä¹‰å·¥å…·

        Args:
            name: å·¥å…·åç§°
            description: å·¥å…·æè¿°
            input_schema: è¾“å…¥å‚æ•° schemaï¼ˆJSON Schema æ ¼å¼ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåå·¥å…·
        for i, tool in enumerate(self._custom_tools):
            if tool["name"] == name:
                # æ›´æ–°ç°æœ‰å·¥å…·
                self._custom_tools[i] = {
                    "name": name,
                    "description": description,
                    "input_schema": input_schema,
                }
                logger.debug(f"æ›´æ–°è‡ªå®šä¹‰å·¥å…·: {name}")
                return

        # æ·»åŠ æ–°å·¥å…·
        self._custom_tools.append(
            {"name": name, "description": description, "input_schema": input_schema}
        )
        logger.debug(f"æ³¨å†Œè‡ªå®šä¹‰å·¥å…·: {name}")

    def remove_custom_tool(self, name: str) -> bool:
        """
        ç§»é™¤è‡ªå®šä¹‰å·¥å…·

        Args:
            name: å·¥å…·åç§°

        Returns:
            æ˜¯å¦æˆåŠŸç§»é™¤
        """
        for i, tool in enumerate(self._custom_tools):
            if tool["name"] == name:
                self._custom_tools.pop(i)
                logger.debug(f"ç§»é™¤è‡ªå®šä¹‰å·¥å…·: {name}")
                return True
        return False

    def get_custom_tools(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰è‡ªå®šä¹‰å·¥å…·

        Returns:
            è‡ªå®šä¹‰å·¥å…·åˆ—è¡¨
        """
        return self._custom_tools.copy()

    def clear_custom_tools(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰è‡ªå®šä¹‰å·¥å…·"""
        self._custom_tools.clear()
        logger.debug("æ¸…ç©ºæ‰€æœ‰è‡ªå®šä¹‰å·¥å…·")

    # ============================================================
    # å·¥å…·å¤„ç†
    # ============================================================

    def get_native_tool(self, tool_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å– Claude åŸç”Ÿå·¥å…·çš„ API æ ¼å¼

        Args:
            tool_name: å·¥å…·åç§°

        Returns:
            å·¥å…· schemaï¼Œå¦‚æœä¸æ˜¯åŸç”Ÿå·¥å…·åˆ™è¿”å› None
        """
        if tool_name in self.NATIVE_TOOLS:
            schema = self.NATIVE_TOOLS[tool_name].copy()
            return schema

        return None

    def get_claude_native_tool(self, tool_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å– Claude åŸç”Ÿå·¥å…·çš„ API æ ¼å¼ï¼ˆåˆ«åæ–¹æ³•ï¼Œå…¼å®¹ llm_service.pyï¼‰

        Args:
            tool_name: å·¥å…·åç§°

        Returns:
            å·¥å…· schemaï¼Œå¦‚æœä¸æ˜¯åŸç”Ÿå·¥å…·åˆ™è¿”å› None
        """
        return self.get_native_tool(tool_name)

    def convert_to_tool_schema(self, capability: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°†èƒ½åŠ›å®šä¹‰è½¬æ¢ä¸º Claude API æ ¼å¼

        Args:
            capability: èƒ½åŠ›å®šä¹‰

        Returns:
            Claude API æ ¼å¼çš„å·¥å…·å®šä¹‰
        """
        name = capability.get("name", "")

        # æ£€æŸ¥æ˜¯å¦æ˜¯åŸç”Ÿå·¥å…·
        native_tool = self.get_native_tool(name)
        if native_tool:
            return native_tool

        # è‡ªå®šä¹‰å·¥å…·
        input_schema = capability.get(
            "input_schema", {"type": "object", "properties": {}, "required": []}
        )
        description = capability.get("metadata", {}).get("description", f"Tool: {name}")

        tool_def = {"name": name, "description": description, "input_schema": input_schema}

        # ğŸ”§ ä¸åœ¨è¿™é‡Œæ·»åŠ  cache_controlï¼Œç»Ÿä¸€åœ¨ create_message_* æ–¹æ³•ä¸­å¤„ç†
        # Claude API é™åˆ¶æœ€å¤š 4 ä¸ªå¸¦ cache_control çš„ block

        return tool_def

    def convert_to_claude_tool(self, capability: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°† capabilities.yaml ä¸­çš„å·¥å…·å®šä¹‰è½¬æ¢ä¸º Claude API æ ¼å¼ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰

        Args:
            capability: capabilities.yaml ä¸­çš„èƒ½åŠ›å®šä¹‰

        Returns:
            Claude API æ ¼å¼çš„å·¥å…·å®šä¹‰
        """
        return self.convert_to_tool_schema(capability)

    def configure_deferred_tools(
        self, tools: List[Dict[str, Any]], frequent_tools: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        é…ç½®å»¶è¿ŸåŠ è½½çš„å·¥å…·

        Args:
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            frequent_tools: å¸¸ç”¨å·¥å…·åç§°ï¼ˆä¸å»¶è¿ŸåŠ è½½ï¼‰

        Returns:
            é…ç½®å¥½çš„å·¥å…·åˆ—è¡¨
        """
        if frequent_tools is None:
            # ğŸ†• ä» config/tool_registry.yaml ç»Ÿä¸€é…ç½®è¯»å–
            frequent_tools = get_frequent_tools()

        configured = []

        # é…ç½®å·¥å…·
        for tool in tools:
            tool_name = tool.get("name", "")
            tool_copy = tool.copy()

            if tool_name not in frequent_tools:
                tool_copy["defer_loading"] = True

            configured.append(tool_copy)

        return configured

    def _format_tools(self, tools: List[Union[ToolType, str, Dict]]) -> List[Dict[str, Any]]:
        """
        æ ¼å¼åŒ–å·¥å…·åˆ—è¡¨

        æ”¯æŒä¸‰ç§è¾“å…¥ï¼š
        1. ToolType æšä¸¾
        2. å­—ç¬¦ä¸²
        3. å®Œæ•´ schema å­—å…¸
        """
        formatted = []

        for idx, tool in enumerate(tools):
            try:
                if isinstance(tool, ToolType):
                    schema = self.get_native_tool(tool.value)
                    if schema:
                        formatted.append(schema)
                    else:
                        raise ValueError(f"Unknown ToolType: {tool}")

                elif isinstance(tool, str):
                    schema = self.get_native_tool(tool)
                    if schema:
                        formatted.append(schema)
                    else:
                        raise ValueError(f"Unknown tool name: {tool}")

                elif isinstance(tool, dict):
                    self._validate_tool_dict(tool, idx)
                    formatted.append(tool)

                else:
                    raise ValueError(f"Invalid tool format: {tool}")

                # éªŒè¯ JSON å¯åºåˆ—åŒ–
                json.dumps(formatted[-1])

            except Exception as e:
                logger.error(f"å¤„ç†å·¥å…· #{idx} æ—¶å‡ºé”™: {e}")
                raise

        return formatted

    def _validate_tool_dict(self, tool_dict: Dict[str, Any], index: int) -> None:
        """éªŒè¯å·¥å…·å­—å…¸æ˜¯å¦åŒ…å«ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡"""
        for key, value in tool_dict.items():
            if isinstance(value, ToolType):
                raise ValueError(f"Tool #{index} contains ToolType enum in key '{key}': {value}")
            elif isinstance(value, dict):
                self._validate_tool_dict(value, index)
            elif isinstance(value, list):
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        self._validate_tool_dict(item, index)
                    elif isinstance(item, ToolType):
                        raise ValueError(f"Tool #{index} contains ToolType in list '{key}[{i}]'")

    # ============================================================
    # ç¼“å­˜æ–­ç‚¹ç®¡ç†
    # ============================================================

    def _apply_cache_breakpoints(self, system_blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ğŸ†• å‰ç¼€ç¼“å­˜ä¼˜åŒ–ï¼šæ™ºèƒ½æ·»åŠ å¤šå±‚ç¼“å­˜æ–­ç‚¹

        Claude çš„ç¼“å­˜æ˜¯ç´¯ç§¯å¼å‰ç¼€åŒ¹é…ï¼š
        - ä»å¼€å¤´åˆ°æ–­ç‚¹çš„æ•´ä¸ªå‰ç¼€åºåˆ—ä¼šè¢«ç¼“å­˜
        - å¤šä¸ªæ–­ç‚¹å¯ä»¥å®ç°åˆ†çº§ç¼“å­˜ï¼Œæé«˜ä¸åŒåœºæ™¯çš„å‘½ä¸­ç‡
        - æœ€å¤šæ”¯æŒ 4 ä¸ªæ–­ç‚¹

        ç­–ç•¥ï¼š
        - æ ¹æ® _cache_layer å…ƒæ•°æ®è¯†åˆ«æ¯å±‚çš„ç¼“å­˜è¾¹ç•Œ
        - åœ¨æ¯å±‚çš„æœ€åä¸€ä¸ª block æ·»åŠ  cache_control
        - åŠ¨æ€å†…å®¹ï¼ˆ_cache_layer=0ï¼‰ä¸æ·»åŠ æ–­ç‚¹

        ç¼“å­˜æ•ˆæœï¼š
        - æ–­ç‚¹ 1 (æ¡†æ¶è§„åˆ™): è·¨ Agentã€è·¨ç”¨æˆ·å…±äº«
        - æ–­ç‚¹ 2 (å®ä¾‹æç¤ºè¯): åŒ Agent ä¸åŒç”¨æˆ·å…±äº«
        - æ–­ç‚¹ 3 (Skills+å·¥å…·): åŒ Agent åŒç”¨æˆ·ä¸åŒè½®æ¬¡å…±äº«

        Args:
            system_blocks: å¸¦ _cache_layer å…ƒæ•°æ®çš„ system blocks

        Returns:
            æ·»åŠ äº† cache_control çš„ system blocks
        """
        if not system_blocks:
            return system_blocks

        # ğŸ” åˆ†ææ¯å±‚çš„è¾¹ç•Œ
        # æ‰¾åˆ°æ¯ä¸ª cache_layer çš„æœ€åä¸€ä¸ª block ç´¢å¼•
        layer_boundaries: Dict[int, int] = {}  # {layer: last_index}

        for idx, block in enumerate(system_blocks):
            layer = block.get("_cache_layer", 0)
            if layer > 0:  # åªå¤„ç†éœ€è¦ç¼“å­˜çš„å±‚
                layer_boundaries[layer] = idx

        # ğŸ”§ åœ¨æ¯å±‚è¾¹ç•Œæ·»åŠ ç¼“å­˜æ–­ç‚¹ï¼ˆæœ€å¤š 4 ä¸ªï¼‰
        # Claude API é™åˆ¶æœ€å¤š 4 ä¸ªå¸¦ cache_control çš„ block
        breakpoint_count = 0
        max_breakpoints = 4

        # æŒ‰å±‚çº§æ’åºï¼ˆ1, 2, 3...ï¼‰ï¼Œä¾æ¬¡æ·»åŠ æ–­ç‚¹
        for layer in sorted(layer_boundaries.keys()):
            if breakpoint_count >= max_breakpoints:
                logger.warning(f"âš ï¸ å·²è¾¾åˆ°ç¼“å­˜æ–­ç‚¹ä¸Šé™ ({max_breakpoints})ï¼Œè·³è¿‡ Layer {layer}")
                break

            idx = layer_boundaries[layer]
            system_blocks[idx]["cache_control"] = {"type": "ephemeral", "ttl": "1h"}
            breakpoint_count += 1

            # è®¡ç®—è¯¥å±‚çš„ token æ•°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            layer_text = system_blocks[idx].get("text", "")
            layer_tokens = self.count_tokens(layer_text)

            logger.debug(
                f"ğŸ”’ ç¼“å­˜æ–­ç‚¹ {breakpoint_count}: block[{idx}] "
                f"(Layer {layer}, ~{layer_tokens:,} tokens, 1h TTL)"
            )

        # ğŸ“Š ç¼“å­˜ç­–ç•¥æ—¥å¿—
        total_cached_tokens = sum(
            self.count_tokens(b.get("text", "")) for b in system_blocks if b.get("cache_control")
        )
        uncached_blocks = [i for i, b in enumerate(system_blocks) if not b.get("cache_control")]

        logger.info(
            f"ğŸ—‚ï¸ å‰ç¼€ç¼“å­˜ç­–ç•¥: {breakpoint_count} ä¸ªæ–­ç‚¹, "
            f"~{total_cached_tokens:,} tokens å¯ç¼“å­˜, "
            f"æœªç¼“å­˜ blocks: {uncached_blocks or 'æ— '}"
        )

        return system_blocks

    # ============================================================
    # æ ¸å¿ƒ API æ–¹æ³•
    # ============================================================

    @with_retry(
        max_retries=3,
        base_delay=1.0,
        retryable_errors=(
            # Anthropic ç‰¹å®šå¼‚å¸¸
            anthropic.APIConnectionError,
            anthropic.APITimeoutError,
            anthropic.RateLimitError,
            # HTTPX åº•å±‚å¼‚å¸¸
            httpx.RemoteProtocolError,
            httpx.ConnectError,
            httpx.TimeoutException,
        ),
    )
    async def create_message_async(
        self,
        messages: List[Message],
        system: Optional[Union[str, List[Dict[str, Any]]]] = None,
        tools: Optional[List[Union[ToolType, str, Dict]]] = None,
        invocation_type: Optional[str] = None,
        override_thinking: Optional[bool] = None,
        is_probe: bool = False,
        **kwargs,
    ) -> LLMResponse:
        """
        åˆ›å»ºæ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰

        ğŸ†• V7.3: è‡ªåŠ¨ç½‘ç»œé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰
        - æœ€å¤§é‡è¯• 3 æ¬¡
        - åŸºç¡€å»¶è¿Ÿ 1 ç§’ï¼ˆæŒ‡æ•°å¢é•¿ï¼š1s â†’ 2s â†’ 4sï¼‰
        - è‡ªåŠ¨å¤„ç†ï¼šè¿æ¥é”™è¯¯ã€è¶…æ—¶ã€é™æµï¼ˆ429ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤ºè¯ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
                - str: å•å±‚ç¼“å­˜ï¼ˆå‘åå…¼å®¹ï¼Œå¯ç”¨ç¼“å­˜æ—¶è‡ªåŠ¨åŒ…è£…ä¸º 5 åˆ†é’Ÿ TTLï¼‰
                - List[Dict]: å¤šå±‚ç¼“å­˜ï¼ˆæ”¯æŒè‡ªå®šä¹‰ TTLï¼Œå¦‚ 1hï¼‰
            tools: å·¥å…·åˆ—è¡¨
            invocation_type: è°ƒç”¨æ–¹å¼
            is_probe: æ˜¯å¦ä¸ºæ¢æµ‹è¯·æ±‚ï¼ˆæ¢æµ‹å¤±è´¥ä¸è®°å½• ERRORï¼‰
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆæ”¯æŒ max_tokens, temperature è¦†ç›–ï¼‰

        Returns:
            LLMResponse å“åº”å¯¹è±¡

        Example:
            # å•å±‚ç¼“å­˜ï¼ˆå‘åå…¼å®¹ï¼‰
            response = await llm.create_message_async(messages, system="You are helpful")

            # å¤šå±‚ç¼“å­˜ï¼ˆClaude å›ºå®š 5 åˆ†é’Ÿ TTLï¼‰
            system_blocks = [
                {"type": "text", "text": "æ¡†æ¶è§„åˆ™", "cache_control": {"type": "ephemeral", "ttl": "1h"}},
                {"type": "text", "text": "å®ä¾‹æç¤ºè¯", "cache_control": {"type": "ephemeral", "ttl": "1h"}},
                {"type": "text", "text": "ç”¨æˆ·ç”»åƒ"}  # ä¸ç¼“å­˜
            ]
            response = await llm.create_message_async(messages, system=system_blocks)
        """
        # æ„å»ºè¯·æ±‚å‚æ•°ï¼ˆæ”¯æŒ kwargs è¦†ç›–ï¼‰
        # ä½¿ç”¨ adaptor è½¬æ¢æ¶ˆæ¯ï¼ˆè‡ªåŠ¨å¤„ç† tool_result åˆ†ç¦»ç­‰ï¼‰
        converted = self._adaptor.convert_messages_to_provider(messages)
        formatted_messages = converted["messages"]

        # ğŸ›¡ï¸ æ–­è¨€ï¼šadaptor å±‚å·²ç¡®ä¿æ¶ˆæ¯ä»¥ user ç»“å°¾ï¼Œæ­¤å¤„ä»…æ£€æµ‹å¼‚å¸¸
        if formatted_messages and formatted_messages[-1].get("role") == "assistant":
            logger.error(
                "ğŸ› [Async] adaptor è¾“å‡ºçš„æ¶ˆæ¯ä»ä»¥ assistant ç»“å°¾"
                f"ï¼ˆå…± {len(formatted_messages)} æ¡ï¼‰ï¼Œè¯·æ’æŸ¥ adaptor é€»è¾‘"
            )

        request_params = {
            "model": self.config.model,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "messages": formatted_messages,
        }

        # System promptï¼ˆæ”¯æŒå¤šå±‚ç¼“å­˜ï¼‰
        if system:
            if isinstance(system, list):
                # å¤šå±‚ç¼“å­˜æ ¼å¼ï¼šæ™ºèƒ½æ·»åŠ ç¼“å­˜æ–­ç‚¹
                system_blocks = [
                    block.copy() if isinstance(block, dict) else {"type": "text", "text": block}
                    for block in system
                ]

                if self.config.enable_caching and system_blocks:
                    # ğŸ†• å‰ç¼€ç¼“å­˜ä¼˜åŒ–ï¼šæ ¹æ® _cache_layer å…ƒæ•°æ®æ™ºèƒ½æ·»åŠ æ–­ç‚¹
                    system_blocks = self._apply_cache_breakpoints(system_blocks)

                # æ¸…ç†å…ƒæ•°æ®ï¼ˆClaude API ä¸æ¥å—è‡ªå®šä¹‰å­—æ®µï¼‰
                for block in system_blocks:
                    block.pop("_cache_layer", None)

                request_params["system"] = system_blocks
                logger.debug(f"ğŸ—‚ï¸ ä½¿ç”¨å¤šå±‚ç¼“å­˜ system prompt: {len(system_blocks)} å±‚")
            elif self.config.enable_caching:
                # å­—ç¬¦ä¸²æ ¼å¼ + å¯ç”¨ç¼“å­˜ï¼šè‡ªåŠ¨åŒ…è£…ä¸ºå•å±‚ç¼“å­˜ï¼ˆ1 å°æ—¶ TTLï¼‰
                request_params["system"] = [
                    {
                        "type": "text",
                        "text": system,
                        "cache_control": {"type": "ephemeral", "ttl": "1h"},
                    }
                ]
            else:
                # å­—ç¬¦ä¸²æ ¼å¼ + ç¦ç”¨ç¼“å­˜ï¼šç›´æ¥ä½¿ç”¨
                request_params["system"] = system

        # Extended Thinkingï¼ˆæ”¯æŒåŠ¨æ€è¦†ç›–ï¼‰
        # override_thinking ä¼˜å…ˆçº§é«˜äºé…ç½®ï¼šNone=ä½¿ç”¨é…ç½®, True/False=å¼ºåˆ¶å¼€å¯/å…³é—­
        effective_thinking = (
            override_thinking if override_thinking is not None else self.config.enable_thinking
        )
        if effective_thinking:
            request_params["thinking"] = {
                "type": "enabled",
                "budget_tokens": self.config.thinking_budget,
            }
            request_params["temperature"] = 1.0  # Required for thinking
        else:
            request_params["temperature"] = self.config.temperature

        # Tools
        all_tools = []
        tool_names_seen = set()

        # æ·»åŠ ç”¨æˆ·æŒ‡å®šçš„å·¥å…·
        if tools:
            for tool in self._format_tools(tools):
                tool_name = tool.get("name", "")
                if tool_name and tool_name not in tool_names_seen:
                    all_tools.append(tool)
                    tool_names_seen.add(tool_name)

        # æ·»åŠ è‡ªå®šä¹‰å·¥å…·ï¼ˆé¿å…é‡å¤ï¼‰
        for custom_tool in self._custom_tools:
            tool_name = custom_tool.get("name", "")
            if tool_name and tool_name not in tool_names_seen:
                tool_def = custom_tool.copy()
                all_tools.append(tool_def)
                tool_names_seen.add(tool_name)

        if all_tools:
            # ğŸ”§ ç¼“å­˜ç­–ç•¥ï¼šåªå¯¹æœ€åä¸€ä¸ªå·¥å…·æ·»åŠ  cache_controlï¼ˆ1 å°æ—¶ TTLï¼‰
            # Claude API é™åˆ¶æœ€å¤š 4 ä¸ªå¸¦ cache_control çš„ block
            # å·¥å…·å®šä¹‰åœ¨è¿è¡ŒæœŸç¨³å®šï¼Œä½¿ç”¨è¾ƒé•¿çš„ç¼“å­˜æ—¶é—´
            if self.config.enable_caching and all_tools:
                all_tools[-1] = all_tools[-1].copy()
                all_tools[-1]["cache_control"] = {"type": "ephemeral", "ttl": "1h"}

            request_params["tools"] = all_tools

            # tool_choice: force specific tool usage (e.g. structured output)
            tool_choice = kwargs.get("tool_choice")
            if tool_choice:
                request_params["tool_choice"] = tool_choice

            # è°ƒè¯•æ—¥å¿—
            logger.debug(f"Tools: {[t.get('name', 'unknown') for t in all_tools]}")

        # Context Editing
        if self._context_editing_enabled:
            request_params["context_management"] = self._context_editing_config

        # è°ƒè¯•æ—¥å¿—
        logger.debug(f"ğŸ“¤ LLM è¯·æ±‚: model={self.config.model}, messages={len(messages)}")

        # ğŸš¨ è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°å®Œæ•´ messagesï¼ˆç”¨äºæ’æŸ¥ 403 é”™è¯¯ï¼‰
        logger.info("=" * 80)
        logger.info("ğŸ” [DEBUG-ASYNC] å®Œæ•´ request_params:")
        logger.info(f"   model: {request_params.get('model')}")
        logger.info(f"   max_tokens: {request_params.get('max_tokens')}")
        if "system" in request_params:
            system_val = request_params.get("system")
            if isinstance(system_val, list):
                logger.info(f"   system: (list, {len(system_val)} blocks)")
            else:
                logger.info(f"   system: {str(system_val)[:200]}...")
        logger.info(f"   messages ({len(request_params.get('messages', []))}):")
        for i, msg in enumerate(request_params.get("messages", [])):
            logger.info(f"   â”€â”€ Message [{i}] â”€â”€")
            logger.info(f"      role: {msg.get('role')}")
            content = msg.get("content")
            if isinstance(content, list):
                logger.info(f"      content: (list, {len(content)} blocks)")
                for j, block in enumerate(content):
                    if isinstance(block, dict):
                        block_type = block.get("type", "unknown")
                        if block_type == "text":
                            text_preview = str(block.get("text", ""))[:300]
                            logger.info(f"         [{j}] type=text, text={text_preview}...")
                        elif block_type == "tool_use":
                            logger.info(
                                f"         [{j}] type=tool_use, id={block.get('id')}, name={block.get('name')}"
                            )
                        elif block_type == "tool_result":
                            logger.info(
                                f"         [{j}] type=tool_result, tool_use_id={block.get('tool_use_id')}"
                            )
                        else:
                            logger.info(f"         [{j}] type={block_type}")
                    else:
                        logger.info(f"         [{j}] (non-dict): {str(block)[:100]}...")
            elif isinstance(content, str):
                logger.info(f"      content: {content[:300]}...")
            else:
                logger.info(f"      content: (type={type(content).__name__})")
        logger.info("=" * 80)

        # API è°ƒç”¨
        try:
            if self._betas:
                response = await self.async_client.beta.messages.create(
                    betas=self._betas, **request_params
                )
            else:
                response = await self.async_client.messages.create(**request_params)
        except Exception as e:
            # æ¢æµ‹è¯·æ±‚å¤±è´¥æ—¶ä¸è®°å½• ERRORï¼ˆåœ¨ router.probe ä¸­å·²è®°å½• INFOï¼‰
            if not is_probe:
                logger.error(f"Claude API è°ƒç”¨å¤±è´¥: {e}")
            raise

        # è°ƒè¯•æ—¥å¿—
        logger.debug(f"ğŸ“¥ LLM å“åº”: stop_reason={response.stop_reason}")

        return self._parse_response(response, invocation_type)

    async def create_message_stream(
        self,
        messages: List[Message],
        system: Optional[Union[str, List[Dict[str, Any]]]] = None,
        tools: Optional[List[Union[ToolType, str, Dict]]] = None,
        on_thinking: Optional[Callable[[str], None]] = None,
        on_content: Optional[Callable[[str], None]] = None,
        on_tool_call: Optional[Callable[[Dict], None]] = None,
        override_thinking: Optional[bool] = None,
        **kwargs,
    ) -> AsyncIterator[LLMResponse]:
        """
        åˆ›å»ºæ¶ˆæ¯ï¼ˆæµå¼ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤ºè¯ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
                - str: å•å±‚ç¼“å­˜ï¼ˆå‘åå…¼å®¹ï¼Œå¯ç”¨ç¼“å­˜æ—¶è‡ªåŠ¨åŒ…è£…ä¸º 5 åˆ†é’Ÿ TTLï¼‰
                - List[Dict]: å¤šå±‚ç¼“å­˜ï¼ˆæ”¯æŒè‡ªå®šä¹‰ TTLï¼Œå¦‚ 1hï¼‰
            tools: å·¥å…·åˆ—è¡¨
            on_thinking: thinking å›è°ƒ
            on_content: content å›è°ƒ
            on_tool_call: tool_call å›è°ƒ
            override_thinking: åŠ¨æ€è¦†ç›– thinking é…ç½®ï¼ˆNone ä½¿ç”¨é»˜è®¤é…ç½®ï¼ŒTrue/False å¼ºåˆ¶å¼€å¯/å…³é—­ï¼‰
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆæ”¯æŒ max_tokens è¦†ç›–ï¼‰

        Yields:
            LLMResponse ç‰‡æ®µ
        """
        # æ„å»ºè¯·æ±‚å‚æ•°ï¼ˆæ”¯æŒ kwargs è¦†ç›–ï¼‰
        # ä½¿ç”¨ adaptor è½¬æ¢æ¶ˆæ¯ï¼ˆè‡ªåŠ¨å¤„ç† tool_result åˆ†ç¦»ç­‰ï¼‰
        converted = self._adaptor.convert_messages_to_provider(messages)
        formatted_messages = converted["messages"]

        # ğŸ›¡ï¸ æ–­è¨€ï¼šadaptor å±‚å·²ç¡®ä¿æ¶ˆæ¯ä»¥ user ç»“å°¾ï¼Œæ­¤å¤„ä»…æ£€æµ‹å¼‚å¸¸
        if formatted_messages and formatted_messages[-1].get("role") == "assistant":
            logger.error(
                "ğŸ› [Stream] adaptor è¾“å‡ºçš„æ¶ˆæ¯ä»ä»¥ assistant ç»“å°¾"
                f"ï¼ˆå…± {len(formatted_messages)} æ¡ï¼‰ï¼Œè¯·æ’æŸ¥ adaptor é€»è¾‘"
            )

        request_params = {
            "model": self.config.model,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "messages": formatted_messages,
        }

        # System promptï¼ˆæ”¯æŒå¤šå±‚ç¼“å­˜ï¼Œä¸ create_message_async ä¿æŒä¸€è‡´ï¼‰
        if system:
            if isinstance(system, list):
                # å¤šå±‚ç¼“å­˜æ ¼å¼ï¼šæ™ºèƒ½æ·»åŠ ç¼“å­˜æ–­ç‚¹
                system_blocks = [
                    block.copy() if isinstance(block, dict) else {"type": "text", "text": block}
                    for block in system
                ]

                if self.config.enable_caching and system_blocks:
                    # ğŸ†• å‰ç¼€ç¼“å­˜ä¼˜åŒ–ï¼šæ ¹æ® _cache_layer å…ƒæ•°æ®æ™ºèƒ½æ·»åŠ æ–­ç‚¹
                    system_blocks = self._apply_cache_breakpoints(system_blocks)

                # æ¸…ç†å…ƒæ•°æ®ï¼ˆClaude API ä¸æ¥å—è‡ªå®šä¹‰å­—æ®µï¼‰
                for block in system_blocks:
                    block.pop("_cache_layer", None)

                request_params["system"] = system_blocks
                logger.debug(f"ğŸ—‚ï¸ [Stream] ä½¿ç”¨å¤šå±‚ç¼“å­˜ system prompt: {len(system_blocks)} å±‚")
            elif self.config.enable_caching:
                # å­—ç¬¦ä¸²æ ¼å¼ + å¯ç”¨ç¼“å­˜ï¼šè‡ªåŠ¨åŒ…è£…ä¸ºå•å±‚ç¼“å­˜ï¼ˆ1 å°æ—¶ TTLï¼‰
                request_params["system"] = [
                    {
                        "type": "text",
                        "text": system,
                        "cache_control": {"type": "ephemeral", "ttl": "1h"},
                    }
                ]
            else:
                # å­—ç¬¦ä¸²æ ¼å¼ + ç¦ç”¨ç¼“å­˜ï¼šç›´æ¥ä½¿ç”¨
                request_params["system"] = system

        # Extended Thinkingï¼ˆæ”¯æŒåŠ¨æ€è¦†ç›–ï¼‰
        # override_thinking ä¼˜å…ˆçº§é«˜äºé…ç½®ï¼šNone=ä½¿ç”¨é…ç½®, True/False=å¼ºåˆ¶å¼€å¯/å…³é—­
        effective_thinking = (
            override_thinking if override_thinking is not None else self.config.enable_thinking
        )
        if effective_thinking:
            request_params["thinking"] = {
                "type": "enabled",
                "budget_tokens": self.config.thinking_budget,
            }
            request_params["temperature"] = 1.0

        # Tools
        all_tools = []
        tool_names_seen = set()

        # æ·»åŠ ç”¨æˆ·æŒ‡å®šçš„å·¥å…·
        if tools:
            for tool in self._format_tools(tools):
                tool_name = tool.get("name", "")
                if tool_name and tool_name not in tool_names_seen:
                    all_tools.append(tool)
                    tool_names_seen.add(tool_name)

        # æ·»åŠ è‡ªå®šä¹‰å·¥å…·ï¼ˆé¿å…é‡å¤ï¼‰
        for custom_tool in self._custom_tools:
            tool_name = custom_tool.get("name", "")
            if tool_name and tool_name not in tool_names_seen:
                tool_def = custom_tool.copy()
                all_tools.append(tool_def)
                tool_names_seen.add(tool_name)

        if all_tools:
            # ğŸ”§ ç¼“å­˜ç­–ç•¥ï¼šåªå¯¹æœ€åä¸€ä¸ªå·¥å…·æ·»åŠ  cache_controlï¼ˆ1 å°æ—¶ TTLï¼‰
            # Claude API é™åˆ¶æœ€å¤š 4 ä¸ªå¸¦ cache_control çš„ block
            if self.config.enable_caching:
                all_tools[-1] = all_tools[-1].copy()
                all_tools[-1]["cache_control"] = {"type": "ephemeral", "ttl": "1h"}

            request_params["tools"] = all_tools

        # è¯·æ±‚æ—¥å¿—ï¼ˆINFO çº§åˆ«ï¼‰
        logger.info(
            f"ğŸ“¤ Claude è¯·æ±‚: model={self.config.model}, tools={len(all_tools)}, messages={len(formatted_messages)}"
        )

        # Request detail logging (DEBUG level to avoid stdout buffer overflow)
        if logger.isEnabledFor(10):  # DEBUG
            logger.debug("=" * 80)
            logger.debug("ğŸ” å®Œæ•´ request_params:")
            logger.debug(f"   model: {request_params.get('model')}")
            logger.debug(f"   max_tokens: {request_params.get('max_tokens')}")
            if "thinking" in request_params:
                logger.debug(f"   thinking: {request_params.get('thinking')}")
            if "system" in request_params:
                system_val = request_params.get("system")
                if isinstance(system_val, list):
                    logger.debug(f"   system: (list, {len(system_val)} blocks)")
                    for idx, block in enumerate(system_val):
                        if isinstance(block, dict):
                            text_preview = str(block.get("text", ""))[:200]
                            logger.debug(
                                f"      [{idx}] type={block.get('type')}, text={text_preview}..."
                            )
                else:
                    logger.debug(f"   system: {str(system_val)[:200]}...")
            logger.debug(f"   messages ({len(request_params.get('messages', []))}):")
            for i, msg in enumerate(request_params.get("messages", [])):
                logger.debug(f"   â”€â”€ Message [{i}] â”€â”€")
                logger.debug(f"      role: {msg.get('role')}")
                content = msg.get("content")
                if isinstance(content, list):
                    logger.debug(f"      content: (list, {len(content)} blocks)")
                    for j, block in enumerate(content):
                        if isinstance(block, dict):
                            block_type = block.get("type", "unknown")
                            if block_type == "text":
                                text_preview = str(block.get("text", ""))[:300]
                                logger.debug(f"         [{j}] type=text, text={text_preview}...")
                            elif block_type == "tool_use":
                                logger.debug(
                                    f"         [{j}] type=tool_use, id={block.get('id')}, name={block.get('name')}"
                                )
                            elif block_type == "tool_result":
                                logger.debug(
                                    f"         [{j}] type=tool_result, tool_use_id={block.get('tool_use_id')}"
                                )
                            else:
                                logger.debug(
                                    f"         [{j}] type={block_type}"
                                )
                elif isinstance(content, str):
                    logger.debug(f"      content: {content[:300]}...")
            if "tools" in request_params:
                logger.debug(f"   tools ({len(request_params['tools'])}):")
                for tool in request_params["tools"]:
                    logger.debug(f"      - {tool.get('name', 'unknown')}")
            logger.debug("=" * 80)

        # è¯¦ç»†æ—¥å¿—ï¼šå®Œæ•´è¯·æ±‚å‚æ•°
        if LLM_DEBUG_VERBOSE:
            logger.info("=" * 60)
            logger.info("ğŸ“¤ [VERBOSE] å®Œæ•´è¯·æ±‚å‚æ•°:")
            # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            verbose_params = request_params.copy()
            # æ‰“å° system promptï¼ˆå¯èƒ½å¾ˆé•¿ï¼Œæˆªæ–­ï¼‰
            if "system" in verbose_params:
                system_preview = str(verbose_params["system"])[:500]
                logger.info(
                    f"   system: {system_preview}{'...' if len(str(verbose_params['system'])) > 500 else ''}"
                )
            # æ‰“å°å®Œæ•´ messagesï¼ˆå®‰å…¨åºåˆ—åŒ–ï¼‰
            logger.info(f"   messages ({len(verbose_params.get('messages', []))}):")
            for i, msg in enumerate(verbose_params.get("messages", [])):
                msg_preview = self._safe_json_dumps(msg, indent=2)
                logger.info(f"   [{i}] {msg_preview}")
            # æ‰“å° tools
            if "tools" in verbose_params:
                logger.info(f"   tools ({len(verbose_params['tools'])}):")
                for tool in verbose_params["tools"]:
                    logger.info(f"      - {tool.get('name', 'unknown')}")
            logger.info("=" * 60)
        else:
            logger.debug(f"ğŸ“¤ Messages æ•°é‡: {len(request_params.get('messages', []))}")
        for i, msg in enumerate(request_params.get("messages", [])):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            if isinstance(content, list):
                types = [b.get("type", "unknown") for b in content if isinstance(b, dict)]
                logger.debug(f"   [{i}] {role}: blocks={types}")
            else:
                preview = str(content)[:100] + "..." if len(str(content)) > 100 else str(content)
                logger.debug(f"   [{i}] {role}: {preview}")

        # ç´¯ç§¯å˜é‡
        accumulated_thinking = ""
        accumulated_content = ""
        tool_calls = []
        stop_reason = None
        usage = {}  # ğŸ”¢ æµå¼æ¨¡å¼ä¸‹ä» final_message è·å–
        final_message = None  # ğŸš¨ å¿…é¡»åœ¨ try å¤–åˆå§‹åŒ–ï¼Œé˜²æ­¢ä¸­æ–­æ—¶ UnboundLocalError

        # ğŸ”„ æµå¼é‡è¯•é…ç½®ï¼šç½‘ç»œä¸­æ–­æ—¶è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š 2 æ¬¡ï¼‰
        _STREAM_MAX_RETRIES = 2
        _stream_attempt = 0

        event_count = 0  # ğŸš¨ åœ¨ try å¤–åˆå§‹åŒ–ï¼Œç¡®ä¿ except ä¸­å¯ç”¨
        try:
            stream_ctx = self.async_client.messages.stream(**request_params)

            async with stream_ctx as stream:
                async for event in stream:
                    event_count += 1
                    if not hasattr(event, "type"):
                        continue

                    if event.type == "content_block_start":
                        if hasattr(event, "content_block"):
                            block = event.content_block
                            if hasattr(block, "type"):
                                block_type = block.type

                                if block_type == "thinking" and on_thinking:
                                    on_thinking("")
                                elif block_type == "text" and on_content:
                                    on_content("")
                                # ğŸ†• å®¢æˆ·ç«¯å·¥å…·è°ƒç”¨ - ç«‹å³ yield tool_use_start
                                elif block_type == "tool_use":
                                    tool_id = getattr(block, "id", "")
                                    tool_name = getattr(block, "name", "")
                                    if on_tool_call:
                                        on_tool_call(
                                            {
                                                "id": tool_id,
                                                "name": tool_name,
                                                "input": {},  # input åç»­æµå¼å‘é€
                                                "type": "tool_use",
                                            }
                                        )
                                    # ğŸ†• yield tool_use_start äº‹ä»¶
                                    yield LLMResponse(
                                        content="",
                                        model=self.config.model,  # ğŸ†•
                                        is_stream=True,
                                        tool_use_start={
                                            "id": tool_id,
                                            "name": tool_name,
                                            "type": "tool_use",
                                        },
                                    )

                    elif event.type == "content_block_delta":
                        if hasattr(event, "delta"):
                            delta = event.delta
                            if hasattr(delta, "type"):
                                if delta.type == "thinking_delta":
                                    text = getattr(delta, "thinking", "")
                                    accumulated_thinking += text
                                    if on_thinking:
                                        on_thinking(text)
                                    yield LLMResponse(
                                        content="",
                                        thinking=text,
                                        model=self.config.model,
                                        is_stream=True,
                                    )

                                elif delta.type == "text_delta":
                                    text = getattr(delta, "text", "")
                                    accumulated_content += text
                                    if on_content:
                                        on_content(text)
                                    yield LLMResponse(
                                        content=text, model=self.config.model, is_stream=True
                                    )

                                elif delta.type == "input_json_delta":
                                    partial_json = getattr(delta, "partial_json", "")
                                    if on_tool_call:
                                        on_tool_call(
                                            {"partial_input": partial_json, "type": "input_delta"}
                                        )
                                    # ğŸ†• yield input_delta äº‹ä»¶
                                    yield LLMResponse(
                                        content="",
                                        model=self.config.model,  # ğŸ†•
                                        is_stream=True,
                                        input_delta=partial_json,
                                    )

                    elif event.type == "message_stop":
                        final_message = None
                        try:
                            final_message = await stream.get_final_message()
                            stop_reason = getattr(final_message, "stop_reason", None)

                            # ğŸ”¢ æå– usage ä¿¡æ¯
                            if hasattr(final_message, "usage") and final_message.usage:
                                usage = {
                                    "input_tokens": final_message.usage.input_tokens,
                                    "output_tokens": final_message.usage.output_tokens,
                                    "thinking_tokens": 0,  # ğŸ†• Extended Thinking tokens
                                }
                                if hasattr(final_message.usage, "cache_read_input_tokens"):
                                    usage["cache_read_tokens"] = (
                                        final_message.usage.cache_read_input_tokens
                                    )
                                if hasattr(final_message.usage, "cache_creation_input_tokens"):
                                    usage["cache_creation_tokens"] = (
                                        final_message.usage.cache_creation_input_tokens
                                    )

                                # ğŸ†• è®¡ç®— Extended Thinking tokens
                                if accumulated_thinking:
                                    usage["thinking_tokens"] = self.count_tokens(
                                        accumulated_thinking
                                    )

                                # ğŸ“Š Token ä½¿ç”¨é‡æ—¥å¿—
                                input_tokens = usage.get("input_tokens", 0)
                                output_tokens = usage.get("output_tokens", 0)
                                thinking_tokens = usage.get("thinking_tokens", 0)
                                total_tokens = input_tokens + output_tokens + thinking_tokens
                                logger.info(
                                    f"ğŸ“Š Token ä½¿ç”¨: input={input_tokens:,}, output={output_tokens:,}, "
                                    f"thinking={thinking_tokens:,}, total={total_tokens:,} (model={self.config.model})"
                                )

                                # ğŸ†• Cache æ•ˆæœæ—¥å¿—ï¼ˆContext Engineering ç›‘æ§ï¼‰
                                cache_read = usage.get("cache_read_tokens", 0)
                                cache_create = usage.get("cache_creation_tokens", 0)
                                if cache_read > 0:
                                    # cache å‘½ä¸­ï¼ŒèŠ‚çœæˆæœ¬ï¼ˆ90% æŠ˜æ‰£ï¼‰
                                    saved = cache_read * 0.003 * 0.9 / 1000  # $3/M * 90% off
                                    logger.info(
                                        f"âœ… Cache HIT: {cache_read:,} tokens (saved ~${saved:.4f})"
                                    )
                                elif cache_create > 0:
                                    logger.debug(f"ğŸ“¦ Cache CREATED: {cache_create:,} tokens")

                            if hasattr(final_message, "content"):
                                for block in final_message.content:
                                    if not hasattr(block, "type"):
                                        continue
                                    block_type = block.type

                                    # å·¥å…·è°ƒç”¨
                                    if block_type == "tool_use":
                                        tool_calls.append(
                                            {
                                                "id": getattr(block, "id", ""),
                                                "name": getattr(block, "name", ""),
                                                "input": getattr(block, "input", {}),
                                                "type": "tool_use",
                                            }
                                        )
                        except Exception as e:
                            logger.warning(f"è·å–æœ€ç»ˆæ¶ˆæ¯å¤±è´¥: {e}")
        except (
            httpx.RemoteProtocolError,  # peer closed / incomplete chunked read
            httpx.ConnectError,         # connection refused / reset
            httpx.TimeoutException,     # read timeout mid-stream
            anthropic.APIConnectionError,  # SDK wrapper for network errors
            anthropic.APITimeoutError,     # SDK timeout wrapper
        ) as stream_error:
            # ğŸ”„ å¯é‡è¯•çš„ç½‘ç»œé”™è¯¯ï¼šæµå¼ä¼ è¾“ä¸­æ–­
            _stream_attempt += 1
            error_msg = str(stream_error)
            logger.warning(
                f"âš ï¸ æµå¼ä¼ è¾“ä¸­æ–­ (attempt {_stream_attempt}/{_STREAM_MAX_RETRIES}): {error_msg}"
            )
            logger.warning(f"   å·²æ¥æ”¶äº‹ä»¶æ•°: {event_count}")
            logger.warning(f"   å·²ç´¯ç§¯ content: {len(accumulated_content)} chars")
            logger.warning(f"   å·²è§£æ tool_calls: {len(tool_calls)}")

            # å¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œä¸”æ²¡æœ‰å®Œæ•´è§£æå‡º tool_call â†’ é‡è¯•
            if _stream_attempt <= _STREAM_MAX_RETRIES and not tool_calls:
                import asyncio as _asyncio

                delay = 1.0 * _stream_attempt
                logger.info(f"ğŸ”„ {delay}s åé‡è¯•æµå¼è°ƒç”¨...")
                await _asyncio.sleep(delay)

                # ä¿å­˜ç´¯ç§¯çŠ¶æ€ï¼ˆfallback å¤±è´¥æ—¶ç”¨äºé™çº§è¿”å›ï¼‰
                _saved_content = accumulated_content
                _saved_thinking = accumulated_thinking

                # é‡ç½®ç´¯ç§¯å˜é‡
                accumulated_thinking = ""
                accumulated_content = ""
                tool_calls = []
                stop_reason = None
                usage = {}
                event_count = 0

                # ä½¿ç”¨éæµå¼ fallbackï¼šç”¨ create_message_async æ›¿ä»£
                logger.info("ğŸ”„ å›é€€åˆ°éæµå¼è°ƒç”¨ä»¥ç¡®ä¿å®Œæ•´æ€§...")
                try:
                    fallback_response = await self.create_message_async(
                        messages=messages,
                        system=system,
                        tools=tools,
                        override_thinking=override_thinking,
                        **kwargs,
                    )
                    # å°†éæµå¼ç»“æœè½¬ä¸ºå•æ¬¡ yield
                    yield fallback_response
                    return
                except Exception as fallback_err:
                    logger.error(f"âŒ éæµå¼ fallback ä¹Ÿå¤±è´¥: {fallback_err}")
                    # Restore accumulated state for partial response below
                    accumulated_content = _saved_content
                    accumulated_thinking = _saved_thinking

            # è¶…è¿‡é‡è¯•æ¬¡æ•°æˆ–å·²æœ‰ tool_calls â†’ é™çº§è¿”å›éƒ¨åˆ†å“åº”
            if accumulated_content or accumulated_thinking or tool_calls:
                logger.warning("âš ï¸ è¿”å›éƒ¨åˆ†å“åº”ï¼ˆé‡è¯•å·²è€—å°½ï¼‰...")
                raw_content = self._build_raw_content_from_parts(
                    accumulated_thinking, accumulated_content, tool_calls
                )
                yield LLMResponse(
                    content=accumulated_content,
                    thinking=accumulated_thinking if accumulated_thinking else None,
                    tool_calls=tool_calls if tool_calls else None,
                    stop_reason="stream_error",
                    model=self.config.model,
                    raw_content=raw_content,
                    is_stream=False,
                )
                return

            # æ²¡æœ‰ä»»ä½•å†…å®¹ï¼ŒæŠ›å‡ºåŸå§‹é”™è¯¯
            raise
        except Exception as stream_error:
            # ğŸš¨ éç½‘ç»œé”™è¯¯ï¼ˆå¦‚ API é”™è¯¯ï¼‰ï¼šä¸é‡è¯•ï¼Œç›´æ¥é™çº§
            error_msg = str(stream_error)
            logger.error(f"âŒ æµå¼ä¼ è¾“å¼‚å¸¸: {error_msg}")
            logger.error(f"   å·²æ¥æ”¶äº‹ä»¶æ•°: {event_count}")
            logger.error(f"   å·²ç´¯ç§¯ content: {len(accumulated_content)} chars")
            logger.error(f"   å·²è§£æ tool_calls: {len(tool_calls)}")

            if accumulated_content or accumulated_thinking or tool_calls:
                logger.warning("âš ï¸ è¿”å›éƒ¨åˆ†å“åº”...")
                raw_content = self._build_raw_content_from_parts(
                    accumulated_thinking, accumulated_content, tool_calls
                )
                yield LLMResponse(
                    content=accumulated_content,
                    thinking=accumulated_thinking if accumulated_thinking else None,
                    tool_calls=tool_calls if tool_calls else None,
                    stop_reason="stream_error",
                    model=self.config.model,
                    raw_content=raw_content,
                    is_stream=False,
                )
                return

            raise

        # ğŸš¨ Guard: stream ended without message_stop (silent disconnect)
        #
        # Per Anthropic docs: "When receiving a streaming response via SSE,
        # it's possible that an error can occur after returning a 200 response,
        # in which case error handling wouldn't follow standard mechanisms."
        #
        # The SDK may NOT raise RemoteProtocolError if the server closed
        # gracefully after HTTP 200 + partial SSE. Detect this by checking
        # for missing message_stop (final_message is None).
        if final_message is None and stop_reason is None:
            logger.warning(
                f"âš ï¸ æµå¼ç»“æŸä½†æ—  message_stop (events={event_count}, "
                f"content={len(accumulated_content)} chars) â€” è§†ä¸ºé™é»˜æ–­è¿"
            )
            if _stream_attempt < _STREAM_MAX_RETRIES and not tool_calls:
                _stream_attempt += 1
                import asyncio as _asyncio

                delay = 1.0 * _stream_attempt
                logger.info(f"ğŸ”„ {delay}s åé‡è¯•ï¼ˆé™é»˜æ–­è¿æ¢å¤ï¼‰...")
                await _asyncio.sleep(delay)

                _saved_content = accumulated_content
                _saved_thinking = accumulated_thinking

                logger.info("ğŸ”„ å›é€€åˆ°éæµå¼è°ƒç”¨ä»¥ç¡®ä¿å®Œæ•´æ€§...")
                try:
                    fallback_response = await self.create_message_async(
                        messages=messages,
                        system=system,
                        tools=tools,
                        override_thinking=override_thinking,
                        **kwargs,
                    )
                    yield fallback_response
                    return
                except Exception as fallback_err:
                    logger.error(f"âŒ éæµå¼ fallback ä¹Ÿå¤±è´¥: {fallback_err}")
                    accumulated_content = _saved_content
                    accumulated_thinking = _saved_thinking

            # Fallback failed or retries exhausted â†’ return partial
            if accumulated_content or accumulated_thinking:
                raw_content = self._build_raw_content_from_parts(
                    accumulated_thinking, accumulated_content, tool_calls
                )
                yield LLMResponse(
                    content=accumulated_content,
                    thinking=accumulated_thinking if accumulated_thinking else None,
                    tool_calls=tool_calls if tool_calls else None,
                    stop_reason="stream_error",
                    model=self.config.model,
                    raw_content=raw_content,
                    is_stream=False,
                )
                return

        # æ„å»º raw_content
        # ä¼˜å…ˆä½¿ç”¨ final_messageï¼ˆåŒ…å« thinking signatureï¼‰
        if final_message and hasattr(final_message, "content"):
            raw_content = self._build_raw_content(final_message)
        else:
            # é™çº§ï¼šä½¿ç”¨ç´¯ç§¯çš„å†…å®¹ï¼ˆæ²¡æœ‰ signatureï¼‰
            raw_content = self._build_raw_content_from_parts(
                accumulated_thinking, accumulated_content, tool_calls
            )

        # å“åº”æ—¥å¿—ï¼ˆINFO çº§åˆ«ï¼‰
        raw_types = [b.get("type", "unknown") for b in raw_content]
        tool_names = [tc.get("name", "") for tc in tool_calls] if tool_calls else []
        logger.info(
            f"ğŸ“¥ Claude å“åº”: stop_reason={stop_reason or 'end_turn'}, blocks={raw_types}, tools={tool_names}"
        )

        # è¯¦ç»†æ—¥å¿—ï¼šå®Œæ•´å“åº”å†…å®¹
        if LLM_DEBUG_VERBOSE:
            logger.info("=" * 60)
            logger.info("ğŸ“¥ [VERBOSE] å®Œæ•´å“åº”å†…å®¹:")
            logger.info(f"   stop_reason: {stop_reason}")
            if accumulated_thinking:
                thinking_preview = accumulated_thinking[:1000]
                logger.info(
                    f"   thinking ({len(accumulated_thinking)} chars): {thinking_preview}{'...' if len(accumulated_thinking) > 1000 else ''}"
                )
            if accumulated_content:
                content_preview = accumulated_content[:2000]
                logger.info(
                    f"   content ({len(accumulated_content)} chars): {content_preview}{'...' if len(accumulated_content) > 2000 else ''}"
                )
            if tool_calls:
                logger.info(f"   tool_calls ({len(tool_calls)}):")
                for tc in tool_calls:
                    logger.info(f"      {self._safe_json_dumps(tc, indent=2)}")
            logger.info(f"   raw_content ({len(raw_content)} blocks):")
            for i, block in enumerate(raw_content):
                block_preview = self._safe_json_dumps(block)
                if len(block_preview) > 500:
                    block_preview = block_preview[:500] + "..."
                logger.info(f"      [{i}] {block_preview}")
            logger.info("=" * 60)

        # è¿”å›æœ€ç»ˆå“åº”
        if accumulated_content or accumulated_thinking or tool_calls:
            yield LLMResponse(
                content=accumulated_content,
                thinking=accumulated_thinking if accumulated_thinking else None,
                tool_calls=tool_calls if tool_calls else None,
                stop_reason=stop_reason or "end_turn",
                usage=usage if usage else None,  # ğŸ”¢ æµå¼æ¨¡å¼ä¹Ÿè¿”å› usage
                model=self.config.model,  # ğŸ†• å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°
                raw_content=raw_content,
                is_stream=False,
            )

    def _safe_json_dumps(self, obj: Any, indent: int = None) -> str:
        """
        å®‰å…¨çš„ JSON åºåˆ—åŒ–ï¼Œå¤„ç†ç‰¹æ®Šå¯¹è±¡ï¼ˆå¦‚ WebSearchResultBlockï¼‰

        Args:
            obj: è¦åºåˆ—åŒ–çš„å¯¹è±¡
            indent: ç¼©è¿›çº§åˆ«

        Returns:
            JSON å­—ç¬¦ä¸²
        """

        def default_handler(o) -> Any:
            """è‡ªå®šä¹‰åºåˆ—åŒ–å¤„ç†å™¨"""
            if hasattr(o, "model_dump"):
                # Pydantic v2 å¯¹è±¡
                return o.model_dump()
            elif hasattr(o, "dict"):
                # Pydantic v1 å¯¹è±¡
                return o.dict()
            elif hasattr(o, "__dict__"):
                # æ™®é€šå¯¹è±¡
                return o.__dict__
            else:
                # Fallback
                return str(o)

        try:
            return json.dumps(obj, ensure_ascii=False, indent=indent, default=default_handler)
        except Exception:
            return str(obj)

    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®— token æ•°é‡

        TODO: ä½¿ç”¨ Claude å®˜æ–¹ API ç²¾ç¡®è®¡ç®—
        - client.messages.count_tokens() æ”¯æŒæ¶ˆæ¯ã€å·¥å…·ã€å›¾ç‰‡ç­‰
        - å‚è€ƒ: https://docs.anthropic.com/en/api/messages-count-tokens

        å½“å‰ä½¿ç”¨çˆ¶ç±»çš„ tiktoken å®ç°ã€‚

        Args:
            text: è¦è®¡ç®—çš„æ–‡æœ¬

        Returns:
            token æ•°é‡
        """
        # TODO: å®ç° Claude å®˜æ–¹ token è®¡ç®—
        # response = self.sync_client.messages.count_tokens(
        #     model=self.model,
        #     messages=[{"role": "user", "content": text}]
        # )
        # return response.input_tokens
        return super().count_tokens(text)

    # ============================================================
    # å“åº”è§£æ
    # ============================================================

    def _parse_response(
        self, response: anthropic.types.Message, invocation_type: Optional[str] = None
    ) -> LLMResponse:
        """è§£æ Claude API å“åº”ä¸ºç»Ÿä¸€æ ¼å¼"""
        thinking_text = ""
        content_text = ""
        tool_calls = []
        invocation_method = invocation_type or "direct"

        for block in response.content:
            if not hasattr(block, "type"):
                continue

            if block.type == "thinking":
                thinking_text = getattr(block, "thinking", "")
            elif block.type == "text":
                content_text = getattr(block, "text", "")
            elif block.type == "tool_use":
                tool_name = getattr(block, "name", "")

                tool_calls.append(
                    {
                        "id": getattr(block, "id", ""),
                        "name": tool_name,
                        "input": getattr(block, "input", {}),
                        "invocation_method": "direct",
                    }
                )

        # Usage ä¿¡æ¯
        usage = {}
        if hasattr(response, "usage"):
            usage = {
                "input_tokens": response.usage.input_tokens,
                "output_tokens": response.usage.output_tokens,
                "thinking_tokens": 0,  # ğŸ†• Extended Thinking tokens
            }
            if hasattr(response.usage, "cache_read_input_tokens"):
                usage["cache_read_tokens"] = response.usage.cache_read_input_tokens
            if hasattr(response.usage, "cache_creation_input_tokens"):
                usage["cache_creation_tokens"] = response.usage.cache_creation_input_tokens

            # ğŸ†• è®¡ç®— Extended Thinking tokens
            if thinking_text:
                # Anthropic ç›®å‰æœªåœ¨ usage ä¸­å•ç‹¬è¿”å› thinking_tokens
                # ä½¿ç”¨ tiktoken è¿›è¡Œè¿‘ä¼¼è®¡ç®—
                usage["thinking_tokens"] = self.count_tokens(thinking_text)

            # ğŸ“Š Token ä½¿ç”¨é‡æ—¥å¿—
            input_tokens = usage.get("input_tokens", 0)
            output_tokens = usage.get("output_tokens", 0)
            thinking_tokens = usage.get("thinking_tokens", 0)
            total_tokens = input_tokens + output_tokens + thinking_tokens
            logger.info(
                f"ğŸ“Š Token ä½¿ç”¨: input={input_tokens:,}, output={output_tokens:,}, "
                f"thinking={thinking_tokens:,}, total={total_tokens:,} (model={self.config.model})"
            )

            # ğŸ†• Cache æ•ˆæœæ—¥å¿—ï¼ˆContext Engineering ç›‘æ§ï¼‰
            cache_read = usage.get("cache_read_tokens") or 0
            cache_create = usage.get("cache_creation_tokens") or 0
            if cache_read > 0:
                saved = cache_read * 0.003 * 0.9 / 1000
                logger.info(f"âœ… Cache HIT: {cache_read:,} tokens (saved ~${saved:.4f})")
            elif cache_create > 0:
                logger.debug(f"ğŸ“¦ Cache CREATED: {cache_create:,} tokens")

        # æ„å»º raw_content
        raw_content = self._build_raw_content(response)

        return LLMResponse(
            content=content_text,
            thinking=thinking_text if thinking_text else None,
            tool_calls=tool_calls if tool_calls else None,
            stop_reason=response.stop_reason,
            usage=usage,
            model=self.config.model,  # ğŸ†• å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°
            raw_content=raw_content,
            cache_read_tokens=usage.get("cache_read_tokens", 0),
            cache_creation_tokens=usage.get("cache_creation_tokens", 0),
        )

    def _build_raw_content(self, response: anthropic.types.Message) -> List[Dict[str, Any]]:
        """
        æ„å»ºåŸå§‹ content å—åˆ—è¡¨ï¼ˆç”¨äºæ¶ˆæ¯ç»­ä¼ ï¼‰

        Claude åŸç”Ÿåè®®æ”¯æŒçš„ content block ç±»å‹ï¼š
        - thinking: æ€è€ƒè¿‡ç¨‹ï¼ˆå¸¦ signatureï¼‰
        - text: æ–‡æœ¬å†…å®¹
        - tool_use: å·¥å…·è°ƒç”¨

        è§„åˆ™ï¼š
        1. thinking å—å¿…é¡»æœ‰æœ‰æ•ˆçš„ signature å­—æ®µ
        2. tool_use å—å¿…é¡»æœ‰ id å’Œ name
        3. è·³è¿‡ç©ºæ–‡æœ¬å—
        """
        raw_content = []

        for block in response.content:
            if not hasattr(block, "type"):
                continue

            block_type = block.type

            if block_type == "thinking":
                thinking_text = getattr(block, "thinking", "")
                signature = getattr(block, "signature", "")

                if thinking_text and signature:
                    raw_content.append(
                        {"type": "thinking", "thinking": thinking_text, "signature": signature}
                    )
                elif thinking_text:
                    logger.warning(f"Thinking block without signature, skipping")

            elif block_type == "text":
                text_content = getattr(block, "text", "")
                if text_content:
                    raw_content.append({"type": "text", "text": text_content})

            # å®¢æˆ·ç«¯å·¥å…·è°ƒç”¨
            elif block_type == "tool_use":
                tool_id = getattr(block, "id", "")
                tool_name = getattr(block, "name", "")
                tool_input = getattr(block, "input", {})

                if tool_id and tool_name:
                    raw_content.append(
                        {"type": "tool_use", "id": tool_id, "name": tool_name, "input": tool_input}
                    )
                else:
                    logger.warning(f"Invalid tool_use block: id={tool_id}, name={tool_name}")

            else:
                # æœªçŸ¥ç±»å‹ï¼Œè®°å½•è­¦å‘Šä½†ä¸è·³è¿‡ï¼ˆå¯èƒ½æ˜¯æ–°çš„ block ç±»å‹ï¼‰
                logger.warning(f"Unknown content block type: {block_type}")
                # å°è¯•è½¬æ¢ä¸ºå­—å…¸
                try:
                    block_dict = {"type": block_type}
                    for attr in ["id", "name", "input", "content", "tool_use_id"]:
                        if hasattr(block, attr):
                            block_dict[attr] = getattr(block, attr)
                    raw_content.append(block_dict)
                except Exception as e:
                    logger.error(f"Failed to convert unknown block: {e}")

        return raw_content

    def _build_raw_content_from_parts(
        self, thinking: str, content: str, tool_calls: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ä»æµå¼ç´¯ç§¯çš„éƒ¨åˆ†æ„å»º raw_contentï¼ˆé™çº§æ–¹æ¡ˆï¼‰

        æ³¨æ„ï¼šè¿™æ˜¯é™çº§æ–¹æ¡ˆï¼Œä»…åœ¨æ— æ³•è·å– final_message æ—¶ä½¿ç”¨ã€‚
        ä¸åŒ…å« thinking å—ï¼ˆå› ä¸ºæ²¡æœ‰ signatureï¼‰ï¼Œä¼šå¯¼è‡´åç»­è½®æ¬¡
        Extended Thinking å¤±è´¥ã€‚

        ä¼˜å…ˆä½¿ç”¨ _build_raw_content(final_message) æ¥è·å–å®Œæ•´çš„
        thinking å—ï¼ˆåŒ…æ‹¬ signatureï¼‰ã€‚
        """
        raw_content = []

        # ä¸åŒ…å« thinking å—ï¼ˆæ²¡æœ‰ signature ä¼šå¯¼è‡´åç»­ Extended Thinking å¤±è´¥ï¼‰

        if content:
            raw_content.append({"type": "text", "text": content})

        for tc in tool_calls:
            raw_content.append(
                {"type": "tool_use", "id": tc["id"], "name": tc["name"], "input": tc["input"]}
            )

        return raw_content

    def _remove_thinking_blocks(self, messages: List[Dict]) -> List[Dict]:
        """
        ä»æ¶ˆæ¯ä¸­ç§»é™¤ thinking blocks

        å½“ç¦ç”¨ Extended Thinking æ—¶ï¼Œå¿…é¡»ä»å†å²æ¶ˆæ¯ä¸­ç§»é™¤ thinking blocksï¼Œ
        å¦åˆ™ Claude API ä¼šæŠ¥é”™ï¼š
        "When thinking is disabled, assistant message cannot contain thinking blocks"

        å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š
        "You may omit thinking blocks from previous assistant turns"

        Args:
            messages: æ ¼å¼åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨

        Returns:
            ç§»é™¤ thinking blocks åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        cleaned_messages = []

        for msg in messages:
            if not isinstance(msg, dict):
                cleaned_messages.append(msg)
                continue

            role = msg.get("role")
            content = msg.get("content")

            # åªå¤„ç† assistant æ¶ˆæ¯
            if role == "assistant" and isinstance(content, list):
                # è¿‡æ»¤æ‰ thinking å’Œ redacted_thinking blocks
                filtered_content = [
                    block
                    for block in content
                    if isinstance(block, dict)
                    and block.get("type") not in ("thinking", "redacted_thinking")
                ]

                if filtered_content:
                    cleaned_messages.append({"role": "assistant", "content": filtered_content})
                # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œè·³è¿‡è¯¥æ¶ˆæ¯
            else:
                cleaned_messages.append(msg)

        return cleaned_messages

    # ============================================================
    # Files API
    # ============================================================

    async def download_file(
        self, file_id: str, output_path: str, overwrite: bool = True
    ) -> Optional[FileInfo]:
        """
        ä¸‹è½½æ–‡ä»¶ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰

        Args:
            file_id: æ–‡ä»¶ ID
            output_path: è¾“å‡ºè·¯å¾„
            overwrite: æ˜¯å¦è¦†ç›–å·²æœ‰æ–‡ä»¶

        Returns:
            FileInfo æˆ– None
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(output_path) and not overwrite:
                logger.warning(f"æ–‡ä»¶å·²å­˜åœ¨: {output_path}")
                return None

            # ç¡®ä¿è¾“å‡ºè·¯å¾„ä¸ºç»å¯¹è·¯å¾„ï¼ˆæ‰“åŒ…å cwd å¯èƒ½åªè¯»ï¼‰
            if not os.path.isabs(output_path):
                from utils.app_paths import get_user_data_dir
                output_path = str(get_user_data_dir() / output_path)

            # åˆ›å»ºç›®å½•
            output_dir = os.path.dirname(output_path)
            if output_dir:
                Path(output_dir).mkdir(parents=True, exist_ok=True)

            # è·å–å…ƒæ•°æ®
            metadata = self.sync_client.beta.files.retrieve_metadata(file_id=file_id)

            # ä¸‹è½½æ–‡ä»¶
            file_content = self.sync_client.beta.files.download(file_id=file_id)

            # å¼‚æ­¥å†™å…¥æ–‡ä»¶
            async with aiofiles.open(output_path, "wb") as f:
                await f.write(file_content.read())

            logger.info(f"âœ… æ–‡ä»¶å·²ä¸‹è½½: {output_path} ({metadata.size_bytes} bytes)")

            return FileInfo(
                file_id=metadata.id,
                filename=metadata.filename,
                size_bytes=metadata.size_bytes,
                mime_type=metadata.mime_type,
                created_at=metadata.created_at,
                downloadable=metadata.downloadable,
            )

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def get_file_info(self, file_id: str) -> Optional[FileInfo]:
        """
        è·å–æ–‡ä»¶å…ƒæ•°æ®

        Args:
            file_id: æ–‡ä»¶ ID

        Returns:
            FileInfo æˆ– None
        """
        try:
            metadata = self.sync_client.beta.files.retrieve_metadata(file_id=file_id)
            return FileInfo(
                file_id=metadata.id,
                filename=metadata.filename,
                size_bytes=metadata.size_bytes,
                mime_type=metadata.mime_type,
                created_at=metadata.created_at,
                downloadable=metadata.downloadable,
            )
        except Exception as e:
            logger.error(f"âŒ è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
            return None

    def list_files(self) -> List[FileInfo]:
        """
        åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶

        Returns:
            FileInfo åˆ—è¡¨
        """
        try:
            files = self.sync_client.beta.files.list()
            return [
                FileInfo(
                    file_id=f.id,
                    filename=f.filename,
                    size_bytes=f.size_bytes,
                    mime_type=f.mime_type,
                    created_at=f.created_at,
                    downloadable=f.downloadable,
                )
                for f in files.data
            ]
        except Exception as e:
            logger.error(f"âŒ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def extract_file_ids_from_response(self, response) -> List[str]:
        """
        ä»å“åº”ä¸­æå– file_id

        Args:
            response: Claude API å“åº”

        Returns:
            file_id åˆ—è¡¨
        """
        file_ids = []

        def find_file_ids(obj, depth=0) -> None:
            """é€’å½’æŸ¥æ‰¾ file_id"""
            if depth > 10:
                return

            if hasattr(obj, "file_id") and obj.file_id:
                file_ids.append(obj.file_id)

            if hasattr(obj, "content"):
                content = obj.content
                if isinstance(content, (list, tuple)):
                    for item in content:
                        find_file_ids(item, depth + 1)
                elif hasattr(content, "__dict__"):
                    find_file_ids(content, depth + 1)

            if hasattr(obj, "__dict__"):
                for key, value in obj.__dict__.items():
                    if key == "file_id" and value:
                        if value not in file_ids:
                            file_ids.append(value)
                    elif hasattr(value, "__dict__") or isinstance(value, (list, tuple)):
                        find_file_ids(value, depth + 1)

        if hasattr(response, "content"):
            for block in response.content:
                find_file_ids(block)

        return list(set(file_ids))

    # ============================================================
    # Citations (å¼•ç”¨)
    # ============================================================

    def enable_citations(self) -> None:
        """å¯ç”¨ Citations åŠŸèƒ½"""
        self._citations_enabled = True
        logger.info("âœ… Citations å·²å¯ç”¨")

    def disable_citations(self) -> None:
        """ç¦ç”¨ Citations åŠŸèƒ½"""
        self._citations_enabled = False

    def create_document_content(
        self, documents: List[Dict[str, Any]], enable_citations: bool = True
    ) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºå¸¦å¼•ç”¨çš„æ–‡æ¡£å†…å®¹

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å«ï¼š
                - type: "text" æˆ– "pdf"
                - data: æ–‡æ¡£å†…å®¹ï¼ˆtext/base64ï¼‰
                - title: æ–‡æ¡£æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            enable_citations: æ˜¯å¦å¯ç”¨å¼•ç”¨

        Returns:
            æ ¼å¼åŒ–çš„æ–‡æ¡£å†…å®¹åˆ—è¡¨

        ç¤ºä¾‹ï¼š
            docs = llm.create_document_content([
                {"type": "text", "data": "è¿™æ˜¯æ–‡æ¡£å†…å®¹...", "title": "æ–‡æ¡£1"}
            ])
        """
        formatted = []

        for doc in documents:
            doc_type = doc.get("type", "text")
            data = doc.get("data", "")
            title = doc.get("title", "")

            if doc_type == "text":
                formatted.append(
                    {
                        "type": "document",
                        "source": {"type": "text", "media_type": "text/plain", "data": data},
                        "title": title,
                        "citations": {"enabled": enable_citations},
                    }
                )
            elif doc_type == "pdf":
                formatted.append(
                    {
                        "type": "document",
                        "source": {"type": "base64", "media_type": "application/pdf", "data": data},
                        "title": title,
                        "citations": {"enabled": enable_citations},
                    }
                )

        return formatted

    async def create_message_with_citations(
        self, query: str, documents: List[Dict[str, Any]], system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """
        ä½¿ç”¨å¼•ç”¨åŠŸèƒ½åˆ›å»ºæ¶ˆæ¯

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            documents: æ–‡æ¡£åˆ—è¡¨
            system: ç³»ç»Ÿæç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMResponseï¼ˆåŒ…å«å¼•ç”¨ä¿¡æ¯ï¼‰

        ç¤ºä¾‹ï¼š
            response = await llm.create_message_with_citations(
                query="æ–‡æ¡£ä¸­æåˆ°äº†ä»€ä¹ˆ?",
                documents=[
                    {"type": "text", "data": "è¿™æ˜¯æ–‡æ¡£å†…å®¹...", "title": "æ–‡æ¡£1"}
                ]
            )
        """
        # æ„å»ºå¸¦å¼•ç”¨çš„å†…å®¹
        content = self.create_document_content(documents, enable_citations=True)
        content.append({"type": "text", "text": query})

        messages = [{"role": "user", "content": content}]

        request_params = {
            "model": self.config.model,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "messages": messages,
        }

        if system:
            request_params["system"] = system

        try:
            response = await self.async_client.messages.create(**request_params)
            return self._parse_response(response)
        except Exception as e:
            logger.error(f"âŒ Citations è°ƒç”¨å¤±è´¥: {e}")
            raise


# ============================================================
# å·¥å‚å‡½æ•°
# ============================================================


def create_claude_service(
    model: str = "claude-sonnet-4-5-20250929",
    api_key: Optional[str] = None,
    enable_thinking: bool = True,
    enable_caching: bool = False,
    **kwargs,
) -> ClaudeLLMService:
    """
    åˆ›å»º Claude æœåŠ¡çš„ä¾¿æ·å‡½æ•°

    Args:
        model: æ¨¡å‹åç§°
        api_key: API å¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        enable_thinking: å¯ç”¨ Extended Thinking
        enable_caching: å¯ç”¨ Prompt Caching
        **kwargs: å…¶ä»–é…ç½®å‚æ•°

    Returns:
        ClaudeLLMService å®ä¾‹
    """
    if api_key is None:
        api_key = os.getenv("ANTHROPIC_API_KEY")

    config = LLMConfig(
        provider=LLMProvider.CLAUDE,
        model=model,
        api_key=api_key,
        enable_thinking=enable_thinking,
        enable_caching=enable_caching,
        **kwargs,
    )

    return ClaudeLLMService(config)


# ============================================================
# æ³¨å†Œåˆ° LLMRegistry
# ============================================================


def _register_claude():
    """å»¶è¿Ÿæ³¨å†Œ Claude Providerï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰"""
    from .registry import LLMRegistry

    LLMRegistry.register(
        name="claude",
        service_class=ClaudeLLMService,
        adaptor_class=ClaudeAdaptor,
        default_model="claude-sonnet-4-5-20250929",
        api_key_env="ANTHROPIC_API_KEY",
        display_name="Claude",
        description="Anthropic Claude ç³»åˆ—æ¨¡å‹",
        supported_features=[
            "extended_thinking",
            "prompt_caching",
            "streaming",
            "tool_calling",
            "skills",
            "files_api",
            "citations",
        ],
    )


# æ¨¡å—åŠ è½½æ—¶æ³¨å†Œ
_register_claude()
