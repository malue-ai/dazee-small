"""
LLM æœåŠ¡åŸºç¡€æ¨¡å—

åŒ…å«ï¼š
- æšä¸¾å®šä¹‰ï¼ˆLLMProvider, ToolType, InvocationTypeï¼‰
- æ•°æ®ç±»ï¼ˆLLMConfig, LLMResponse, Messageï¼‰
- æŠ½è±¡åŸºç±»ï¼ˆBaseLLMServiceï¼‰
- ç»Ÿä¸€çš„ token è®¡ç®—å‡½æ•°ï¼ˆtiktokenï¼‰

è®¾è®¡åŸåˆ™ï¼š
1. åªæä¾›å¼‚æ­¥æ¥å£
2. ç»Ÿä¸€çš„æ•°æ®æ ¼å¼ï¼ˆå…¼å®¹ Claude API æ ¼å¼ï¼Œç”¨äºæ•°æ®åº“å­˜å‚¨ï¼‰
3. æ˜“äºæ‰©å±•ï¼ˆæ”¯æŒ Claudeã€OpenAIã€Gemini ç­‰ï¼‰
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Union

import tiktoken

# ============================================================
# ç»Ÿä¸€çš„ Token è®¡ç®—ï¼ˆä½¿ç”¨ tiktoken cl100k_baseï¼‰
# ============================================================

# å…¨å±€ tokenizer ç¼“å­˜
_tiktoken_encoder = None


def _get_tiktoken_encoder():
    """
    è·å– tiktoken encoderï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œå…¨å±€ç¼“å­˜ï¼‰

    PyInstaller æ‰“åŒ…å importlib.metadata.entry_points() æ— æ³•å‘ç°
    tiktoken çš„ç¼–ç æ’ä»¶ï¼Œéœ€è¦æ‰‹åŠ¨å¯¼å…¥å¹¶æ³¨å†Œã€‚
    """
    global _tiktoken_encoder
    if _tiktoken_encoder is None:
        try:
            _tiktoken_encoder = tiktoken.get_encoding("cl100k_base")
        except ValueError:
            # PyInstaller æ‰“åŒ…ç¯å¢ƒï¼šæ‰‹åŠ¨æ³¨å†Œ tiktoken ç¼–ç æ’ä»¶
            import importlib
            mod = importlib.import_module("tiktoken_ext.openai_public")
            constructors = mod.ENCODING_CONSTRUCTORS()
            # å°†ç¼–ç æ„é€ å™¨æ³¨å†Œåˆ° tiktoken å†…éƒ¨æ³¨å†Œè¡¨
            for enc_name, constructor_fn in constructors.items():
                tiktoken.registry.ENCODING_CONSTRUCTORS[enc_name] = constructor_fn
            _tiktoken_encoder = tiktoken.get_encoding("cl100k_base")
    return _tiktoken_encoder


def count_tokens(text: str) -> int:
    """
    è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡ï¼ˆä½¿ç”¨ tiktoken cl100k_baseï¼‰

    è¿™æ˜¯å¤šæ¨¡å‹é¡¹ç›®çš„ç»Ÿä¸€ token è®¡ç®—æ–¹æ¡ˆã€‚
    cl100k_base ç¼–ç é€‚ç”¨äº GPT-4/Claude ç­‰ä¸»æµæ¨¡å‹çš„è¿‘ä¼¼è®¡ç®—ã€‚

    Args:
        text: è¦è®¡ç®—çš„æ–‡æœ¬

    Returns:
        token æ•°é‡
    """
    if not text:
        return 0
    encoder = _get_tiktoken_encoder()
    return len(encoder.encode(text))


def _extract_message_text(content: Any) -> str:
    """
    é€’å½’æå–æ¶ˆæ¯ä¸­çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹

    æ”¯æŒçš„å†…å®¹æ ¼å¼ï¼š
    - å­—ç¬¦ä¸²
    - åˆ—è¡¨ï¼ˆåŒ…å«å¤šä¸ª blockï¼‰
    - å­—å…¸ï¼ˆtext/tool_result/tool_use/thinking/image blockï¼‰

    æ³¨æ„ï¼šimage block ä½¿ç”¨å›ºå®šå ä½ç¬¦ï¼ˆå®é™… token ç”± Claude æŒ‰åƒç´ è®¡ç®—ï¼‰
    """
    if isinstance(content, str):
        return content
    elif isinstance(content, list):
        return " ".join(_extract_message_text(item) for item in content)
    elif isinstance(content, dict):
        block_type = content.get("type", "")
        if block_type == "text":
            return content.get("text", "")
        elif block_type == "tool_result":
            return _extract_message_text(content.get("content", ""))
        elif block_type == "tool_use":
            tool_name = content.get("name", "")
            tool_input = content.get("input", {})
            return f"{tool_name}: {str(tool_input)}"
        elif block_type == "thinking":
            return content.get("thinking", "")
        elif block_type == "image":
            # å›¾ç‰‡ token ç”± Claude æŒ‰åƒç´ è®¡ç®—ï¼ˆçº¦ 1600 tokens/å¼ ï¼‰
            # ç”¨å›ºå®šå ä½æ–‡æœ¬ä»£æ›¿ base64ï¼Œé¿å… tiktoken è¯¯ç®—
            return "[image: ~1600 tokens]"
        else:
            return str(content.get("text", "") or content.get("content", ""))
    return str(content)


def count_message_tokens(msg: Dict[str, Any]) -> int:
    """
    è®¡ç®—å•æ¡æ¶ˆæ¯çš„ token æ•°

    Args:
        msg: æ¶ˆæ¯å­—å…¸ï¼ˆåŒ…å« role å’Œ contentï¼‰

    Returns:
        token æ•°é‡
    """
    role = msg.get("role", "")
    content = msg.get("content", "")
    text = f"{role}: {_extract_message_text(content)}"
    return count_tokens(text)


def count_messages_tokens(messages: List[Dict[str, Any]], system_prompt: str = "") -> int:
    """
    è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„ token æ•°

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        system_prompt: ç³»ç»Ÿæç¤ºè¯

    Returns:
        token æ•°é‡
    """
    all_text = system_prompt or ""

    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        msg_text = f"{role}: {_extract_message_text(content)}"
        all_text += "\n" + msg_text

    return count_tokens(all_text)


def count_tools_tokens(tools: List[Dict[str, Any]]) -> int:
    """
    è®¡ç®—å·¥å…·å®šä¹‰çš„ token æ•°

    Args:
        tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆClaude API æ ¼å¼ï¼‰

    Returns:
        token æ•°é‡
    """
    if not tools:
        return 0

    import json

    try:
        tools_json = json.dumps(tools, ensure_ascii=False)
        return count_tokens(tools_json)
    except Exception:
        # ä¿å®ˆä¼°è®¡ï¼šæ¯ä¸ªå·¥å…· 1000 tokens
        return len(tools) * 1000


def count_request_tokens(
    messages: List[Dict[str, Any]], system_prompt: str = "", tools: List[Dict[str, Any]] = None
) -> int:
    """
    è®¡ç®—å®Œæ•´ LLM è¯·æ±‚çš„ token æ•°ï¼ˆæ¶ˆæ¯ + ç³»ç»Ÿæç¤º + å·¥å…·ï¼‰

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        æ€» token æ•°
    """
    message_tokens = count_messages_tokens(messages, system_prompt)
    tools_tokens = count_tools_tokens(tools) if tools else 0
    return message_tokens + tools_tokens


# ============================================================
# æšä¸¾å®šä¹‰
# ============================================================


class LLMProvider(Enum):
    """LLM æä¾›å•†"""

    CLAUDE = "claude"
    OPENAI = "openai"
    GEMINI = "gemini"
    QWEN = "qwen"  # é€šä¹‰åƒé—®ï¼ˆé˜¿é‡Œäº‘ï¼‰
    DEEPSEEK = "deepseek"  # DeepSeekï¼ˆæ·±åº¦æ±‚ç´¢ï¼‰
    GLM = "glm"  # GLMï¼ˆæ™ºè°±AIï¼‰
    MINIMAX = "minimax"  # MiniMaxï¼ˆAnthropic API å…¼å®¹ï¼‰


class ToolType(Enum):
    """
    å·¥å…·ç±»å‹ï¼ˆç»Ÿä¸€æŠ½è±¡ï¼‰

    Claude Client Tools (å®¢æˆ·ç«¯æ‰§è¡Œ):
    - BASH: Shell å‘½ä»¤
    - TEXT_EDITOR: æ–‡æœ¬ç¼–è¾‘å™¨
    - COMPUTER_USE: è®¡ç®—æœºä½¿ç”¨ (Beta)

    è‡ªå®šä¹‰å·¥å…·:
    - CUSTOM: ç”¨æˆ·è‡ªå®šä¹‰å·¥å…·

    æ³¨ï¼šæ‰€æœ‰æœåŠ¡å™¨å·¥å…·å·²ç§»é™¤ï¼Œæœç´¢é€šè¿‡ Skills æä¾›
    """

    # Client Tools
    BASH = "bash"
    TEXT_EDITOR = "text_editor"
    COMPUTER_USE = "computer_use"

    # Custom
    CUSTOM = "custom"


class InvocationType(Enum):
    """
    è°ƒç”¨æ–¹å¼ç±»å‹

    - DIRECT: æ ‡å‡†å·¥å…·è°ƒç”¨
    - PROGRAMMATIC: ç¨‹åºåŒ–å·¥å…·è°ƒç”¨
    - STREAMING: ç»†ç²’åº¦æµå¼
    """

    DIRECT = "direct"
    PROGRAMMATIC = "programmatic"
    STREAMING = "streaming"


# ============================================================
# æ•°æ®ç±»
# ============================================================


@dataclass
class LLMConfig:
    """
    LLM é…ç½®

    Attributes:
        provider: LLM æä¾›å•†
        model: æ¨¡å‹åç§°
        api_key: API å¯†é’¥
        enable_thinking: å¯ç”¨ Extended Thinkingï¼ˆClaude ç‰¹æœ‰ï¼‰
        thinking_budget: Thinking token é¢„ç®—
        enable_caching: å¯ç”¨ Prompt Caching
        enable_streaming: å¯ç”¨æµå¼è¾“å‡º
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§è¾“å‡º token æ•°
        tools: å·¥å…·åˆ—è¡¨
        base_url: API åŸºç¡€ URLï¼ˆå¯é€‰ï¼Œç”¨äºè‡ªå®šä¹‰ endpoint æˆ–ä»£ç†ï¼‰
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    """

    provider: LLMProvider
    model: str
    api_key: str

    # Core capabilities
    enable_thinking: bool = True
    thinking_budget: int = 10000
    enable_caching: bool = False
    enable_streaming: bool = False

    # åŸºç¡€å‚æ•°
    temperature: float = 1.0
    max_tokens: int = 32768

    # Tools
    tools: List[Dict[str, Any]] = field(default_factory=list)

    # é«˜çº§åŠŸèƒ½
    enable_context_editing: bool = False
    enable_structured_output: bool = False

    # ç½‘ç»œé…ç½®
    base_url: Optional[str] = None  # API åŸºç¡€ URLï¼ˆå¯é€‰ï¼Œç”¨äºè‡ªå®šä¹‰ endpointï¼‰
    timeout: float = 120.0  # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 2 åˆ†é’Ÿ
    max_retries: int = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°


@dataclass
class Message:
    """
    ç»Ÿä¸€çš„æ¶ˆæ¯æ ¼å¼ï¼ˆå…¼å®¹ Claude API æ ¼å¼ï¼‰

    Attributes:
        role: è§’è‰² ("user" | "assistant")
        content: æ¶ˆæ¯å†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ– content blocks åˆ—è¡¨ï¼‰

    Content Blocks æ ¼å¼ï¼ˆç”¨äºå¤æ‚æ¶ˆæ¯ï¼‰ï¼š
    ```python
    [
        {"type": "text", "text": "..."},
        {"type": "thinking", "thinking": "...", "signature": "..."},
        {"type": "tool_use", "id": "...", "name": "...", "input": {...}},
        {"type": "tool_result", "tool_use_id": "...", "content": "..."}
    ]
    ```
    """

    role: str
    content: Union[str, List[Dict[str, Any]]]


@dataclass
class LLMResponse:
    """
    ç»Ÿä¸€çš„ LLM å“åº”æ ¼å¼ï¼ˆå…¼å®¹ Claude API æ ¼å¼ï¼‰

    Attributes:
        content: æ–‡æœ¬å†…å®¹
        thinking: Extended Thinking å†…å®¹ï¼ˆClaude ç‰¹æœ‰ï¼‰
        tool_calls: å·¥å…·è°ƒç”¨åˆ—è¡¨
        stop_reason: åœæ­¢åŸå›  (end_turn, tool_use, max_tokens, etc.)
        usage: Token ä½¿ç”¨ç»Ÿè®¡
        model: å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆç”¨äºå‡†ç¡®è®¡è´¹ï¼‰ğŸ†•
        raw_content: åŸå§‹ content blocksï¼ˆç”¨äºæ¶ˆæ¯ç»­ä¼ ï¼‰
        is_stream: æ˜¯å¦ä¸ºæµå¼å“åº”
        cache_read_tokens: ç¼“å­˜è¯»å– tokensï¼ˆClaude ç‰¹æœ‰ï¼‰
        cache_creation_tokens: ç¼“å­˜åˆ›å»º tokensï¼ˆClaude ç‰¹æœ‰ï¼‰

    ğŸ†• æµå¼å·¥å…·è°ƒç”¨ï¼š
        tool_use_start: å·¥å…·è°ƒç”¨å¼€å§‹ {id, name}
        input_delta: å·¥å…·å‚æ•°å¢é‡ï¼ˆJSON ç‰‡æ®µï¼‰
    """

    content: str
    thinking: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    stop_reason: str = "end_turn"
    usage: Optional[Dict[str, int]] = None

    # ğŸ†• å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆç”¨äºå‡†ç¡®è®¡è´¹ï¼Œå°¤å…¶åœ¨å®¹ç¾åˆ‡æ¢æ—¶ï¼‰
    model: Optional[str] = None

    # åŸå§‹ content å—ï¼ˆç”¨äº tool_use å“åº”çš„æ¶ˆæ¯ç»­ä¼ ï¼‰
    raw_content: Optional[List[Any]] = None

    # æµå¼ç›¸å…³
    is_stream: bool = False

    # ğŸ†• æµå¼å·¥å…·è°ƒç”¨
    tool_use_start: Optional[Dict[str, str]] = None  # {id, name}
    input_delta: Optional[str] = None  # JSON ç‰‡æ®µ

    # Claude ç‰¹æœ‰
    cache_read_tokens: int = 0
    cache_creation_tokens: int = 0


# ============================================================
# æŠ½è±¡åŸºç±»
# ============================================================


class BaseLLMService(ABC):
    """
    LLM æœåŠ¡æŠ½è±¡åŸºç±»

    æ‰€æœ‰ LLM å®ç°å¿…é¡»ç»§æ‰¿æ­¤ç±»å¹¶å®ç°ä»¥ä¸‹æ–¹æ³•ï¼š
    - create_message_async: å¼‚æ­¥åˆ›å»ºæ¶ˆæ¯
    - create_message_stream: æµå¼åˆ›å»ºæ¶ˆæ¯
    - count_tokens: è®¡ç®— token æ•°é‡

    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    # å¼‚æ­¥è°ƒç”¨
    response = await llm.create_message_async(
        messages=[Message(role="user", content="Hello")],
        system="You are a helpful assistant"
    )

    # æµå¼è°ƒç”¨
    async for chunk in llm.create_message_stream(messages, system):
        print(chunk.content, end="")
    ```
    """

    @abstractmethod
    async def create_message_async(
        self,
        messages: List[Message],
        system: Optional[str] = None,
        tools: Optional[List[Union[ToolType, str, Dict]]] = None,
        **kwargs,
    ) -> LLMResponse:
        """
        åˆ›å»ºæ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMResponse å“åº”å¯¹è±¡
        """
        pass

    @abstractmethod
    async def create_message_stream(
        self,
        messages: List[Message],
        system: Optional[str] = None,
        tools: Optional[List[Union[ToolType, str, Dict]]] = None,
        on_thinking: Optional[Callable[[str], None]] = None,
        on_content: Optional[Callable[[str], None]] = None,
        on_tool_call: Optional[Callable[[Dict], None]] = None,
        **kwargs,
    ) -> AsyncIterator[LLMResponse]:
        """
        åˆ›å»ºæ¶ˆæ¯ï¼ˆæµå¼ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·åˆ—è¡¨
            on_thinking: thinking å›è°ƒ
            on_content: content å›è°ƒ
            on_tool_call: tool_call å›è°ƒ
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            LLMResponse ç‰‡æ®µ
        """
        raise NotImplementedError
        yield  # type hint: mark as async generator

    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®— token æ•°é‡ï¼ˆä½¿ç”¨ tiktoken cl100k_baseï¼‰

        TODO: å„ LLM æœåŠ¡å¯ä»¥é‡å†™æ­¤æ–¹æ³•ï¼Œä½¿ç”¨å®˜æ–¹ API è·å–ç²¾ç¡®å€¼
        - Claude: client.messages.count_tokens()
        - OpenAI: tiktoken ç²¾ç¡®è®¡ç®—
        - Qwen/Gemini: å®˜æ–¹ tokenizer

        Args:
            text: è¦è®¡ç®—çš„æ–‡æœ¬

        Returns:
            token æ•°é‡
        """
        return count_tokens(text)

    def convert_to_tool_schema(self, capability: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°†èƒ½åŠ›å®šä¹‰è½¬æ¢ä¸ºå·¥å…· schemaï¼ˆå¯è¢«å­ç±»è¦†ç›–ï¼‰

        Args:
            capability: èƒ½åŠ›å®šä¹‰ï¼ˆæ¥è‡ª capabilities.yamlï¼‰

        Returns:
            å·¥å…· schema
        """
        name = capability.get("name", "")
        input_schema = capability.get(
            "input_schema", {"type": "object", "properties": {}, "required": []}
        )
        description = capability.get("metadata", {}).get("description", f"Tool: {name}")

        return {"name": name, "description": description, "input_schema": input_schema}
