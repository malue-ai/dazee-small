"""
LLM æœåŠ¡æ¨¡å—

æä¾›ç»Ÿä¸€çš„ LLM æœåŠ¡æ¥å£ï¼Œæ”¯æŒå¤šç§ LLM æä¾›å•†ã€‚

åŒå±‚æ³¨å†Œæ¶æ„ï¼š
- LLMRegistryï¼ˆProvider å±‚ï¼‰: å®šä¹‰"å¦‚ä½•è°ƒç”¨"ï¼ˆæœåŠ¡ç±» + é€‚é…å™¨ï¼‰
- ModelRegistryï¼ˆModel å±‚ï¼‰: å®šä¹‰"å…·ä½“é…ç½®"ï¼ˆendpointã€èƒ½åŠ›ã€ç±»å‹ï¼‰

æ¨¡å—ç»“æ„ï¼š
- registry.py: Provider æ³¨å†Œä¸­å¿ƒ
- model_registry.py: Model æ³¨å†Œä¸­å¿ƒï¼ˆæ–°å¢ï¼‰
- base.py: åŸºç¡€ç±»å’Œæ•°æ®æ¨¡å‹
- adaptor.py: æ ¼å¼é€‚é…å™¨
- claude.py: Claude å®ç°
- openai.py: OpenAI å®ç°
- gemini.py: Gemini å®ç°
- qwen.py: åƒé—®å®ç°

ä½¿ç”¨ç¤ºä¾‹ï¼š
```python
from core.llm import (
    create_llm_service, LLMProvider, Message,
    LLMRegistry, ModelRegistry, ModelType
)

# æ–¹å¼ 1ï¼šé€šè¿‡ ModelRegistry åˆ›å»ºï¼ˆæ¨èï¼‰
llm = ModelRegistry.create_service("gpt-4o")
embedder = ModelRegistry.create_service("bge-m3")

# æ–¹å¼ 2ï¼šé€šè¿‡ LLMRegistry åˆ›å»º
llm = LLMRegistry.create_service("claude", model="claude-sonnet-4-5")

# æ–¹å¼ 3ï¼šä½¿ç”¨å·¥å‚å‡½æ•°
llm = create_llm_service(provider=LLMProvider.CLAUDE)

# æŸ¥è¯¢å¯ç”¨æ¨¡å‹
llm_models = ModelRegistry.list_models(model_type=ModelType.LLM)
embedding_models = ModelRegistry.list_models(model_type=ModelType.EMBEDDING)

# å¼‚æ­¥è°ƒç”¨
response = await llm.create_message_async(
    messages=[Message(role="user", content="Hello")],
    system="You are helpful"
)

# æµå¼è°ƒç”¨
async for chunk in llm.create_message_stream(messages, system):
    print(chunk.content, end="")
```
"""

import os
from typing import Optional, Union

# æ ¼å¼é€‚é…å™¨
from .adaptor import (
    BaseAdaptor,
    ClaudeAdaptor,
    GeminiAdaptor,
    OpenAIAdaptor,
    get_adaptor,
)

# åŸºç¡€ç±»å’Œæ•°æ®æ¨¡å‹
from .base import (  # æšä¸¾; æ•°æ®ç±»; æŠ½è±¡åŸºç±»; Token è®¡ç®—
    BaseLLMService,
    InvocationType,
    LLMConfig,
    LLMProvider,
    LLMResponse,
    Message,
    ToolType,
    count_message_tokens,
    count_messages_tokens,
    count_request_tokens,
    count_tokens,
    count_tools_tokens,
)

# Claude å®ç°
from .claude import (
    ClaudeLLMService,
    create_claude_service,
)

# Gemini å®ç°ï¼ˆå ä½ï¼‰
from .gemini import GeminiLLMService

# å¥åº·ç›‘æ§å™¨ï¼ˆğŸ†• V7.10ï¼‰
from .health_monitor import (
    HealthPolicy,
    LLMHealthMonitor,
    get_llm_health_monitor,
)

# Model æ³¨å†Œä¸­å¿ƒï¼ˆæ–°å¢ï¼‰
from .model_registry import (
    AdapterType,
    ModelCapabilities,
    ModelConfig,
    ModelRegistry,
    ModelType,
)

# OpenAI å®ç°ï¼ˆå ä½ï¼‰
from .openai import OpenAILLMService

# ğŸ†• åƒé—®å®ç°
from .qwen import (
    QwenConfig,
    QwenLLMService,
    create_qwen_service,
)

# Provider æ³¨å†Œä¸­å¿ƒ
from .registry import LLMRegistry

# å¤šæ¨¡å‹è·¯ç”±å™¨ï¼ˆğŸ†• V7.10ï¼‰
from .router import (
    ModelRouter,
    RouterPolicy,
    RouteTarget,
)

# å·¥å…·è°ƒç”¨å·¥å…·ï¼ˆğŸ†• V7.10ï¼‰
from .tool_call_utils import normalize_tool_calls

# ============================================================
# å·¥å‚å‡½æ•°
# ============================================================


def _create_single_llm_service(
    provider: Union[LLMProvider, str], model: str, api_key: Optional[str] = None, **kwargs
) -> BaseLLMService:
    """
    åˆ›å»ºå•ä¸ª LLM æœåŠ¡ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰

    é€šè¿‡ LLMRegistry åŠ¨æ€åˆ›å»ºï¼Œæ— éœ€ç¡¬ç¼–ç  if-elif é“¾ã€‚
    æ·»åŠ æ–° Provider åªéœ€ï¼š
    1. åˆ›å»º XxxLLMService ç±»
    2. åœ¨æ–‡ä»¶æœ«å°¾è°ƒç”¨ LLMRegistry.register()
    3. åœ¨é…ç½®ä¸­ä½¿ç”¨ provider: "xxx"

    Args:
        provider: LLM æä¾›å•†ï¼ˆå­—ç¬¦ä¸²æˆ–æšä¸¾ï¼‰
        model: æ¨¡å‹åç§°
        api_key: API å¯†é’¥
        **kwargs: å…¶ä»–é…ç½®å‚æ•°

    Returns:
        LLM æœåŠ¡å®ä¾‹
    """
    # ç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²ï¼ˆLLMRegistry ä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸º keyï¼‰
    provider_str = provider.value if isinstance(provider, LLMProvider) else provider

    # è¿‡æ»¤æ‰ ModelRouter ä¸“æœ‰å‚æ•°
    router_keys = {"fallbacks", "policy", "api_key_env"}
    filtered_kwargs = {k: v for k, v in kwargs.items() if k not in router_keys}

    # ğŸ†• ä½¿ç”¨ LLMRegistry åŠ¨æ€åˆ›å»ºæœåŠ¡ï¼ˆè‡ªåŠ¨å¤„ç† config_classï¼‰
    return LLMRegistry.create_service(
        provider=provider_str, model=model, api_key=api_key, **filtered_kwargs
    )


def create_llm_service(
    provider: Union[LLMProvider, str] = LLMProvider.CLAUDE,
    model: Optional[str] = None,
    api_key: Optional[str] = None,
    **kwargs,
) -> BaseLLMService:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»º LLM æœåŠ¡ï¼ˆæ”¯æŒå¤šæ¨¡å‹å®¹ç¾ï¼‰

    Args:
        provider: LLM æä¾›å•† (claude, openai, gemini)
        model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤æ ¹æ® provider é€‰æ‹©ï¼‰
        api_key: API å¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
            - api_key_env: API Key ç¯å¢ƒå˜é‡å

    Returns:
        LLM æœåŠ¡å®ä¾‹

    Example:
    ```python
    llm = create_llm_service(provider="claude", model="claude-sonnet-4-5")
    ```
    """
    # å­—ç¬¦ä¸²è½¬æšä¸¾
    if isinstance(provider, str):
        provider = LLMProvider(provider)

    # é»˜è®¤æ¨¡å‹
    default_models = {
        LLMProvider.CLAUDE: "claude-sonnet-4-5-20250929",
        LLMProvider.OPENAI: "gpt-4o",
        LLMProvider.GEMINI: "gemini-pro",
        LLMProvider.QWEN: "qwen3-max",
    }

    if model is None:
        model = default_models.get(provider)
        if model is None:
            raise ValueError(
                f"æœªæŒ‡å®šæ¨¡å‹ä¸” provider '{provider}' æ— é»˜è®¤æ¨¡å‹ã€‚"
                f"è¯·åœ¨ config.yaml ä¸­é…ç½® agent.provider å’Œ agent.model"
            )

    # ä»ç¯å¢ƒå˜é‡è¯»å– API Keyï¼ˆæ”¯æŒ api_key_env å‚æ•°ï¼‰
    api_key_env = kwargs.pop("api_key_env", None)
    if api_key is None:
        if api_key_env:
            api_key = os.getenv(api_key_env)
        else:
            env_keys = {
                LLMProvider.CLAUDE: "ANTHROPIC_API_KEY",
                LLMProvider.OPENAI: "OPENAI_API_KEY",
                LLMProvider.GEMINI: "GOOGLE_API_KEY",
                LLMProvider.QWEN: "DASHSCOPE_API_KEY",
            }
            api_key = os.getenv(env_keys.get(provider, "ANTHROPIC_API_KEY"))

    # æ¡Œé¢ç‰ˆç®€åŒ–ï¼šç§»é™¤ fallbacks/ModelRouterï¼Œç›´æ¥åˆ›å»ºå•ä¸ª LLM æœåŠ¡
    # å¿½ç•¥ fallbacks å’Œ policy å‚æ•°ï¼ˆå…¼å®¹æ—§é…ç½®ï¼‰
    kwargs.pop("fallbacks", None)
    kwargs.pop("policy", None)

    return _create_single_llm_service(provider, model, api_key, **kwargs)


# ============================================================
# å¯¼å‡º
# ============================================================

__all__ = [
    # ========== æ³¨å†Œä¸­å¿ƒ ==========
    # Provider æ³¨å†Œä¸­å¿ƒ
    "LLMRegistry",
    # Model æ³¨å†Œä¸­å¿ƒ
    "ModelRegistry",
    "ModelType",
    "AdapterType",
    "ModelConfig",
    "ModelCapabilities",
    # ========== æšä¸¾ ==========
    "LLMProvider",
    "ToolType",
    "InvocationType",
    # ========== æ•°æ®ç±» ==========
    "LLMConfig",
    "LLMResponse",
    "Message",
    # ========== åŸºç±» ==========
    "BaseLLMService",
    # ========== Token è®¡ç®— ==========
    "count_tokens",
    "count_message_tokens",
    "count_messages_tokens",
    "count_tools_tokens",
    "count_request_tokens",
    # ========== å®ç°ç±» ==========
    "ClaudeLLMService",
    "OpenAILLMService",
    "GeminiLLMService",
    "QwenLLMService",
    "QwenConfig",
    # ========== é€‚é…å™¨ ==========
    "BaseAdaptor",
    "ClaudeAdaptor",
    "OpenAIAdaptor",
    "GeminiAdaptor",
    "get_adaptor",
    # ========== è·¯ç”±å™¨ï¼ˆå®¹ç¾ï¼‰==========
    "ModelRouter",
    "RouteTarget",
    "RouterPolicy",
    # ========== å¥åº·ç›‘æ§å™¨ ==========
    "LLMHealthMonitor",
    "HealthPolicy",
    "get_llm_health_monitor",
    # ========== å·¥å…· ==========
    "normalize_tool_calls",
    # ========== å·¥å‚å‡½æ•° ==========
    "create_llm_service",
    "create_claude_service",
    "create_qwen_service",
]
