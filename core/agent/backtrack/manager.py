"""
å›æº¯ç®¡ç†å™¨

èŒè´£ï¼š
- LLM é©±åŠ¨çš„å›æº¯å†³ç­–
- çŠ¶æ€é‡è¯„ä¼°ä¸ç­–ç•¥è°ƒæ•´
- æ‰§è¡Œå›æº¯æ“ä½œï¼ˆPlan é‡è§„åˆ’ã€å·¥å…·æ›¿æ¢ã€æ„å›¾æ¾„æ¸…ã€å‚æ•°è°ƒæ•´ï¼‰

å›æº¯ç±»å‹ï¼š
- PLAN_REPLAN: Plan é‡è§„åˆ’ - å½“å‰ Plan æ­¥éª¤ä¸å¯è¡Œï¼Œé‡æ–°åˆ†è§£
- TOOL_REPLACE: å·¥å…·æ›¿æ¢ - å½“å‰å·¥å…·ä¸é€‚åˆï¼Œæ¢ç”¨æ›¿ä»£å·¥å…·
- INTENT_CLARIFY: æ„å›¾æ¾„æ¸… - ç”¨æˆ·æ„å›¾ç†è§£åå·®ï¼Œè¯·æ±‚æ¾„æ¸…
- PARAM_ADJUST: å‚æ•°è°ƒæ•´ - æ‰§è¡Œå‚æ•°ä¸åˆç†ï¼Œè°ƒæ•´åé‡è¯•
- CONTEXT_ENRICH: ä¸Šä¸‹æ–‡è¡¥å…… - è¡¥å……ä¸Šä¸‹æ–‡ä¿¡æ¯åé‡è¯•
"""

import json
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional

from core.agent.backtrack.error_classifier import (
    BacktrackType,
    ClassifiedError,
    ErrorClassifier,
    ErrorLayer,
    get_error_classifier,
)
from logger import get_logger

logger = get_logger(__name__)


class BacktrackDecision(Enum):
    """å›æº¯å†³ç­–"""

    CONTINUE = "continue"  # ç»§ç»­æ‰§è¡Œä¸‹ä¸€è½®
    BACKTRACK = "backtrack"  # éœ€è¦å›æº¯
    FAIL_GRACEFULLY = "fail_gracefully"  # ä¼˜é›…å¤±è´¥
    ESCALATE = "escalate"  # å‡çº§ï¼ˆéœ€è¦äººå·¥ä»‹å…¥ï¼‰


@dataclass
class BacktrackContext:
    """å›æº¯ä¸Šä¸‹æ–‡"""

    session_id: str
    turn: int
    max_turns: int
    error: ClassifiedError
    execution_history: List[Dict[str, Any]] = field(default_factory=list)
    backtrack_count: int = 0
    max_backtracks: int = 3  # æœ€å¤§å›æº¯æ¬¡æ•°

    # å½“å‰æ‰§è¡ŒçŠ¶æ€
    current_plan: Optional[Dict[str, Any]] = None
    current_step_index: int = 0
    failed_tools: List[str] = field(default_factory=list)
    failed_strategies: List[str] = field(default_factory=list)


@dataclass
class BacktrackResult:
    """å›æº¯ç»“æœ"""

    decision: BacktrackDecision
    backtrack_type: BacktrackType
    action: Dict[str, Any]  # å…·ä½“æ“ä½œ
    reason: str
    confidence: float

    def to_dict(self) -> Dict[str, Any]:
        return {
            "decision": self.decision.value,
            "backtrack_type": self.backtrack_type.value,
            "action": self.action,
            "reason": self.reason,
            "confidence": self.confidence,
        }


class BacktrackManager:
    """
    å›æº¯ç®¡ç†å™¨

    è´Ÿè´£å¤„ç†ä¸šåŠ¡é€»è¾‘å±‚é”™è¯¯çš„å›æº¯å†³ç­–ï¼Œ
    ä½¿ç”¨ LLM è¿›è¡ŒçŠ¶æ€é‡è¯„ä¼°å’Œç­–ç•¥è°ƒæ•´ã€‚
    """

    def __init__(
        self,
        llm_service: Optional[Any] = None,
        error_classifier: Optional[ErrorClassifier] = None,
        max_backtracks: int = 3,
    ):
        """
        åˆå§‹åŒ–å›æº¯ç®¡ç†å™¨

        Args:
            llm_service: LLM æœåŠ¡ï¼ˆç”¨äºæ™ºèƒ½å†³ç­–ï¼‰
            error_classifier: é”™è¯¯åˆ†ç±»å™¨
            max_backtracks: æœ€å¤§å›æº¯æ¬¡æ•°
        """
        self.llm_service = llm_service
        self.error_classifier = error_classifier or get_error_classifier()
        self.max_backtracks = max_backtracks

        # å›æº¯å†å²è®°å½•
        self._backtrack_history: Dict[str, List[BacktrackResult]] = {}

    async def evaluate_and_decide(
        self, ctx: BacktrackContext, use_llm: bool = True
    ) -> BacktrackResult:
        """
        è¯„ä¼°é”™è¯¯å¹¶åšå‡ºå›æº¯å†³ç­–

        Args:
            ctx: å›æº¯ä¸Šä¸‹æ–‡
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½å†³ç­–

        Returns:
            BacktrackResult: å›æº¯å†³ç­–ç»“æœ
        """
        logger.info(
            f"ğŸ”„ å¼€å§‹å›æº¯è¯„ä¼°: session={ctx.session_id}, "
            f"turn={ctx.turn}, backtrack_count={ctx.backtrack_count}"
        )

        # æ£€æŸ¥æ˜¯å¦å·²è¾¾æœ€å¤§å›æº¯æ¬¡æ•°
        if ctx.backtrack_count >= ctx.max_backtracks:
            logger.warning(f"âš ï¸ å·²è¾¾æœ€å¤§å›æº¯æ¬¡æ•° ({ctx.max_backtracks})ï¼Œä¼˜é›…å¤±è´¥")
            return BacktrackResult(
                decision=BacktrackDecision.FAIL_GRACEFULLY,
                backtrack_type=BacktrackType.NO_BACKTRACK,
                action={"message": "å·²è¾¾æœ€å¤§å›æº¯æ¬¡æ•°"},
                reason=f"å·²å°è¯• {ctx.backtrack_count} æ¬¡å›æº¯ä»å¤±è´¥",
                confidence=1.0,
            )

        # æ£€æŸ¥æ˜¯å¦æ˜¯åŸºç¡€è®¾æ–½å±‚é”™è¯¯ï¼ˆä¸åº”è¯¥åˆ°è¿™é‡Œï¼‰
        if ctx.error.is_infrastructure_error():
            logger.info("ğŸ“¦ åŸºç¡€è®¾æ–½å±‚é”™è¯¯ï¼Œä½¿ç”¨ resilience æœºåˆ¶å¤„ç†")
            return BacktrackResult(
                decision=BacktrackDecision.CONTINUE,
                backtrack_type=BacktrackType.NO_BACKTRACK,
                action={"delegate_to": "resilience"},
                reason="åŸºç¡€è®¾æ–½å±‚é”™è¯¯ç”± resilience æœºåˆ¶å¤„ç†",
                confidence=1.0,
            )

        # ä¸šåŠ¡é€»è¾‘å±‚é”™è¯¯ï¼Œéœ€è¦å†³ç­–
        if use_llm and self.llm_service:
            result = await self._llm_decide(ctx)
        else:
            result = self._rule_based_decide(ctx)

        # è®°å½•å›æº¯å†å²
        self._record_backtrack(ctx.session_id, result)

        logger.info(
            f"âœ… å›æº¯å†³ç­–å®Œæˆ: decision={result.decision.value}, "
            f"backtrack_type={result.backtrack_type.value}, "
            f"confidence={result.confidence:.2f}"
        )

        return result

    async def _llm_decide(self, ctx: BacktrackContext) -> BacktrackResult:
        """ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½å†³ç­–"""
        try:
            from prompts.backtrack_prompt import (
                BACKTRACK_DECISION_PROMPT,
                BACKTRACK_SYSTEM_PROMPT,
            )

            prompt = BACKTRACK_DECISION_PROMPT.format(
                error_type=type(ctx.error.original_error).__name__,
                error_message=str(ctx.error.original_error),
                error_category=ctx.error.category.value,
                suggested_backtrack_type=ctx.error.backtrack_type.value,
                turn=ctx.turn,
                max_turns=ctx.max_turns,
                backtrack_count=ctx.backtrack_count,
                max_backtracks=ctx.max_backtracks,
                current_step=self._format_current_step(ctx),
                failed_tools=", ".join(ctx.failed_tools) or "æ— ",
                failed_strategies=", ".join(ctx.failed_strategies) or "æ— ",
                execution_history=self._format_execution_history(ctx),
            )

            response = await self.llm_service.create_message_async(
                messages=[{"role": "user", "content": prompt}],
                system=BACKTRACK_SYSTEM_PROMPT,
            )

            # è§£æå“åº”
            result = self._parse_llm_response(response.content)
            return result

        except Exception as e:
            logger.warning(f"âš ï¸ LLM å†³ç­–å¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™å†³ç­–: {e}")
            return self._rule_based_decide(ctx)

    def _rule_based_decide(self, ctx: BacktrackContext) -> BacktrackResult:
        """
        Conservative fallback when LLM is unavailable.

        Uses PARAM_ADJUST as the safest default (least disruptive).
        If PARAM_ADJUST was already tried for this step, escalates
        via the deterministic escalation path.
        """
        backtrack_type = BacktrackType.PARAM_ADJUST

        # Escalate if this strategy already failed for the current step
        strategy_key = f"{backtrack_type.value}_{ctx.current_step_index}"
        if strategy_key in ctx.failed_strategies:
            backtrack_type = self._escalate_backtrack_type(backtrack_type)

        action = self._build_action(backtrack_type, ctx)

        return BacktrackResult(
            decision=BacktrackDecision.BACKTRACK,
            backtrack_type=backtrack_type,
            action=action,
            reason="LLM ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¿å®ˆå›æº¯ç­–ç•¥",
            confidence=0.4,
        )

    def _escalate_backtrack_type(self, current_type: BacktrackType) -> BacktrackType:
        """å‡çº§å›æº¯ç±»å‹"""
        escalation_path = {
            BacktrackType.PARAM_ADJUST: BacktrackType.TOOL_REPLACE,
            BacktrackType.TOOL_REPLACE: BacktrackType.PLAN_REPLAN,
            BacktrackType.CONTEXT_ENRICH: BacktrackType.INTENT_CLARIFY,
            BacktrackType.PLAN_REPLAN: BacktrackType.INTENT_CLARIFY,
            BacktrackType.INTENT_CLARIFY: BacktrackType.NO_BACKTRACK,  # éœ€è¦äººå·¥
        }

        return escalation_path.get(current_type, BacktrackType.PLAN_REPLAN)

    def _build_action(self, backtrack_type: BacktrackType, ctx: BacktrackContext) -> Dict[str, Any]:
        """æ„å»ºå…·ä½“æ“ä½œ"""
        if backtrack_type == BacktrackType.PLAN_REPLAN:
            return {
                "operation": "replan",
                "from_step": ctx.current_step_index,
                "hint": "åŸºäºå½“å‰é”™è¯¯é‡æ–°è§„åˆ’åç»­æ­¥éª¤",
            }

        if backtrack_type == BacktrackType.TOOL_REPLACE:
            return {
                "operation": "replace_tool",
                "failed_tool": ctx.failed_tools[-1] if ctx.failed_tools else None,
                "find_alternative": True,
            }

        if backtrack_type == BacktrackType.PARAM_ADJUST:
            return {
                "operation": "adjust_params",
                "retry_with_modified_params": True,
            }

        if backtrack_type == BacktrackType.CONTEXT_ENRICH:
            return {
                "operation": "enrich_context",
                "gather_more_info": True,
            }

        if backtrack_type == BacktrackType.INTENT_CLARIFY:
            return {
                "operation": "clarify_intent",
                "ask_user": True,
                "question_hint": "è¯·æ¾„æ¸…æ‚¨çš„å…·ä½“éœ€æ±‚",
            }

        return {"operation": "unknown"}

    def _parse_llm_response(self, content: str) -> BacktrackResult:
        """è§£æ LLM å“åº”"""
        try:
            # æå– JSON
            json_match = content
            if "```json" in content:
                json_match = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                json_match = content.split("```")[1].split("```")[0]

            data = json.loads(json_match.strip())

            decision = BacktrackDecision[data.get("decision", "BACKTRACK").upper()]
            backtrack_type = BacktrackType[
                data.get("backtrack_type", "NO_BACKTRACK").upper().replace("-", "_")
            ]

            return BacktrackResult(
                decision=decision,
                backtrack_type=backtrack_type,
                action=data.get("action", {}),
                reason=data.get("reason", ""),
                confidence=float(data.get("confidence", 0.5)),
            )

        except Exception as e:
            logger.warning(f"âš ï¸ è§£æ LLM å“åº”å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å†³ç­–
            return BacktrackResult(
                decision=BacktrackDecision.BACKTRACK,
                backtrack_type=BacktrackType.PARAM_ADJUST,
                action={"operation": "retry_with_adjustment"},
                reason="LLM å“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥",
                confidence=0.3,
            )

    def _format_current_step(self, ctx: BacktrackContext) -> str:
        """æ ¼å¼åŒ–å½“å‰æ­¥éª¤ä¿¡æ¯"""
        if not ctx.current_plan:
            return "æ— è®¡åˆ’ä¿¡æ¯"

        steps = ctx.current_plan.get("steps", [])
        if ctx.current_step_index < len(steps):
            step = steps[ctx.current_step_index]
            return f"æ­¥éª¤ {ctx.current_step_index + 1}: {step.get('description', 'æœªçŸ¥')}"

        return f"æ­¥éª¤ {ctx.current_step_index + 1}: æœªçŸ¥"

    def _format_execution_history(self, ctx: BacktrackContext) -> str:
        """æ ¼å¼åŒ–æ‰§è¡Œå†å²"""
        if not ctx.execution_history:
            return "æ— æ‰§è¡Œå†å²"

        # åªæ˜¾ç¤ºæœ€è¿‘ 5 æ¡
        recent = ctx.execution_history[-5:]
        lines = []
        for i, entry in enumerate(recent):
            status = "âœ…" if entry.get("success") else "âŒ"
            action = entry.get("action", "unknown")
            result = entry.get("result", "")[:100]  # æˆªæ–­
            lines.append(f"{i+1}. {status} {action}: {result}")

        return "\n".join(lines)

    def _record_backtrack(self, session_id: str, result: BacktrackResult):
        """è®°å½•å›æº¯å†å²"""
        if session_id not in self._backtrack_history:
            self._backtrack_history[session_id] = []

        self._backtrack_history[session_id].append(result)

        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self._backtrack_history[session_id]) > 20:
            self._backtrack_history[session_id] = self._backtrack_history[session_id][-20:]

    def get_backtrack_history(self, session_id: str) -> List[BacktrackResult]:
        """è·å–å›æº¯å†å²"""
        return self._backtrack_history.get(session_id, [])

    def clear_session_history(self, session_id: str):
        """æ¸…é™¤ä¼šè¯å†å²"""
        if session_id in self._backtrack_history:
            del self._backtrack_history[session_id]

    # NOTE: execute_backtrack() å›è°ƒæ–¹æ³•å·²åˆ é™¤ï¼ˆä»æœªè¢«è°ƒç”¨ï¼‰ã€‚
    # å›æº¯æ‰§è¡Œé€»è¾‘å†…è”åœ¨ RVRBExecutor._handle_tool_error_with_backtrack() ä¸­ã€‚


# å…¨å±€å•ä¾‹
_backtrack_manager: Optional[BacktrackManager] = None


def get_backtrack_manager(llm_service: Optional[Any] = None) -> BacktrackManager:
    """è·å–å…¨å±€å›æº¯ç®¡ç†å™¨å®ä¾‹

    æ³¨æ„ï¼šå¦‚æœä¼ å…¥äº† llm_service ä¸”ç°æœ‰å®ä¾‹ç¼ºå°‘ LLMï¼Œä¼šè‡ªåŠ¨æ›´æ–°ã€‚
    è¿™ä¿®å¤äº†å•ä¾‹é¦–æ¬¡ä¸å¸¦ LLM åˆ›å»ºåï¼Œåç»­ LLM å†³ç­–æ°¸è¿œä¸æ‰§è¡Œçš„ Bugã€‚
    """
    global _backtrack_manager
    if _backtrack_manager is None:
        _backtrack_manager = BacktrackManager(llm_service=llm_service)
    elif llm_service and not _backtrack_manager.llm_service:
        # é¦–æ¬¡åˆ›å»ºæ—¶ LLM å¯èƒ½ä¸º Noneï¼Œåç»­ä¼ å…¥æ—¶å¿…é¡»æ›´æ–°
        logger.info("ğŸ”„ BacktrackManager: æ›´æ–° LLM æœåŠ¡ï¼ˆé¦–æ¬¡åˆ›å»ºæ—¶ç¼ºå¤±ï¼‰")
        _backtrack_manager.llm_service = llm_service
    return _backtrack_manager
