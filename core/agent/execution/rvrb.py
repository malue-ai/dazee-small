"""
RVRBExecutor - RVR-B æ‰§è¡Œç­–ç•¥

å®ç° React-Validate-Reflect-Backtrack-Repeat å¾ªç¯ã€‚

èŒè´£ï¼š
- åœ¨ RVR åŸºç¡€ä¸Šå¢åŠ å›æº¯èƒ½åŠ›
- å·¥å…·æ‰§è¡Œå¤±è´¥æ—¶å°è¯•å›æº¯å’Œé‡è¯•
- æ”¯æŒ checkpoint å’Œæ¢å¤

å›æº¯ç±»å‹ï¼š
- PLAN_REPLAN: Plan é‡è§„åˆ’
- TOOL_REPLACE: å·¥å…·æ›¿æ¢
- PARAM_ADJUST: å‚æ•°è°ƒæ•´
- CONTEXT_ENRICH: ä¸Šä¸‹æ–‡è¡¥å……
- INTENT_CLARIFY: æ„å›¾æ¾„æ¸…

è¿ç§»è‡ªï¼šcore/agent/simple/mixins/backtrack_mixin.py
"""

from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any, AsyncGenerator, Dict, List, Optional, Set

from core.agent.backtrack import (
    BacktrackContext,
    BacktrackDecision,
    BacktrackManager,
    BacktrackResult,
    BacktrackType,
    ClassifiedError,
    ErrorClassifier,
    get_backtrack_manager,
    get_error_classifier,
)
from core.agent.errors import record_tool_error
from core.agent.execution.protocol import (
    BaseExecutor,
    ExecutionContext,
    ExecutorConfig,
)
from core.agent.execution.rvr import RVRExecutor
from core.context import stable_json_dumps
from logger import get_logger
from utils.message_utils import (
    append_assistant_message,
    append_user_message,
    dict_list_to_messages,
    messages_to_dict_list,
)

if TYPE_CHECKING:
    from core.context.runtime import RuntimeContext

logger = get_logger(__name__)


def _extract_tool_hints(tool_results: List[Dict[str, Any]]) -> List[str]:
    """Extract _hint values from tool results for mandatory injection into LLM context.

    Checks both top-level and nested result._hint locations, since some tools
    (e.g. nodes) promote _hint to the top level while others may nest it.
    """
    import json

    hints: List[str] = []
    for tr in tool_results:
        content = tr.get("content", "")
        if not isinstance(content, str) or "_hint" not in content:
            continue
        try:
            data = json.loads(content)
        except (json.JSONDecodeError, ValueError):
            continue
        hint = data.get("_hint") or (
            data.get("result", {}).get("_hint") if isinstance(data.get("result"), dict) else None
        )
        if hint:
            hints.append(hint)
    return hints


async def _call_async(fn: Any) -> Any:
    """Type-safe wrapper: callable() narrows to (...)->object which isn't Awaitable."""
    return await fn()


@dataclass
class RVRBState:
    """RVR-B å¾ªç¯çŠ¶æ€ï¼ˆV12: ç§»é™¤å†—ä½™ max_turnsï¼Œç»Ÿä¸€ç”± ExecutorConfig ç®¡ç†ï¼‰"""

    session_id: str
    turn: int = 0
    backtrack_count: int = 0
    max_backtracks: int = 3

    # æ‰§è¡Œå†å²
    execution_history: List[Dict[str, Any]] = field(default_factory=list)

    # å¤±è´¥è®°å½•
    failed_tools: List[str] = field(default_factory=list)
    failed_strategies: List[str] = field(default_factory=list)

    # å¤±è´¥è·¯å¾„è®°å¿†ï¼ˆè®°å½•"å·¥å…·+å‚æ•°â†’å¤±è´¥åŸå› "ï¼Œå¼•å¯¼ LLM é¿å…é‡å¤çŠ¯é”™ï¼‰
    failed_approaches: List[Dict[str, str]] = field(default_factory=list)

    # åŒå·¥å…·è¿ç»­å¤±è´¥è®¡æ•°ï¼ˆæ¯”ç²¾ç¡®ç­¾åå»é‡æ›´å®½æ³›ï¼Œæ•è·å‚æ•°å¾®è°ƒåä»å¤±è´¥çš„æƒ…å†µï¼‰
    _tool_failure_streak: Dict[str, int] = field(default_factory=dict)
    # å› è¿ç»­å¤±è´¥è¢«åŠ¨æ€è£å‰ªçš„å·¥å…·ï¼ˆä» tools_for_llm ä¸­ç§»é™¤ï¼Œç‰©ç†é˜»æ­¢ LLM è°ƒç”¨ï¼‰
    pruned_tools: Set[str] = field(default_factory=set)

    # å›æº¯ç´¯è®¡ token æ¶ˆè€—ï¼ˆç”¨äºäº‹ä»¶ä¸ŠæŠ¥ï¼‰
    total_backtrack_tokens: int = 0

    # Plan ç›¸å…³
    current_plan: Optional[Dict[str, Any]] = None
    current_step_index: int = 0

    # æœ€è¿‘çš„é”™è¯¯
    last_error: Optional[ClassifiedError] = None

    def record_execution(
        self, action: str, success: bool, result: Any = None, error: Optional[Exception] = None
    ):
        """è®°å½•æ‰§è¡Œå†å²"""
        self.execution_history.append(
            {
                "turn": self.turn,
                "action": action,
                "success": success,
                "result": str(result)[:200] if result else None,
                "error": str(error) if error else None,
            }
        )

        if len(self.execution_history) > 50:
            self.execution_history = self.execution_history[-50:]

    def record_tool_failure(self, tool_name: str):
        """è®°å½•å·¥å…·å¤±è´¥"""
        if tool_name not in self.failed_tools:
            self.failed_tools.append(tool_name)

    def record_failed_approach(self, tool_name: str, approach: str, reason: str):
        """
        è®°å½•å¤±è´¥çš„æ–¹æ³•è·¯å¾„

        ç”¨äºåœ¨å›æº¯åæ€ä¸­æ³¨å…¥ï¼Œå¼•å¯¼ LLM é¿å…é‡å¤çŠ¯é”™ã€‚

        Args:
            tool_name: å·¥å…·å
            approach: ç®€è¦æè¿°å°è¯•çš„æ–¹æ³•
            reason: å¤±è´¥åŸå› 
        """
        entry = {"tool": tool_name, "approach": approach, "reason": reason}
        # é¿å…å®Œå…¨é‡å¤
        if entry not in self.failed_approaches:
            self.failed_approaches.append(entry)
        # åªä¿ç•™æœ€è¿‘ 10 æ¡
        if len(self.failed_approaches) > 10:
            self.failed_approaches = self.failed_approaches[-10:]

    def increment_backtrack(self):
        """å¢åŠ å›æº¯è®¡æ•°"""
        self.backtrack_count += 1

    def can_backtrack(self) -> bool:
        """æ˜¯å¦è¿˜å¯ä»¥å›æº¯"""
        return self.backtrack_count < self.max_backtracks

    def record_tool_outcome(self, tool_name: str, success: bool):
        """è®°å½•å·¥å…·æ‰§è¡Œç»“æœï¼Œç»´æŠ¤åŒå·¥å…·è¿ç»­å¤±è´¥è®¡æ•°"""
        if success:
            self._tool_failure_streak[tool_name] = 0
        else:
            self._tool_failure_streak[tool_name] = (
                self._tool_failure_streak.get(tool_name, 0) + 1
            )

    def get_tool_failure_streak(self, tool_name: str) -> int:
        """è·å–å·¥å…·è¿ç»­å¤±è´¥æ¬¡æ•°"""
        return self._tool_failure_streak.get(tool_name, 0)

    def to_backtrack_context(
        self, error: ClassifiedError, max_turns: int = 200
    ) -> BacktrackContext:
        """Convert to BacktrackContext (max_turns = infrastructure safety limit, not semantic)."""
        return BacktrackContext(
            session_id=self.session_id,
            turn=self.turn,
            max_turns=max_turns,
            error=error,
            execution_history=self.execution_history,
            backtrack_count=self.backtrack_count,
            max_backtracks=self.max_backtracks,
            current_plan=self.current_plan,
            current_step_index=self.current_step_index,
            failed_tools=self.failed_tools.copy(),
            failed_strategies=self.failed_strategies.copy(),
        )


class RVRBExecutor(RVRExecutor):
    """
    RVR-B æ‰§è¡Œå™¨

    åœ¨ RVR åŸºç¡€ä¸Šå¢åŠ å›æº¯ï¼ˆBacktrackï¼‰èƒ½åŠ›ã€‚

    å›æº¯ç­–ç•¥ï¼š
    1. æ£€æµ‹å·¥å…·æ‰§è¡Œå¤±è´¥
    2. åˆ†ç±»é”™è¯¯ï¼ˆåŸºç¡€è®¾æ–½å±‚ vs ä¸šåŠ¡é€»è¾‘å±‚ï¼‰
    3. ä¸šåŠ¡é€»è¾‘å±‚é”™è¯¯è¿›è¡Œå›æº¯è¯„ä¼°
    4. æ ¹æ®å›æº¯ç±»å‹æ‰§è¡Œæ¢å¤ç­–ç•¥

    ä½¿ç”¨æ–¹å¼ï¼š
        executor = RVRBExecutor(config=ExecutorConfig(
            enable_backtrack=True,
            max_backtrack_attempts=3
        ))

        async for event in executor.execute(
            messages=messages,
            context=ExecutionContext(llm=llm, session_id=session_id, ...)
        ):
            yield event
    """

    def __init__(self, config: Optional[ExecutorConfig] = None):
        """åˆå§‹åŒ– RVR-B æ‰§è¡Œå™¨"""
        super().__init__(config)

        # ç¡®ä¿å¯ç”¨å›æº¯
        if self.config:
            self.config.enable_backtrack = True

        # å›æº¯ç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._error_classifier: Optional[ErrorClassifier] = None
        self._backtrack_manager: Optional[BacktrackManager] = None

        # çŠ¶æ€ç®¡ç†
        self._rvrb_states: Dict[str, RVRBState] = {}

    @property
    def name(self) -> str:
        return "RVRBExecutor"

    def supports_backtrack(self) -> bool:
        return True

    def _get_error_classifier(self) -> ErrorClassifier:
        """è·å–é”™è¯¯åˆ†ç±»å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._error_classifier is None:
            self._error_classifier = get_error_classifier()
        return self._error_classifier

    def _get_backtrack_manager(self, llm) -> BacktrackManager:
        """è·å–å›æº¯ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œå§‹ç»ˆç¡®ä¿ LLM å·²æ³¨å…¥ï¼‰"""
        if self._backtrack_manager is None:
            self._backtrack_manager = get_backtrack_manager(llm)
        elif llm and not self._backtrack_manager.llm_service:
            # ç¡®ä¿ LLM æœåŠ¡å·²æ³¨å…¥ï¼ˆä¿®å¤é¦–æ¬¡åˆ›å»ºæ—¶ LLM ç¼ºå¤±çš„é—®é¢˜ï¼‰
            self._backtrack_manager.llm_service = llm
        return self._backtrack_manager

    def _get_rvrb_state(self, session_id: str) -> RVRBState:
        """è·å–æˆ–åˆ›å»º RVR-B çŠ¶æ€ï¼ˆV12: ç§»é™¤ max_turnsï¼Œç»Ÿä¸€ç”± ExecutorConfig ç®¡ç†ï¼‰"""
        if session_id not in self._rvrb_states:
            self._rvrb_states[session_id] = RVRBState(
                session_id=session_id,
                max_backtracks=self.config.max_backtrack_attempts if self.config else 3,
            )
        return self._rvrb_states[session_id]

    def _clear_rvrb_state(self, session_id: str):
        """æ¸…é™¤ RVR-B çŠ¶æ€"""
        if session_id in self._rvrb_states:
            del self._rvrb_states[session_id]

    async def _evaluate_backtrack(
        self, error: Exception, tool_name: str, tool_input: Dict[str, Any], state: RVRBState, llm
    ) -> BacktrackResult:
        """
        è¯„ä¼°æ˜¯å¦éœ€è¦å›æº¯

        Args:
            error: å¼‚å¸¸å¯¹è±¡
            tool_name: å·¥å…·åç§°
            tool_input: å·¥å…·è¾“å…¥
            state: RVR-B çŠ¶æ€
            llm: LLM æœåŠ¡

        Returns:
            BacktrackResult: å›æº¯å†³ç­–
        """
        # åˆ†ç±»é”™è¯¯
        classifier = self._get_error_classifier()
        classified_error = classifier.classify_tool_error(
            error=error,
            tool_name=tool_name,
            tool_input=tool_input,
        )

        state.last_error = classified_error
        state.record_tool_failure(tool_name)

        # è®°å½•å¤±è´¥è·¯å¾„ï¼ˆç”¨äºå›æº¯åæ€æ³¨å…¥ï¼‰
        approach_desc = str(tool_input)[:100] if tool_input else "default"
        state.record_failed_approach(
            tool_name=tool_name,
            approach=approach_desc,
            reason=classified_error.suggested_action[:100] if classified_error.suggested_action else str(error)[:100],
        )

        # åŸºç¡€è®¾æ–½å±‚é”™è¯¯ä¸éœ€è¦å›æº¯
        if classified_error.is_infrastructure_error():
            logger.info(f"ğŸ“¦ åŸºç¡€è®¾æ–½å±‚é”™è¯¯ï¼Œå§”æ‰˜ç»™ resilience æœºåˆ¶")
            return BacktrackResult(
                decision=BacktrackDecision.CONTINUE,
                backtrack_type=BacktrackType.NO_BACKTRACK,
                action={"delegate_to": "resilience"},
                reason="åŸºç¡€è®¾æ–½å±‚é”™è¯¯",
                confidence=1.0,
            )

        # æ£€æŸ¥æ˜¯å¦è¿˜å¯ä»¥å›æº¯
        if not state.can_backtrack():
            logger.warning(f"âš ï¸ å·²è¾¾æœ€å¤§å›æº¯æ¬¡æ•° ({state.max_backtracks})")
            return BacktrackResult(
                decision=BacktrackDecision.FAIL_GRACEFULLY,
                backtrack_type=BacktrackType.NO_BACKTRACK,
                action={"message": "å·²è¾¾æœ€å¤§å›æº¯æ¬¡æ•°"},
                reason=f"å·²å°è¯• {state.backtrack_count} æ¬¡å›æº¯",
                confidence=1.0,
            )

        # ä¸šåŠ¡é€»è¾‘å±‚é”™è¯¯ï¼Œè¿›è¡Œå›æº¯è¯„ä¼°
        manager = self._get_backtrack_manager(llm)
        backtrack_ctx = state.to_backtrack_context(classified_error)
        result = await manager.evaluate_and_decide(backtrack_ctx, use_llm=True)

        if result.decision == BacktrackDecision.BACKTRACK:
            state.increment_backtrack()

        return result

    async def _handle_tool_error_with_backtrack(
        self,
        error: Exception,
        tool_name: str,
        tool_input: Dict[str, Any],
        state: RVRBState,
        session_id: str,
        llm,
        tool_executor,
        context_engineering=None,
        tool_selector=None,
        runtime_ctx=None,
    ) -> tuple[str, bool, Optional[Dict]]:
        """
        å¸¦å›æº¯çš„å·¥å…·é”™è¯¯å¤„ç†ï¼ˆV12 å›æº¯â†”ç»ˆæ­¢è”åŠ¨ï¼‰

        V12 æ”¹åŠ¨ï¼š
        - æ–°å¢ runtime_ctx å‚æ•°ï¼Œç”¨äºå°†å›æº¯çŠ¶æ€åŒæ­¥åˆ° RuntimeContext
        - FAIL_GRACEFULLY / ESCALATE æ—¶è®¾ç½® ctx.backtracks_exhausted
        - INTENT_CLARIFY æ—¶è®¾ç½® ctx.backtrack_escalation

        Args:
            error: å¼‚å¸¸å¯¹è±¡
            tool_name: å·¥å…·åç§°
            tool_input: å·¥å…·è¾“å…¥
            state: RVR-B çŠ¶æ€
            session_id: ä¼šè¯ ID
            llm: LLM æœåŠ¡
            tool_executor: å·¥å…·æ‰§è¡Œå™¨
            context_engineering: ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ˆå¯é€‰ï¼‰
            tool_selector: å·¥å…·é€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
            runtime_ctx: RuntimeContextï¼ˆV12ï¼Œç”¨äºå›æº¯â†”ç»ˆæ­¢è”åŠ¨ï¼‰

        Returns:
            (result_content, is_error, backtrack_event)
        """
        # è¯„ä¼°æ˜¯å¦éœ€è¦å›æº¯
        backtrack_result = await self._evaluate_backtrack(
            error=error, tool_name=tool_name, tool_input=tool_input, state=state, llm=llm
        )

        backtrack_event = None

        if backtrack_result.decision == BacktrackDecision.BACKTRACK:
            logger.info(f"ğŸ”„ è§¦å‘å›æº¯: {backtrack_result.backtrack_type.value}")

            # ç”Ÿæˆå›æº¯äº‹ä»¶ï¼ˆV12: é™„å¸¦ç´¯è®¡ä¿¡æ¯ï¼‰
            backtrack_event = {
                "type": "backtrack",
                "data": {
                    **backtrack_result.to_dict(),
                    "attempt": f"{state.backtrack_count}/{state.max_backtracks}",
                    "cumulative_backtrack_tokens": state.total_backtrack_tokens
                    if hasattr(state, "total_backtrack_tokens")
                    else 0,
                },
            }

            # V12: åŒæ­¥å›æº¯è®¡æ•°åˆ° RuntimeContext
            if runtime_ctx:
                runtime_ctx.total_backtracks = state.backtrack_count

            # æ ¹æ®å›æº¯ç±»å‹å¤„ç†
            if backtrack_result.backtrack_type == BacktrackType.TOOL_REPLACE:
                alt_result = await self._try_alternative_tool(
                    tool_name, tool_input, state, tool_executor, tool_selector
                )
                if alt_result:
                    state.record_execution("backtrack:tool_replace", True, alt_result)
                    return alt_result, False, backtrack_event
                # æ›¿ä»£å·¥å…·æŸ¥æ‰¾å¤±è´¥ï¼ˆæˆ–æœªé…ç½® tool_selectorï¼‰ï¼Œ
                # fall through è®© LLM è‡ªè¡Œå†³ç­–æ›¿ä»£æ–¹æ¡ˆ

            # å›æº¯ä¿¡æ¯è¿”å›ç»™ LLMï¼Œå¼•å¯¼å…¶è‡ªè¡Œè°ƒæ•´ç­–ç•¥
            backtrack_info = backtrack_result.to_dict()
            if backtrack_result.backtrack_type == BacktrackType.TOOL_REPLACE:
                backtrack_info["hint"] = (
                    f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥ï¼Œè¯·é€‰æ‹©å…¶ä»–å·¥å…·æˆ–æ–¹æ³•å®Œæˆå½“å‰ä»»åŠ¡ã€‚"
                )
            result_content = stable_json_dumps(
                {"error": str(error), "backtrack": backtrack_info}
            )
            return result_content, True, backtrack_event

        elif backtrack_result.decision in (
            BacktrackDecision.FAIL_GRACEFULLY,
            BacktrackDecision.ESCALATE,
        ):
            # V12 å…³é”®æ”¹åŠ¨ï¼šå›æº¯è€—å°½ / å‡çº§ â†’ åŒæ­¥çŠ¶æ€åˆ° RuntimeContext
            # è¿™æ · AdaptiveTerminator åœ¨æœ¬è½®æœ«å°¾èƒ½æ„ŸçŸ¥åˆ°ï¼Œè§¦å‘ HITL ä¸‰é€‰ä¸€
            logger.warning(
                f"âš ï¸ å›æº¯å‡çº§: decision={backtrack_result.decision.value}, "
                f"backtracks={state.backtrack_count}/{state.max_backtracks}"
            )

            if runtime_ctx:
                runtime_ctx.backtracks_exhausted = True
                runtime_ctx.total_backtracks = state.backtrack_count

                if backtrack_result.backtrack_type == BacktrackType.INTENT_CLARIFY:
                    runtime_ctx.backtrack_escalation = "intent_clarify"
                else:
                    runtime_ctx.backtrack_escalation = "escalate"

            # ç”Ÿæˆå›æº¯è€—å°½äº‹ä»¶
            backtrack_event = {
                "type": "backtrack_exhausted",
                "data": {
                    "decision": backtrack_result.decision.value,
                    "total_attempts": state.backtrack_count,
                    "failed_tools": state.failed_tools,
                    "last_error": str(error)[:200],
                    "escalation": runtime_ctx.backtrack_escalation
                    if runtime_ctx
                    else None,
                },
            }

            # æ„å»ºåŒ…å«å›æº¯å†å²çš„é”™è¯¯æ‘˜è¦ï¼ˆå¸®åŠ© LLM ç†è§£çŠ¶å†µï¼‰
            result_content = stable_json_dumps(
                {
                    "error": str(error),
                    "backtrack_exhausted": True,
                    "attempts": state.backtrack_count,
                    "failed_tools": state.failed_tools,
                    "message": f"å·²å°è¯• {state.backtrack_count} ç§ä¸åŒæ–¹æ³•å‡å¤±è´¥ï¼Œç­‰å¾…ç”¨æˆ·å†³å®š",
                }
            )
            state.record_execution(
                f"backtrack_exhausted:{tool_name}", False, error=error
            )
            return result_content, True, backtrack_event

        # CONTINUE: ä¸éœ€è¦å›æº¯ï¼Œæ­£å¸¸è®°å½•é”™è¯¯
        if context_engineering:
            record_tool_error(context_engineering, tool_name, error, tool_input)

        result_content = stable_json_dumps({"error": str(error)})
        state.record_execution(f"tool:{tool_name}", False, error=error)

        return result_content, True, None

    async def _try_alternative_tool(
        self,
        failed_tool: str,
        tool_input: Dict[str, Any],
        state: RVRBState,
        tool_executor,
        tool_selector=None,
    ) -> Optional[str]:
        """
        å°è¯•ä½¿ç”¨æ›¿ä»£å·¥å…·ï¼ˆV10.1 è§£è€¦ï¼‰

        Args:
            failed_tool: å¤±è´¥çš„å·¥å…·åç§°
            tool_input: å·¥å…·è¾“å…¥
            state: RVR-B çŠ¶æ€
            tool_executor: å·¥å…·æ‰§è¡Œå™¨
            tool_selector: å·¥å…·é€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ›¿ä»£å·¥å…·çš„æ‰§è¡Œç»“æœï¼Œæˆ– None
        """
        if not tool_executor:
            return None

        if not tool_selector or not hasattr(tool_selector, "get_alternative_tools"):
            logger.info(
                f"âš ï¸ TOOL_REPLACE é™çº§: tool_selector æœªé…ç½®ï¼Œ"
                f"è·³è¿‡æ›¿ä»£å·¥å…·æŸ¥æ‰¾ï¼Œå°†é”™è¯¯ä¿¡æ¯è¿”å›ç»™ LLM è‡ªè¡Œå†³ç­–"
            )
            return None

        alternatives = tool_selector.get_alternative_tools(failed_tool)

        for alt_tool in alternatives:
            if alt_tool in state.failed_tools:
                continue

            try:
                logger.info(f"ğŸ”„ å°è¯•æ›¿ä»£å·¥å…·: {alt_tool}")
                result = await tool_executor.execute(alt_tool, tool_input)
                result_content = result if isinstance(result, str) else stable_json_dumps(result)
                logger.info(f"âœ… æ›¿ä»£å·¥å…·æˆåŠŸ: {alt_tool}")
                return result_content
            except Exception as e:
                logger.warning(f"âš ï¸ æ›¿ä»£å·¥å…·ä¹Ÿå¤±è´¥: {alt_tool} - {e}")
                state.record_tool_failure(alt_tool)
                continue

        return None

    # ==================== Termination Reply ====================

    _TERMINATION_REASON_HINTS = {
        "max_turns": "å·²è¾¾åˆ°æœ€å¤§æ‰§è¡Œè½®æ¬¡ã€‚",
        "max_duration": "ä»»åŠ¡æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå·²è‡ªåŠ¨æš‚åœã€‚",
        "idle_timeout": "æ‰§è¡Œè¿‡ç¨‹ä¸­ç­‰å¾…è¶…æ—¶ã€‚",
        "consecutive_failures": "è¿ç»­å¤šæ¬¡æ‰§è¡Œå¤±è´¥ã€‚",
        "user_stop": "ç”¨æˆ·å·²è¯·æ±‚åœæ­¢ã€‚",
        "hitl_no_confirm": "æœ‰æ“ä½œéœ€è¦ç”¨æˆ·ç¡®è®¤ï¼Œä½†å½“å‰æ— æ³•è·å–ç¡®è®¤ï¼Œå·²æš‚åœã€‚",
    }

    async def _generate_termination_reply(
        self,
        llm,
        llm_messages: list,
        system_prompt,
        ctx: "RuntimeContext",
        session_id: str,
        broadcaster,
        usage_tracker,
        reason: str,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Terminator ç»ˆæ­¢æ—¶ï¼Œè®© LLM åšæœ€åä¸€æ¬¡æ— å·¥å…·å›å¤ã€‚

        ç»™ LLM å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ + ç»ˆæ­¢åŸå› ï¼Œè®©å®ƒè‡ªç„¶åœ°æ€»ç»“å·²å®Œæˆçš„å·¥ä½œ
        å’Œç»ˆæ­¢çš„åŸå› ï¼Œè€Œä¸æ˜¯è¾“å‡ºç¡¬ç¼–ç æ–‡æ¡ˆã€‚

        å…³é”®ï¼štools=[] é˜»æ­¢ LLM è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œé˜²æ­¢å†æ¬¡è¿›å…¥å¾ªç¯ã€‚
        """
        reason_hint = self._TERMINATION_REASON_HINTS.get(reason, "ä»»åŠ¡å·²æš‚åœã€‚")

        # æ³¨å…¥ä¸€æ¡ user æ¶ˆæ¯ï¼Œå‘ŠçŸ¥ LLM éœ€è¦æ”¶å°¾
        from core.llm.base import Message

        termination_instruction = Message(
            role="user",
            content=(
                f"[ç³»ç»Ÿæç¤º] {reason_hint}\n"
                "è¯·ç®€è¦æ€»ç»“ä½ ç›®å‰å®Œæˆäº†å“ªäº›å·¥ä½œã€è¿˜æœ‰ä»€ä¹ˆæœªå®Œæˆï¼Œ"
                "ä»¥åŠç”¨æˆ·æ¥ä¸‹æ¥å¯ä»¥æ€ä¹ˆåšã€‚ä¸è¦è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œç›´æ¥å›å¤ç”¨æˆ·ã€‚"
            ),
        )
        final_messages = llm_messages + [termination_instruction]

        try:
            # è°ƒç”¨ LLMï¼Œä¸ä¼ ä»»ä½•å·¥å…·ï¼Œå¼ºåˆ¶çº¯æ–‡æœ¬å›å¤
            async for event in self._process_stream(
                llm=llm,
                messages=final_messages,
                system_prompt=system_prompt,
                tools=[],  # å…³é”®ï¼šæ— å·¥å…· â†’ LLM åªèƒ½ç”Ÿæˆæ–‡å­—
                ctx=ctx,
                session_id=session_id,
                broadcaster=broadcaster,
                usage_tracker=usage_tracker,
            ):
                yield event

            # æ ‡è®°å®Œæˆ
            final_content = (
                ctx.last_llm_response.content if ctx.last_llm_response else ""
            )
            if final_content:
                ctx.set_completed(final_content, reason)
        except Exception as e:
            # LLM è°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ç¡¬ç¼–ç  fallback
            logger.warning(f"ç»ˆæ­¢å›å¤ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ fallback: {e}")
            _fallback = f"{reason_hint}å¦‚éœ€ç»§ç»­è¯·å†æ¬¡å‘é€æ¶ˆæ¯ã€‚"
            yield {"type": "content", "data": {"text": _fallback}}
            ctx.set_completed(_fallback, reason)

    # ==================== Context Pollution æ¸…ç† ====================

    def _clean_backtrack_results(
        self,
        tool_results: List[Dict[str, Any]],
        state: "RVRBState",
    ) -> List[Dict[str, Any]]:
        """
        Context Pollution æ¸…ç† + å›æº¯æ¶ˆæ¯å‹ç¼©

        å›æº¯å‘ç”Ÿåï¼Œå°†å¤±è´¥çš„ tool_result æ›¿æ¢ä¸ºç®€æ´çš„å›æº¯æ‘˜è¦ï¼Œ
        é¿å…é”™è¯¯ä¿¡æ¯æ±¡æŸ“åç»­ LLM æ¨ç†ä¸Šä¸‹æ–‡ã€‚

        2025 ç ”ç©¶è¡¨æ˜ï¼šcontext pollutionï¼ˆé”™è¯¯ä¿¡æ¯æ®‹ç•™ï¼‰æ˜¯ Agent å›æº¯å
        æ€§èƒ½ä¸‹é™çš„ä¸»è¦åŸå› ã€‚æ¸…ç†æ±¡æŸ“ä¸Šä¸‹æ–‡å¯æ˜¾è‘—æå‡é‡è¯•æˆåŠŸç‡ã€‚

        ç­–ç•¥ï¼š
        - æˆåŠŸçš„ tool_resultï¼šä¿ç•™åŸæ ·
        - å¤±è´¥çš„ tool_resultï¼šæ›¿æ¢ä¸ºç®€æ´æ‘˜è¦ + åæ€å»ºè®®
        - å¤šæ¬¡å¤±è´¥ï¼šå‹ç¼©ä¸ºä¸€æ¡æ±‡æ€»ï¼ˆèŠ‚çœ tokenï¼‰

        Args:
            tool_results: åŸå§‹ tool_result åˆ—è¡¨
            state: RVR-B çŠ¶æ€ï¼ˆå«å¤±è´¥å†å²ï¼‰

        Returns:
            æ¸…ç†åçš„ tool_result åˆ—è¡¨
        """
        if not state.backtrack_count:
            # æœªå‘ç”Ÿå›æº¯ï¼ŒåŸæ ·è¿”å›
            return tool_results

        cleaned = []
        failed_summaries = []

        for result in tool_results:
            if not result.get("is_error"):
                # æˆåŠŸçš„ç»“æœä¿ç•™
                cleaned.append(result)
            else:
                # å¤±è´¥çš„ç»“æœæ”¶é›†æ‘˜è¦
                content = result.get("content", "")
                # æˆªå–é”™è¯¯æ ¸å¿ƒä¿¡æ¯ï¼ˆä¸è¶…è¿‡ 100 å­—ç¬¦ï¼‰
                error_brief = content[:100] if isinstance(content, str) else str(content)[:100]
                failed_summaries.append(error_brief)

        if failed_summaries:
            # å°†å¤šæ¡å¤±è´¥å‹ç¼©ä¸ºä¸€æ¡ç®€æ´çš„å›æº¯æ‘˜è¦ + åæ€
            reflection = self._build_reflection_summary(failed_summaries, state)
            cleaned.append({
                "type": "tool_result",
                "tool_use_id": "backtrack_summary",
                "content": reflection,
                "is_error": False,  # æ ‡è®°ä¸ºéé”™è¯¯ï¼Œè®© LLM è§†ä¸ºå‚è€ƒä¿¡æ¯
            })

        return cleaned if cleaned else tool_results

    def _build_reflection_summary(
        self,
        failed_summaries: List[str],
        state: "RVRBState",
    ) -> str:
        """
        æ„å»º Contrastive Reflection åæ€æ‘˜è¦

        åœ¨é‡è¯•å‰å‘Šè¯‰ LLM"å‘ç”Ÿäº†ä»€ä¹ˆ + ä¸ºä»€ä¹ˆå¤±è´¥ + æ€ä¹ˆé¿å…"ï¼Œ
        å¼•å¯¼ LLM ç”¨ä¸åŒç­–ç•¥é‡è¯•è€Œéé‡å¤çŠ¯é”™ã€‚

        åŒ…å« failed_approaches è·¯å¾„è®°å¿†ï¼Œæ˜ç¡®åˆ—å‡ºå·²å°è¯•è¿‡çš„æ–¹æ³•ã€‚
        """
        failed_tools = list(state.failed_tools) if hasattr(state, "failed_tools") else []

        parts = [
            f"[å›æº¯åæ€] å·²å°è¯• {state.backtrack_count} æ¬¡å›æº¯ã€‚",
        ]

        if failed_tools:
            parts.append(f"å¤±è´¥çš„å·¥å…·: {', '.join(failed_tools)}ã€‚")

        if len(failed_summaries) == 1:
            parts.append(f"å¤±è´¥åŸå› : {failed_summaries[0]}")
        elif failed_summaries:
            parts.append(f"å¤±è´¥åŸå› æ±‡æ€»: {'; '.join(failed_summaries[:3])}")

        # æ³¨å…¥å¤±è´¥è·¯å¾„è®°å¿†ï¼ˆè®© LLM æ˜ç¡®çŸ¥é“å“ªäº›æ–¹æ³•å·²è¯•è¿‡ï¼‰
        if state.failed_approaches:
            parts.append("å·²å°è¯•è¿‡çš„æ–¹æ³•ï¼ˆä¸è¦é‡å¤ï¼‰:")
            for i, fa in enumerate(state.failed_approaches[-5:], 1):
                parts.append(
                    f"  {i}. {fa['tool']}: {fa['approach']} â†’ å¤±è´¥: {fa['reason']}"
                )

        parts.append("è¯·ä½¿ç”¨å®Œå…¨ä¸åŒçš„ç­–ç•¥æˆ–å·¥å…·é‡è¯•ã€‚")

        return "\n".join(parts)

    def _build_progressive_hint(
        self,
        tool_name: str,
        error_msg: str,
        state: RVRBState,
    ) -> Optional[str]:
        """
        æ¸è¿›å¼å¤±è´¥å¼•å¯¼ï¼ˆProgressive Hint Escalationï¼‰

        æ ¹æ®åŒä¸€å·¥å…·è¿ç»­å¤±è´¥æ¬¡æ•°ç”Ÿæˆä¸åŒå¼ºåº¦çš„å¼•å¯¼ï¼Œå¯¹æ¨ç†èƒ½åŠ›å¼±çš„æ¨¡å‹å°¤å…¶æœ‰æ•ˆï¼š
        - Level 1ï¼ˆé¦–æ¬¡å¤±è´¥ï¼‰ï¼šæ¸©å’Œå¼•å¯¼ï¼Œæç¤ºåˆ†æåŸå› 
        - Level 2ï¼ˆè¿ç»­2æ¬¡ï¼‰ï¼šæ˜¾å¼çº¦æŸï¼Œåˆ—å‡ºå·²è¯•æ–¹æ³•ï¼Œç¦æ­¢é‡å¤
        - Level 3ï¼ˆè¿ç»­3æ¬¡+ï¼‰ï¼šå¼ºåˆ¶è½¬å‘ï¼Œå·¥å…·ä»å¯ç”¨åˆ—è¡¨ä¸­åŠ¨æ€ç§»é™¤
        """
        streak = state.get_tool_failure_streak(tool_name)
        if streak <= 0:
            return None

        tool_approaches = [
            fa for fa in state.failed_approaches if fa["tool"] == tool_name
        ]

        if streak == 1:
            return (
                f"[å·¥å…·å¤±è´¥æé†’] {tool_name} æ‰§è¡Œå¤±è´¥: {error_msg[:150]}\n"
                "è¯·åˆ†æå¤±è´¥åŸå› ï¼Œè°ƒæ•´å‚æ•°æˆ–æ¢ç”¨å…¶ä»–å·¥å…·ã€‚"
                "ä¸è¦ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å‚æ•°é‡è¯•ã€‚"
            )

        if streak == 2:
            approaches_lines = "\n".join(
                f"  - {fa['approach'][:80]} â†’ {fa['reason'][:60]}"
                for fa in tool_approaches[-3:]
            )
            return (
                f"[ç³»ç»Ÿçº¦æŸ] {tool_name} å·²è¿ç»­å¤±è´¥ {streak} æ¬¡ã€‚\n"
                f"å·²å°è¯•è¿‡çš„æ–¹æ³•ï¼ˆç¦æ­¢é‡å¤ï¼‰:\n{approaches_lines}\n"
                "è¦æ±‚ï¼šå¿…é¡»æ¢ç”¨å®Œå…¨ä¸åŒçš„å·¥å…·ï¼Œæˆ–ä½¿ç”¨æ ¹æœ¬ä¸åŒçš„å‚æ•°ã€‚"
                "å¦‚æœæ²¡æœ‰æ›¿ä»£æ–¹æ¡ˆï¼Œç›´æ¥åŸºäºå·²æœ‰ä¿¡æ¯å›ç­”ç”¨æˆ·ã€‚"
            )

        # streak >= 3: å¼ºåˆ¶è½¬å‘ï¼ˆå‰¯ä½œç”¨ï¼šç”±è°ƒç”¨æ–¹è´Ÿè´£å†™å…¥ pruned_toolsï¼‰
        return (
            f"[å¼ºåˆ¶è½¬å‘] {tool_name} å·²è¿ç»­å¤±è´¥ {streak} æ¬¡ï¼Œå·²è¢«ç¦ç”¨ã€‚\n"
            f"ä½ æ— æ³•å†ä½¿ç”¨ {tool_name}ã€‚è¯·ä½¿ç”¨å…¶ä»–å·¥å…·å®Œæˆä»»åŠ¡ï¼Œ"
            "æˆ–ç›´æ¥å‘Šè¯‰ç”¨æˆ·å½“å‰æ— æ³•å®Œæˆè¯¥æ“ä½œå¹¶è¯´æ˜åŸå› ã€‚"
        )

    async def execute(
        self,
        messages: List[Dict[str, Any]],
        context: ExecutionContext,
        config: Optional[ExecutorConfig] = None,
        **kwargs,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æ‰§è¡Œ RVR-B ä¸»å¾ªç¯

        Args:
            messages: åˆå§‹æ¶ˆæ¯åˆ—è¡¨
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            config: æ‰§è¡Œé…ç½®
            **kwargs: é¢å¤–å‚æ•°

        Yields:
            äº‹ä»¶å­—å…¸
        """
        cfg = config or self.config

        # V10.1: ä» context è·å–ä¾èµ–ï¼ˆè§£è€¦ agentï¼‰
        llm = context.llm
        tool_executor = context.tool_executor
        broadcaster = context.broadcaster
        ctx = context.runtime_ctx
        session_id = context.session_id
        conversation_id = context.conversation_id
        system_prompt = context.system_prompt
        tools_for_llm = context.tools_for_llm
        intent = context.intent
        plan_cache = context.plan_cache

        # éªŒè¯å¿…éœ€ä¾èµ–
        if not llm:
            logger.error("âŒ RVRBExecutor: llm æœªæä¾›")
            yield {"type": "error", "data": {"message": "æ‰§è¡Œå™¨é…ç½®é”™è¯¯: llm æœªæä¾›"}}
            return

        if not ctx:
            logger.error("âŒ RVRBExecutor: runtime_ctx æœªæä¾›")
            yield {"type": "error", "data": {"message": "æ‰§è¡Œå™¨é…ç½®é”™è¯¯: runtime_ctx æœªæä¾›"}}
            return

        # è·å–é¢å¤–ä¾èµ–ï¼ˆV10.2 ToolExecutionFlow éœ€è¦ï¼‰
        usage_tracker = context.extra.get("usage_tracker")
        if not usage_tracker:
            from models.usage import UsageTracker

            usage_tracker = UsageTracker()

        context_engineering = context.extra.get("context_engineering")
        plan_todo_tool = context.extra.get("plan_todo_tool")
        event_manager = context.extra.get("event_manager")
        state_manager = context.extra.get("state_manager")

        logger.info(
            f"ğŸš€ RVRBExecutor å¼€å§‹æ‰§è¡Œ (signal-driven termination): "
            f"max_backtrack={cfg.max_backtrack_attempts}"
        )

        # åˆå§‹åŒ– RVR-B çŠ¶æ€
        state = self._get_rvrb_state(session_id)
        state.current_plan = plan_cache.get("plan")

        # è½¬æ¢æ¶ˆæ¯
        llm_messages = dict_list_to_messages(messages)

        # Context Engineering
        def _refresh_plan_injection(_llm_messages: List, *, inject_errors: bool) -> List:
            if not context_engineering or not plan_cache.get("plan"):
                return _llm_messages
            prepared_messages = context_engineering.prepare_messages_for_llm(
                messages=messages_to_dict_list(_llm_messages),
                plan=plan_cache.get("plan"),
                inject_plan=True,
                inject_errors=inject_errors,
            )
            return dict_list_to_messages(prepared_messages)

        turn = 0
        while True:
            # æ¯è½®è°ƒç”¨ LLM å‰åˆ·æ–° Plan æ³¨å…¥ï¼ˆPlan å¯èƒ½åœ¨ä¸Šä¸€è½®å·¥å…·è°ƒç”¨ä¸­è¢«æ›´æ–°ï¼‰
            llm_messages = _refresh_plan_injection(llm_messages, inject_errors=(turn == 0))

            ctx.next_turn()
            ctx.touch_activity()  # æ›´æ–°æ´»åŠ¨æ—¶é—´ï¼ˆç”¨äº idle_timeout æ£€æµ‹ï¼‰
            state.turn = turn

            logger.info(f"{'='*60}")
            logger.info(
                f"ğŸ”„ RVR-B Turn {turn + 1} (backtracks: {state.backtrack_count}/{state.max_backtracks})"
            )
            logger.info(f"{'='*60}")

            if cfg.enable_stream:
                # åŠ¨æ€å·¥å…·è£å‰ªï¼šè¿ç»­å¤±è´¥çš„å·¥å…·ä»å¯ç”¨åˆ—è¡¨ä¸­ç§»é™¤
                effective_tools = tools_for_llm
                if state.pruned_tools and tools_for_llm:
                    candidate = [
                        t for t in tools_for_llm
                        if t.get("name") not in state.pruned_tools
                    ]
                    if candidate:
                        effective_tools = candidate
                        logger.info(f"ğŸš« åŠ¨æ€è£å‰ªå·¥å…·: {state.pruned_tools}")
                    else:
                        logger.warning(
                            f"âš ï¸ æ‰€æœ‰å·¥å…·å‡å·²è£å‰ªï¼Œä¿åº•ä¿ç•™å…¨éƒ¨å·¥å…·: "
                            f"{state.pruned_tools}"
                        )

                # æµå¼å¤„ç†ï¼ˆV10.1: ä½¿ç”¨çˆ¶ç±»çš„ _process_streamï¼‰
                async for event in self._process_stream(
                    llm=llm,
                    messages=llm_messages,
                    system_prompt=system_prompt,
                    tools=effective_tools,
                    ctx=ctx,
                    session_id=session_id,
                    broadcaster=broadcaster,
                    usage_tracker=usage_tracker,
                ):
                    yield event

                response = ctx.last_llm_response
                if response:
                    if response.stop_reason == "tool_use" and response.tool_calls:
                        # å·¥å…·è°ƒç”¨è®°å½•å·²ç§»è‡³ _handle_tool_calls_with_backtrack_stream
                        # åœ¨å·¥å…·æ‰§è¡Œåè®°å½• (identity, output_fingerprint)ï¼Œè€Œéæ‰§è¡Œå‰åªè®°åç§°

                        # V11.1: HITL å±é™©æ“ä½œç¡®è®¤ï¼ˆæ‰§è¡Œå‰æ‹¦æˆªï¼Œç­‰å¾…ç”¨æˆ·å†³ç­–ï¼‰
                        hitl_rejected = False
                        if cfg.terminator:
                            try:
                                pending_names = [
                                    t.get("name") for t in response.tool_calls if t.get("name")
                                ]
                                from core.termination.protocol import TerminationAction

                                hitl_decision = cfg.terminator.evaluate(
                                    ctx,
                                    last_stop_reason="tool_use",
                                    pending_tool_names=pending_names,
                                )
                                if (
                                    hitl_decision.action == TerminationAction.ASK_USER
                                    and "hitl_confirm" in (hitl_decision.reason or "")
                                ):
                                    # é€šçŸ¥å‰ç«¯æ˜¾ç¤ºç¡®è®¤å¼¹çª—
                                    yield {
                                        "type": "hitl_confirm",
                                        "data": {
                                            "reason": hitl_decision.reason,
                                            "tools": pending_names,
                                            "message": "å±é™©æ“ä½œéœ€ç”¨æˆ·ç¡®è®¤",
                                        },
                                    }

                                    # ç­‰å¾…ç”¨æˆ·å†³ç­–ï¼ˆapprove / rejectï¼‰
                                    wait_fn = (context.extra or {}).get(
                                        "wait_hitl_confirm_async"
                                    )
                                    if callable(wait_fn):
                                        user_choice = await _call_async(wait_fn)
                                        if user_choice == "approve":
                                            logger.info(
                                                f"HITL å·²æ‰¹å‡†: {pending_names}"
                                            )
                                            # ç”¨æˆ·æ‰¹å‡† â†’ ç»§ç»­æ‰§è¡Œå·¥å…·
                                        else:
                                            # ç”¨æˆ·æ‹’ç» â†’ æ‰§è¡Œ on_rejection ç­–ç•¥
                                            logger.info(
                                                f"HITL å·²æ‹’ç»: {pending_names}ï¼Œ"
                                                f"æ‰§è¡Œå›é€€ç­–ç•¥"
                                            )
                                            hitl_rejected = True
                                            async for evt in self._handle_hitl_rejection(
                                                context, ctx, cfg
                                            ):
                                                yield evt
                                            break
                                    else:
                                        # æ— ç­‰å¾…å‡½æ•°ï¼Œä¿å®ˆåœæ­¢ï¼ˆä¸æ‰§è¡Œå±é™©æ“ä½œï¼‰
                                        logger.warning(
                                            "HITL ç¡®è®¤: æ—  wait å‡½æ•°ï¼Œ"
                                            "ä¿å®ˆåœæ­¢ï¼ˆä¸æ‰§è¡Œå±é™©æ“ä½œï¼‰"
                                        )
                                        ctx.stop_reason = (
                                            hitl_decision.reason or "hitl_confirm"
                                        )
                                        async for evt in self._generate_termination_reply(
                                            llm=llm,
                                            llm_messages=llm_messages,
                                            system_prompt=system_prompt,
                                            ctx=ctx,
                                            session_id=session_id,
                                            broadcaster=broadcaster,
                                            usage_tracker=usage_tracker,
                                            reason="hitl_no_confirm",
                                        ):
                                            yield evt
                                        break
                            except Exception as e:
                                logger.warning(
                                    f"HITL æ£€æŸ¥å¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ: {e}",
                                    exc_info=True,
                                )

                        if hitl_rejected:
                            break

                        # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¸¦å›æº¯ï¼ŒV10.2 ä½¿ç”¨ ToolExecutionFlowï¼‰
                        async for event in self._handle_tool_calls_with_backtrack_stream(
                            response,
                            llm_messages,
                            session_id,
                            conversation_id,
                            ctx,
                            state,
                            llm,
                            tool_executor,
                            broadcaster,
                            usage_tracker,
                            context_engineering=context_engineering,
                            plan_cache=plan_cache,
                            plan_todo_tool=plan_todo_tool,
                            event_manager=event_manager,
                            state_manager=state_manager,
                        ):
                            yield event
                    elif response.stop_reason == "stream_error":
                        # ğŸš¨ LLM æµå¼ä¸­æ–­ï¼ˆç½‘ç»œé”™è¯¯å fallback ä¹Ÿå¤±è´¥ï¼‰
                        # ä¸æŒä¹…åŒ–ä¸å®Œæ•´çš„ tool_use blocksï¼Œé€šçŸ¥å‰ç«¯é”™è¯¯
                        logger.warning(
                            "æµå¼ä¸­æ–­: stop_reason=stream_errorï¼Œ"
                            "ä¸¢å¼ƒä¸å®Œæ•´ tool_useï¼Œç»ˆæ­¢æœ¬è½®"
                        )
                        # Yield error event so frontend can exit
                        # "executing" state instead of hanging
                        yield {
                            "type": "error",
                            "data": {
                                "message": "ç½‘ç»œæ³¢åŠ¨å¯¼è‡´å›å¤ä¸­æ–­ï¼Œè¯·é‡è¯•",
                                "recoverable": True,
                            },
                        }
                        ctx.set_completed(
                            response.content or "ï¼ˆå›å¤å› ç½‘ç»œä¸­æ–­è€Œä¸å®Œæ•´ï¼‰",
                            "stream_error",
                        )
                        state.record_execution("stream_error", False, "LLM stream interrupted")
                        break
                    else:
                        ctx.set_completed(response.content, response.stop_reason)
                        state.record_execution("complete", True, response.content)
                        break
            else:
                # éæµå¼å¤„ç† - åŠ¨æ€å·¥å…·è£å‰ª
                effective_tools_ns = tools_for_llm
                if state.pruned_tools and tools_for_llm:
                    candidate_ns = [
                        t for t in tools_for_llm
                        if t.get("name") not in state.pruned_tools
                    ]
                    if candidate_ns:
                        effective_tools_ns = candidate_ns
                        logger.info(f"ğŸš« åŠ¨æ€è£å‰ªå·¥å…·(non-stream): {state.pruned_tools}")
                    else:
                        logger.warning(
                            f"âš ï¸ æ‰€æœ‰å·¥å…·å‡å·²è£å‰ª(non-stream)ï¼Œä¿åº•ä¿ç•™å…¨éƒ¨å·¥å…·"
                        )
                response = await llm.create_message_async(
                    messages=llm_messages, system=system_prompt, tools=effective_tools_ns  # type: ignore[arg-type]
                )

                usage_tracker.accumulate(response)

                if response.content:
                    yield {"type": "content", "data": {"text": response.content}}

                if response.stop_reason != "tool_use":
                    ctx.set_completed(response.content, response.stop_reason)
                    state.record_execution("complete", True, response.content)
                    break

                # å·¥å…·è°ƒç”¨è®°å½•å·²ç§»è‡³ _handle_tool_calls_with_backtrack_non_stream
                # åœ¨å·¥å…·æ‰§è¡Œåè®°å½• (identity, output_fingerprint)ï¼Œè€Œéæ‰§è¡Œå‰åªè®°åç§°

                # V11.1: HITL å±é™©æ“ä½œç¡®è®¤ï¼ˆéæµå¼ï¼Œç­‰å¾…ç”¨æˆ·å†³ç­–ï¼‰
                hitl_rejected_ns = False
                if cfg.terminator and response.tool_calls:
                    try:
                        pending_names = [
                            t.get("name") for t in response.tool_calls if t.get("name")
                        ]
                        from core.termination.protocol import TerminationAction

                        hitl_decision = cfg.terminator.evaluate(
                            ctx,
                            last_stop_reason="tool_use",
                            pending_tool_names=pending_names,
                        )
                        if (
                            hitl_decision.action == TerminationAction.ASK_USER
                            and "hitl_confirm" in (hitl_decision.reason or "")
                        ):
                            yield {
                                "type": "hitl_confirm",
                                "data": {
                                    "reason": hitl_decision.reason,
                                    "tools": pending_names,
                                    "message": "å±é™©æ“ä½œéœ€ç”¨æˆ·ç¡®è®¤",
                                },
                            }
                            wait_fn = (context.extra or {}).get(
                                "wait_hitl_confirm_async"
                            )
                            if callable(wait_fn):
                                user_choice = await _call_async(wait_fn)
                                if user_choice == "approve":
                                    logger.info(f"HITL å·²æ‰¹å‡†ï¼ˆéæµå¼ï¼‰: {pending_names}")
                                else:
                                    logger.info(
                                        f"HITL å·²æ‹’ç»ï¼ˆéæµå¼ï¼‰: {pending_names}"
                                    )
                                    hitl_rejected_ns = True
                                    async for evt in self._handle_hitl_rejection(
                                        context, ctx, cfg
                                    ):
                                        yield evt
                                    break
                            else:
                                ctx.stop_reason = (
                                    hitl_decision.reason or "hitl_confirm"
                                )
                                break
                    except Exception as e:
                        logger.warning(f"HITL æ£€æŸ¥å¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ: {e}", exc_info=True)

                if hitl_rejected_ns:
                    break

                await self._handle_tool_calls_with_backtrack_non_stream(
                    response,
                    llm_messages,
                    session_id,
                    conversation_id,
                    ctx,
                    state,
                    llm,
                    tool_executor,
                    broadcaster,
                    usage_tracker,
                    context_engineering=context_engineering,
                    plan_cache=plan_cache,
                    plan_todo_tool=plan_todo_tool,
                    event_manager=event_manager,
                    state_manager=state_manager,
                )

            turn += 1

            if ctx.is_completed():
                break

            # HITL pendingï¼šå·¥å…·è¿”å› pending_user_input åæš‚åœæ‰§è¡Œï¼Œç­‰å¾…ä¸‹ä¸€è½®æ¶ˆæ¯
            if ctx.stop_reason == "hitl_pending":
                break

            # V12: ç»ˆæ­¢ç­–ç•¥ï¼ˆå›æº¯â†”ç»ˆæ­¢è”åŠ¨ï¼Œä¿¡å·é©±åŠ¨ï¼Œæ— ç¡¬æ€§ max_turnsï¼‰
            if cfg.terminator and not ctx.is_completed():
                try:
                    from core.termination.protocol import (
                        FinishReason,
                        TerminationAction,
                    )

                    last_reason = (
                        getattr(ctx.last_llm_response, "stop_reason", None)
                        if ctx.last_llm_response
                        else None
                    )
                    _stop_requested = (
                        context.stop_event.is_set() if context.stop_event else False
                    )

                    # V12.1: ä» UsageTracker ä¼°ç®—è´¹ç”¨ï¼ˆåŸºäº ModelRegistry çœŸå®å®šä»·ï¼‰
                    _current_cost = None
                    if usage_tracker:
                        _current_cost = usage_tracker.estimate_cost()

                    decision = cfg.terminator.evaluate(
                        ctx,
                        last_stop_reason=last_reason,
                        stop_requested=_stop_requested,
                        pending_tool_names=None,
                        current_cost_usd=_current_cost,
                    )

                    if decision.should_stop:
                        ctx.stop_reason = decision.reason or "terminator"
                        # è®°å½•ç»“æ„åŒ–ç»ˆæ­¢åŸå› åˆ° RuntimeContextï¼ˆä¾›åç»­åˆ†æå’Œå‰ç«¯å±•ç¤ºï¼‰
                        if decision.finish_reason:
                            ctx.finish_reason = decision.finish_reason.value

                        # --- FinishReason å¤„ç†è·¯ç”± ---
                        # COMPLETED / AGENT_DECISION / USER_STOP / USER_ABORT:
                        #   æ­£å¸¸åœæ­¢ï¼Œç›´æ¥ break
                        # MAX_TURNS / MAX_DURATION / IDLE_TIMEOUT:
                        #   å®‰å…¨å…œåº•ï¼Œç›´æ¥ break
                        # HITL_CONFIRM:
                        #   å·²åœ¨å·¥å…·æ‰§è¡Œå‰æ‹¦æˆªå¤„ç†ï¼ˆline 693-751ï¼‰ï¼Œæ­¤å¤„ä¸éœ€è¦é¢å¤–åˆ†æ”¯
                        # CONSECUTIVE_FAILURES:
                        #   æ¨é€ ROLLBACK_OPTIONSï¼ˆä¸‹æ–¹å¤„ç†ï¼‰
                        # BACKTRACK_EXHAUSTED / INTENT_CLARIFY / COST_LIMIT / LONG_RUNNING_CONFIRM:
                        #   ASK_USER ç±»å‹ï¼Œåœ¨ should_stop=False åˆ†æ”¯å¤„ç†ï¼ˆä¸‹æ–¹ï¼‰

                        # V11.1: è¿ç»­å¤±è´¥ â†’ æ¨é€å›æ»šé€‰é¡¹ï¼ˆäº‹ä»¶ç±»å‹å¯¹é½å‰ç«¯ï¼‰
                        if decision.action == TerminationAction.ROLLBACK_OPTIONS:
                            _state_mgr = (context.extra or {}).get("state_manager")
                            _options = (
                                _state_mgr.get_rollback_options(session_id)
                                if _state_mgr
                                else []
                            )
                            yield {
                                "type": "rollback_options",
                                "data": {
                                    "task_id": session_id,
                                    "options": _options,
                                    "reason": decision.reason,
                                },
                            }

                        # å…œåº•ï¼šè®© LLM åšæœ€åä¸€æ¬¡æ— å·¥å…·å›å¤ï¼Œæ€»ç»“è¿›åº¦
                        #
                        # åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆç»ˆæ­¢å›å¤ï¼š
                        # - å¦‚æœæœ€åä¸€æ¬¡ LLM å“åº”æ˜¯ tool_useï¼ˆæ–‡æœ¬åœ¨å·¥å…·è°ƒç”¨ä¹‹å‰ï¼Œ
                        #   ä¸ç®—"æœ€ç»ˆå›å¤"ï¼‰ï¼Œå¿…é¡»ç”Ÿæˆæ€»ç»“
                        # - å¦‚æœæ²¡æœ‰ä»»ä½•æ–‡æœ¬å†…å®¹ï¼Œä¹Ÿå¿…é¡»ç”Ÿæˆæ€»ç»“
                        _last_was_tool_use = (
                            ctx.last_llm_response
                            and ctx.last_llm_response.stop_reason == "tool_use"
                        )
                        _has_final_text = (
                            not _last_was_tool_use
                            and ctx.last_llm_response
                            and ctx.last_llm_response.content
                            and ctx.last_llm_response.content.strip()
                        )
                        if not _has_final_text:
                            _reason = decision.reason or "unknown"
                            async for evt in self._generate_termination_reply(
                                llm=llm,
                                llm_messages=llm_messages,
                                system_prompt=system_prompt,
                                ctx=ctx,
                                session_id=session_id,
                                broadcaster=broadcaster,
                                usage_tracker=usage_tracker,
                                reason=_reason,
                            ):
                                yield evt
                        break

                    # === V12 æ–°å¢ï¼šå›æº¯è€—å°½ â†’ HITL ä¸‰é€‰ä¸€ ===
                    if (
                        decision.action == TerminationAction.ASK_USER
                        and decision.finish_reason == FinishReason.BACKTRACK_EXHAUSTED
                    ):
                        total_bt = getattr(ctx, "total_backtracks", 0)
                        yield {
                            "type": "backtrack_exhausted_confirm",
                            "data": {
                                "turn": ctx.current_turn,
                                "total_backtracks": total_bt,
                                "message": (
                                    f"å°æ­å­å·²ç»å°è¯•äº† {total_bt} ç§"
                                    f"ä¸åŒçš„æ–¹æ³•ï¼Œä½†éƒ½æ²¡æˆåŠŸã€‚æ‚¨å¸Œæœ›æ€ä¹ˆåšï¼Ÿ"
                                ),
                                "options": [
                                    {"id": "retry", "label": "æ¢ä¸ªæ€è·¯å†è¯•è¯•"},
                                    {"id": "rollback", "label": "æ’¤é”€å·²åšçš„æ“ä½œ"},
                                    {"id": "stop", "label": "å°±è¿™æ ·å§ï¼Œå…ˆä¸åšäº†"},
                                ],
                            },
                        }
                        wait_fn = (context.extra or {}).get(
                            "wait_backtrack_confirm_async"
                        )
                        if callable(wait_fn):
                            user_choice = await _call_async(wait_fn)
                            if user_choice == "rollback":
                                _state_mgr = (context.extra or {}).get(
                                    "state_manager"
                                )
                                _options = (
                                    _state_mgr.get_rollback_options(session_id)
                                    if _state_mgr
                                    else []
                                )
                                yield {
                                    "type": "rollback_options",
                                    "data": {
                                        "task_id": session_id,
                                        "options": _options,
                                        "reason": "ç”¨æˆ·é€‰æ‹©å›æ»š",
                                    },
                                }
                                ctx.stop_reason = "user_rollback_after_backtrack"
                                break
                            elif user_choice == "stop":
                                ctx.stop_reason = "user_stop_after_backtrack"
                                break
                            else:
                                # retry: é‡ç½®å›æº¯è®¡æ•°ï¼Œå…è®¸æ–°ä¸€è½®å›æº¯
                                state.backtrack_count = 0
                                state.pruned_tools.clear()
                                state._tool_failure_streak.clear()
                                ctx.backtracks_exhausted = False
                                ctx.backtrack_escalation = None
                                ctx.consecutive_failures = 0
                                logger.info("ğŸ”„ ç”¨æˆ·é€‰æ‹©é‡è¯•ï¼Œå›æº¯è®¡æ•°å·²é‡ç½®")
                        else:
                            # æ— ç­‰å¾…å‡½æ•°ï¼šé™çº§ä¸ºåœæ­¢
                            ctx.stop_reason = "backtrack_exhausted_no_confirm"
                            break

                    # === V12 æ–°å¢ï¼šæ„å›¾æ¾„æ¸… â†’ HITL è¯¢é—® ===
                    elif (
                        decision.action == TerminationAction.ASK_USER
                        and decision.finish_reason == FinishReason.INTENT_CLARIFY
                    ):
                        yield {
                            "type": "intent_clarify_request",
                            "data": {
                                "message": "å°æ­å­ä¸å¤ªç¡®å®šæ‚¨çš„å…·ä½“éœ€æ±‚ï¼Œèƒ½å†æè¿°ä¸€ä¸‹å—ï¼Ÿ",
                                "context": str(state.last_error)[:200]
                                if state.last_error
                                else "",
                            },
                        }
                        wait_fn = (context.extra or {}).get(
                            "wait_intent_clarify_async"
                        )
                        if callable(wait_fn):
                            clarification = await _call_async(wait_fn)
                            append_user_message(
                                llm_messages,
                                [{"type": "text", "text": clarification}],
                            )
                            ctx.backtrack_escalation = None
                            ctx.backtracks_exhausted = False
                            logger.info("ğŸ“ ç”¨æˆ·æ¾„æ¸…æ„å›¾ï¼Œç»§ç»­æ‰§è¡Œ")
                        else:
                            ctx.stop_reason = "intent_clarify_no_confirm"
                            break

                    # === V12.1 é‡æ„ï¼šè´¹ç”¨ç¡®è®¤ â†’ HITL é˜¶æ¢¯å¼æé†’ ===
                    # æ‰€æœ‰é˜¶æ¢¯éƒ½æ˜¯ HITL è¯¢é—®ï¼Œæ™ºèƒ½ä½“ä¸ä¼šä¸»åŠ¨æ›¿ç”¨æˆ·ç»ˆæ­¢ä»»åŠ¡
                    elif (
                        decision.action == TerminationAction.ASK_USER
                        and decision.finish_reason == FinishReason.COST_LIMIT
                    ):
                        cost_display = (
                            f"${_current_cost:.4f}" if _current_cost else "æœªçŸ¥"
                        )
                        # åˆ¤æ–­æ˜¯å¦ä¸ºç´§æ€¥çº§åˆ«
                        is_urgent = decision.reason.startswith("cost_urgent:")
                        event_type = (
                            "cost_urgent_confirm" if is_urgent else "cost_limit_confirm"
                        )
                        message = (
                            f"è´¹ç”¨æé†’ï¼šæœ¬æ¬¡ä»»åŠ¡è´¹ç”¨å·²è¾¾ {cost_display}ï¼Œ"
                            f"{'è´¹ç”¨è¾ƒé«˜ï¼Œè¯·ç¡®è®¤' if is_urgent else 'æ˜¯å¦ç»§ç»­ï¼Ÿ'}"
                        )
                        yield {
                            "type": event_type,
                            "data": {
                                "turn": ctx.current_turn,
                                "current_cost": cost_display,
                                "is_urgent": is_urgent,
                                "message": message,
                                "options": [
                                    {"id": "continue", "label": "ç»§ç»­æ‰§è¡Œ"},
                                    {"id": "stop", "label": "åœæ­¢ä»»åŠ¡"},
                                ],
                            },
                        }
                        wait_fn = (context.extra or {}).get(
                            "wait_cost_confirm_async"
                        )
                        if callable(wait_fn):
                            user_choice = await _call_async(wait_fn)
                            if user_choice == "stop":
                                ctx.stop_reason = "user_stop_cost_limit"
                                break
                            # continue: æ ‡è®°å·²ç¡®è®¤ï¼Œä¸å†é‡å¤è¯¢é—®
                            level = "urgent" if is_urgent else "confirm"
                            cfg.terminator.confirm_cost_continue(level=level)
                            logger.info(f"ç”¨æˆ·ç¡®è®¤ç»§ç»­ï¼ˆè´¹ç”¨{level}çº§åˆ«ï¼‰")
                        else:
                            ctx.stop_reason = "cost_limit_no_confirm"
                            break

                    # === V12.1: è´¹ç”¨é¢„è­¦ï¼ˆéé˜»å¡ï¼Œä»…é€šçŸ¥å‰ç«¯ï¼‰===
                    # å½“ terminator æ ‡è®° _cost_warned ä¸” decision æ­£å¸¸ç»§ç»­æ—¶ï¼Œå‘é€æç¤º
                    if (
                        _current_cost is not None
                        and getattr(cfg.terminator, "_cost_warned", False)
                        and not decision.should_stop
                        and decision.finish_reason != FinishReason.COST_LIMIT
                    ):
                        cost_display = f"${_current_cost:.4f}"
                        yield {
                            "type": "cost_warn",
                            "data": {
                                "turn": ctx.current_turn,
                                "current_cost": cost_display,
                                "message": f"æœ¬æ¬¡ä»»åŠ¡è´¹ç”¨å·²è¾¾ {cost_display}",
                            },
                        }

                    # é•¿ä»»åŠ¡ç¡®è®¤ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                    if (
                        decision.action == TerminationAction.ASK_USER
                        and decision.reason == "long_running_confirm"
                    ):
                        wait_fn = (context.extra or {}).get(
                            "wait_long_run_confirm_async"
                        )
                        if callable(wait_fn):
                            yield {
                                "type": "long_running_confirm",
                                "data": {
                                    "turn": ctx.current_turn,
                                    "message": f"ä»»åŠ¡å·²æ‰§è¡Œ {ctx.current_turn} è½®ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ",
                                },
                            }
                            await _call_async(wait_fn)
                            cfg.terminator.confirm_long_running()
                except Exception as e:
                    logger.warning(
                        f"terminator.evaluate() å¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ: {e}",
                        exc_info=True,
                    )

        # ---------- P0: æœ€ç»ˆå›å¤å…œåº•ï¼ˆä¸ RVRExecutor ä¿æŒä¸€è‡´ï¼‰----------
        if not ctx.is_completed() or not (ctx.final_result and ctx.final_result.strip()):
            _last_msg = llm_messages[-1] if llm_messages else None
            _last_role = (
                getattr(_last_msg, "role", None)
                or (_last_msg.get("role") if isinstance(_last_msg, dict) else None)
            )
            _last_content = (
                getattr(_last_msg, "content", "")
                or (_last_msg.get("content", "") if isinstance(_last_msg, dict) else "")
            )
            _needs_summary = (
                _last_role != "assistant"
                or not (isinstance(_last_content, str) and _last_content.strip())
            )

            if _needs_summary and llm:
                logger.info("ğŸ”„ æœ€ç»ˆå›å¤å…œåº•(RVRB): å¾ªç¯ç»“æŸä½†æ— éç©º assistant å›å¤ï¼Œç”Ÿæˆæ€»ç»“...")
                try:
                    # æ³¨å…¥æ€»ç»“æŒ‡ä»¤ï¼Œå¼•å¯¼ LLM è¾“å‡ºæœ€ç»ˆå›å¤
                    # ä¸æ³¨å…¥æ—¶ LLM å¯èƒ½è¿”å›ç©ºå†…å®¹ï¼ˆå› ä¸ºå®ƒä¸çŸ¥é“éœ€è¦æ€»ç»“ï¼‰
                    from core.llm.base import Message

                    summary_instruction = Message(
                        role="user",
                        content=(
                            "[ç³»ç»Ÿæç¤º] ä½ å·²å®Œæˆæ‰€æœ‰å·¥å…·è°ƒç”¨ã€‚"
                            "è¯·ç›´æ¥ç”¨è‡ªç„¶è¯­è¨€å›å¤ç”¨æˆ·ï¼Œç®€è¦æ€»ç»“ä½ å®Œæˆäº†ä»€ä¹ˆã€"
                            "ç»“æœæ˜¯ä»€ä¹ˆã€‚ä¸è¦è°ƒç”¨ä»»ä½•å·¥å…·ã€‚"
                        ),
                    )
                    summary_messages = llm_messages + [summary_instruction]

                    async for event in self._process_stream(
                        llm=llm,
                        messages=summary_messages,
                        system_prompt=system_prompt,
                        tools=[],
                        ctx=ctx,
                        session_id=session_id,
                        broadcaster=broadcaster,
                        usage_tracker=usage_tracker,
                        is_first_turn=False,
                    ):
                        yield event

                    final_resp = ctx.last_llm_response
                    if final_resp and final_resp.content:
                        ctx.set_completed(final_resp.content, final_resp.stop_reason)
                except Exception as e:
                    logger.warning(f"æœ€ç»ˆå›å¤å…œåº•(RVRB)å¤±è´¥: {e}", exc_info=True)

            # æœ€ç»ˆä¿åº•ï¼šå¦‚æœ P0 ä¹Ÿæ²¡æœ‰äº§ç”Ÿå†…å®¹ï¼ˆLLM è°ƒç”¨å¤±è´¥ç­‰ï¼‰ï¼Œ
            # é€šè¿‡ broadcaster å‘é€æœ€ä½é™åº¦æ–‡æœ¬ï¼Œç¡®ä¿ç”¨æˆ·çœ‹åˆ°å›å¤
            if not ctx.is_completed() or not (ctx.final_result and ctx.final_result.strip()):
                _fallback_text = "ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œå¦‚æœ‰é—®é¢˜è¯·ç»§ç»­å‘æˆ‘æé—®ã€‚"
                logger.warning("âš ï¸ P0 å…œåº•åä»æ— æœ€ç»ˆå›å¤ï¼Œå‘é€ä¿åº•æ¶ˆæ¯")
                try:
                    from core.agent.content_handler import create_content_handler

                    _fb_handler = create_content_handler(
                        broadcaster, ctx.block, session_id=session_id
                    )
                    await _fb_handler.handle_text(_fallback_text)
                    await _fb_handler.stop_block(session_id)
                    ctx.set_completed(_fallback_text, "fallback")
                except Exception as fb_err:
                    logger.error(f"ä¿åº•æ¶ˆæ¯ä¹Ÿå¤±è´¥: {fb_err}", exc_info=True)

        # æ¸…ç†çŠ¶æ€
        self._clear_rvrb_state(session_id)
        logger.info(f"âœ… RVRBExecutor æ‰§è¡Œå®Œæˆ: turns={ctx.current_turn}")

    async def _handle_tool_calls_with_backtrack_stream(
        self,
        response,
        llm_messages: List,
        session_id: str,
        conversation_id: str,
        ctx: "RuntimeContext",
        state: RVRBState,
        llm,
        tool_executor,
        broadcaster,
        usage_tracker,
        context_engineering=None,
        plan_cache: Optional[Dict] = None,
        plan_todo_tool=None,
        event_manager=None,
        state_manager=None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆæµå¼ï¼Œå¸¦å›æº¯ï¼ŒV10.2 ä½¿ç”¨ ToolExecutionFlowï¼‰"""
        from core.agent.content_handler import create_content_handler
        from core.agent.tools.flow import (
            ToolExecutionContext,
            ToolExecutionFlow,
            create_tool_execution_flow,
        )

        client_tools = [tc for tc in response.tool_calls if tc.get("type") == "tool_use"]

        # åˆ›å»º ToolExecutionContext
        tool_context = ToolExecutionContext(
            session_id=session_id,
            conversation_id=conversation_id,
            tool_executor=tool_executor,
            broadcaster=broadcaster,
            event_manager=event_manager,
            context_engineering=context_engineering,
            plan_cache=plan_cache or {},
            plan_todo_tool=plan_todo_tool,
            state_manager=state_manager,
        )

        flow = create_tool_execution_flow()
        content_handler = create_content_handler(broadcaster, ctx.block, session_id=session_id)
        tool_results = []
        _round_failures = []

        for tool_call in client_tools:
            tool_name = tool_call["name"]
            tool_input = tool_call["input"] or {}
            tool_id = tool_call["id"]

            _skip_compress = False
            try:
                # ä½¿ç”¨ ToolExecutionFlow æ‰§è¡Œå•ä¸ªå·¥å…·
                result_info = await flow.execute_single(tool_call, tool_context)
                result = result_info.result
                _skip_compress = isinstance(result, dict) and result.pop("_skip_fresh_compress", False)
                result_content = result if isinstance(result, str) else stable_json_dumps(result)
                is_error = result_info.is_error

                if not is_error:
                    state.record_execution(f"tool:{tool_name}", True, result_content)
                else:
                    raise Exception(result_info.error_msg or "å·¥å…·æ‰§è¡Œå¤±è´¥")

            except Exception as e:
                logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_name} - {e}")

                # å¸¦å›æº¯çš„é”™è¯¯å¤„ç†ï¼ˆV12: ä¼ å…¥ runtime_ctx ç”¨äºå›æº¯â†”ç»ˆæ­¢è”åŠ¨ï¼‰
                result_content, is_error, backtrack_event = (
                    await self._handle_tool_error_with_backtrack(
                        error=e,
                        tool_name=tool_name,
                        tool_input=tool_input,
                        state=state,
                        session_id=session_id,
                        llm=llm,
                        tool_executor=tool_executor,
                        context_engineering=context_engineering,
                        runtime_ctx=ctx,
                    )
                )

                # å‘é€å›æº¯äº‹ä»¶
                if backtrack_event:
                    yield backtrack_event

            state.record_tool_outcome(tool_name, not is_error)
            if is_error:
                _round_failures.append((tool_name, str(result_content)[:150]))

            yield await content_handler.emit_block(
                session_id=session_id,
                block_type="tool_result",
                content={"tool_use_id": tool_id, "content": result_content, "is_error": is_error},
            )

            # Immediate compression: prevent large tool outputs from bloating context
            from core.context.compaction import compress_fresh_tool_result
            compressed_content = (
                result_content if _skip_compress
                else compress_fresh_tool_result(result_content)
                if isinstance(result_content, str) else result_content
            )
            tool_results.append(
                {
                    "type": "tool_result",
                    "tool_use_id": tool_id,
                    "content": compressed_content,
                    "is_error": is_error,
                }
            )

            # Record tool call signature for dedup detection
            ctx.record_tool_call(tool_name, tool_input)

        append_assistant_message(llm_messages, response.raw_content)

        if tool_results:
            # Context Pollution æ¸…ç†ï¼šå›æº¯æˆåŠŸåï¼ˆæ›¿ä»£å·¥å…·è¿”å›äº†æ­£ç¡®ç»“æœï¼‰ï¼Œ
            # å°†å¤±è´¥çš„ tool_result æ›¿æ¢ä¸ºç®€æ´æ‘˜è¦ï¼Œé¿å…é”™è¯¯ä¿¡æ¯æ±¡æŸ“åç»­ LLM æ¨ç†ã€‚
            cleaned_results = self._clean_backtrack_results(tool_results, state)
            append_user_message(llm_messages, cleaned_results)

        # æ¸è¿›å¼å¤±è´¥å¼•å¯¼ï¼šå·¥å…·å¤±è´¥å³æ³¨å…¥ï¼ˆä¸ç­‰å›æº¯è§¦å‘ï¼‰ï¼ŒæŒ‰è¿ç»­å¤±è´¥æ¬¡æ•°å‡çº§å¼ºåº¦
        if _round_failures:
            _progressive_hints = []
            for _tn, _err in _round_failures:
                _hint = self._build_progressive_hint(_tn, _err, state)
                if _hint:
                    _progressive_hints.append(_hint)
                    # streak >= 3 æ—¶ç”±è°ƒç”¨æ–¹è´Ÿè´£å†™å…¥ pruned_toolsï¼ˆä¿æŒ _build_progressive_hint æ— å‰¯ä½œç”¨ï¼‰
                    if state.get_tool_failure_streak(_tn) >= 3:
                        state.pruned_tools.add(_tn)
            if _progressive_hints:
                append_user_message(
                    llm_messages, "\n\n".join(_progressive_hints)
                )
                logger.info(
                    f"ğŸ“Š æ¸è¿›å¼å¤±è´¥å¼•å¯¼: {len(_progressive_hints)} æ¡"
                    f" (pruned={state.pruned_tools or 'none'})"
                )

        # è½¨è¿¹å»é‡ï¼šå®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨è¿ç»­ N æ¬¡ â†’ æ³¨å…¥åæ€æç¤ºå¼•å¯¼ LLM æ¢æ€è·¯
        if ctx.detect_repeated_call(threshold=4):
            _dedup_hint = (
                "[ç³»ç»Ÿæç¤º] æ£€æµ‹åˆ°å®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨å·²è¿ç»­æ‰§è¡Œå¤šæ¬¡ï¼Œç»“æœä¸ä¼šæ”¹å˜ã€‚"
                "è¯·åœ¨ Thinking ä¸­åˆ†æåŸå› ï¼Œå°è¯•ä¸åŒçš„å‚æ•°ã€æ¢ä¸€ä¸ªå·¥å…·ã€æˆ–ç›´æ¥åŸºäºå·²æœ‰ä¿¡æ¯å›ç­”ç”¨æˆ·ã€‚"
            )
            append_user_message(llm_messages, _dedup_hint)
            logger.warning(
                f"ğŸ” è½¨è¿¹å»é‡: å®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨è¿ç»­ "
                f"{ctx._consecutive_duplicate_count + 1} æ¬¡ï¼Œæ³¨å…¥åæ€æç¤º"
            )

        # HITL pending æ£€æµ‹ï¼šå¦‚æœå·¥å…·è¿”å›äº† pending_user_inputï¼Œæš‚åœæ‰§è¡Œç­‰å¾…ç”¨æˆ·å“åº”ã€‚
        # é˜²æ­¢ LLM çœ‹åˆ° pending ç»“æœåå†æ¬¡è°ƒç”¨ hitlï¼ˆè¿ç»­ 2 æ¬¡è°ƒç”¨ bugï¼‰ã€‚
        for _tr in tool_results:
            _tr_content = _tr.get("content", "")
            if isinstance(_tr_content, str) and "pending_user_input" in _tr_content:
                ctx.stop_reason = "hitl_pending"
                logger.info("HITL pending æ£€æµ‹ï¼šå·¥å…·è¿”å› pending_user_inputï¼Œæš‚åœæ‰§è¡Œç­‰å¾…ç”¨æˆ·å“åº”")
                break

        # _hint å¼ºåˆ¶æ³¨å…¥ï¼šå·¥å…·ç»“æœä¸­å« _hint å­—æ®µæ—¶ï¼Œæå‡ä¸ºç‹¬ç«‹ç³»ç»Ÿæ¶ˆæ¯ï¼Œ
        # ç¡®ä¿ LLM ä¸ä¼šå›  _hint åŸ‹åœ¨åµŒå¥— JSON ä¸­è€Œå¿½ç•¥å®ƒã€‚
        _injected_hints = _extract_tool_hints(tool_results)
        if _injected_hints:
            _hint_msg = "[ç³»ç»Ÿæç¤º] " + " ".join(_injected_hints)
            append_user_message(llm_messages, _hint_msg)
            logger.info(f"ğŸ”” _hint å¼ºåˆ¶æ³¨å…¥ï¼ˆstreamï¼‰: {_hint_msg[:120]}...")

        # æ›´æ–°è¿ç»­å¤±è´¥è®¡æ•°ï¼ˆä¾›ç»ˆæ­¢ç­–ç•¥ä¸è‡ªåŠ¨å›æ»šä½¿ç”¨ï¼‰
        if any(r.get("is_error") for r in tool_results):
            ctx.consecutive_failures += 1
        else:
            ctx.consecutive_failures = 0
        ctx.touch_activity()  # å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ›´æ–°æ´»åŠ¨æ—¶é—´ï¼ˆidle_timeout æ£€æµ‹ï¼‰

    async def _handle_tool_calls_with_backtrack_non_stream(
        self,
        response,
        llm_messages: List,
        session_id: str,
        conversation_id: str,
        ctx: "RuntimeContext",
        state: RVRBState,
        llm,
        tool_executor,
        broadcaster,
        usage_tracker,
        context_engineering=None,
        plan_cache: Optional[Dict] = None,
        plan_todo_tool=None,
        event_manager=None,
        state_manager=None,
    ) -> None:
        """å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆéæµå¼ï¼Œå¸¦å›æº¯ï¼ŒV10.2 ä½¿ç”¨ ToolExecutionFlowï¼‰"""
        from core.agent.tools.flow import (
            ToolExecutionContext,
            ToolExecutionFlow,
            create_tool_execution_flow,
        )

        client_tools = [tc for tc in response.tool_calls if tc.get("type") == "tool_use"]

        append_assistant_message(llm_messages, response.raw_content)

        # åˆ›å»º ToolExecutionContext
        tool_context = ToolExecutionContext(
            session_id=session_id,
            conversation_id=conversation_id,
            tool_executor=tool_executor,
            broadcaster=broadcaster,
            event_manager=event_manager,
            context_engineering=context_engineering,
            plan_cache=plan_cache or {},
            plan_todo_tool=plan_todo_tool,
            state_manager=state_manager,
        )

        flow = create_tool_execution_flow()
        tool_results = []
        _round_failures = []

        for tool_call in client_tools:
            tool_name = tool_call["name"]
            tool_input = tool_call["input"] or {}
            tool_id = tool_call["id"]

            _skip_compress = False
            try:
                # ä½¿ç”¨ ToolExecutionFlow æ‰§è¡Œå•ä¸ªå·¥å…·
                result_info = await flow.execute_single(tool_call, tool_context)
                result = result_info.result
                _skip_compress = isinstance(result, dict) and result.pop("_skip_fresh_compress", False)
                result_content = result if isinstance(result, str) else stable_json_dumps(result)
                is_error = result_info.is_error

                if not is_error:
                    state.record_execution(f"tool:{tool_name}", True, result_content)
                else:
                    raise Exception(result_info.error_msg or "å·¥å…·æ‰§è¡Œå¤±è´¥")

            except Exception as e:
                logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_name} - {e}")

                # V12: ä¼ å…¥ runtime_ctx ç”¨äºå›æº¯â†”ç»ˆæ­¢è”åŠ¨
                result_content, is_error, _ = await self._handle_tool_error_with_backtrack(
                    error=e,
                    tool_name=tool_name,
                    tool_input=tool_input,
                    state=state,
                    session_id=session_id,
                    llm=llm,
                    tool_executor=tool_executor,
                    context_engineering=context_engineering,
                    runtime_ctx=ctx,
                )

            state.record_tool_outcome(tool_name, not is_error)
            if is_error:
                _round_failures.append((tool_name, str(result_content)[:150]))

            # Immediate compression (non-stream path, same as stream)
            from core.context.compaction import compress_fresh_tool_result
            compressed_content = (
                result_content if _skip_compress
                else compress_fresh_tool_result(result_content)
                if isinstance(result_content, str) else result_content
            )
            tool_results.append(
                {
                    "type": "tool_result",
                    "tool_use_id": tool_id,
                    "content": compressed_content,
                    "is_error": is_error,
                }
            )

            # Record tool call signature for dedup detection
            ctx.record_tool_call(tool_name, tool_input)

        if tool_results:
            # Context Pollution æ¸…ç†ï¼ˆä¸ stream ç‰ˆæœ¬å¯¹é½ï¼‰
            cleaned_results = self._clean_backtrack_results(tool_results, state)
            append_user_message(llm_messages, cleaned_results)

        # æ¸è¿›å¼å¤±è´¥å¼•å¯¼ï¼ˆä¸ stream ç‰ˆæœ¬å¯¹é½ï¼‰
        if _round_failures:
            _progressive_hints = []
            for _tn, _err in _round_failures:
                _hint = self._build_progressive_hint(_tn, _err, state)
                if _hint:
                    _progressive_hints.append(_hint)
                    if state.get_tool_failure_streak(_tn) >= 3:
                        state.pruned_tools.add(_tn)
            if _progressive_hints:
                append_user_message(
                    llm_messages, "\n\n".join(_progressive_hints)
                )
                logger.info(
                    f"ğŸ“Š æ¸è¿›å¼å¤±è´¥å¼•å¯¼(non-stream): {len(_progressive_hints)} æ¡"
                    f" (pruned={state.pruned_tools or 'none'})"
                )

        # è½¨è¿¹å»é‡ï¼šå®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨è¿ç»­ N æ¬¡ â†’ æ³¨å…¥åæ€æç¤ºå¼•å¯¼ LLM æ¢æ€è·¯
        if ctx.detect_repeated_call(threshold=4):
            _dedup_hint = (
                "[ç³»ç»Ÿæç¤º] æ£€æµ‹åˆ°å®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨å·²è¿ç»­æ‰§è¡Œå¤šæ¬¡ï¼Œç»“æœä¸ä¼šæ”¹å˜ã€‚"
                "è¯·åœ¨ Thinking ä¸­åˆ†æåŸå› ï¼Œå°è¯•ä¸åŒçš„å‚æ•°ã€æ¢ä¸€ä¸ªå·¥å…·ã€æˆ–ç›´æ¥åŸºäºå·²æœ‰ä¿¡æ¯å›ç­”ç”¨æˆ·ã€‚"
            )
            append_user_message(llm_messages, _dedup_hint)
            logger.warning(
                f"ğŸ” è½¨è¿¹å»é‡: å®Œå…¨ç›¸åŒçš„å·¥å…·è°ƒç”¨è¿ç»­ "
                f"{ctx._consecutive_duplicate_count + 1} æ¬¡ï¼Œæ³¨å…¥åæ€æç¤º"
            )

        # HITL pending æ£€æµ‹ï¼ˆnon-stream ç‰ˆæœ¬ï¼Œé€»è¾‘ä¸ stream ç‰ˆæœ¬ä¸€è‡´ï¼‰
        for _tr in tool_results:
            _tr_content = _tr.get("content", "")
            if isinstance(_tr_content, str) and "pending_user_input" in _tr_content:
                ctx.stop_reason = "hitl_pending"
                logger.info("HITL pending æ£€æµ‹ï¼šå·¥å…·è¿”å› pending_user_inputï¼Œæš‚åœæ‰§è¡Œç­‰å¾…ç”¨æˆ·å“åº”")
                break

        # _hint å¼ºåˆ¶æ³¨å…¥ï¼ˆnon-stream ç‰ˆæœ¬ï¼Œé€»è¾‘ä¸ stream ç‰ˆæœ¬ä¸€è‡´ï¼‰
        _injected_hints = _extract_tool_hints(tool_results)
        if _injected_hints:
            _hint_msg = "[ç³»ç»Ÿæç¤º] " + " ".join(_injected_hints)
            append_user_message(llm_messages, _hint_msg)
            logger.info(f"ğŸ”” _hint å¼ºåˆ¶æ³¨å…¥ï¼ˆnon-streamï¼‰: {_hint_msg[:120]}...")

        # æ›´æ–°è¿ç»­å¤±è´¥è®¡æ•°ï¼ˆä¾›ç»ˆæ­¢ç­–ç•¥ä¸è‡ªåŠ¨å›æ»šä½¿ç”¨ï¼‰
        if any(r.get("is_error") for r in tool_results):
            ctx.consecutive_failures += 1
        else:
            ctx.consecutive_failures = 0
        ctx.touch_activity()  # å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ›´æ–°æ´»åŠ¨æ—¶é—´ï¼ˆidle_timeout æ£€æµ‹ï¼‰


def create_rvrb_executor(
    config: Optional[ExecutorConfig] = None, max_backtracks: int = 3
) -> RVRBExecutor:
    """
    åˆ›å»º RVR-B æ‰§è¡Œå™¨

    Args:
        config: æ‰§è¡Œé…ç½®
        max_backtracks: æœ€å¤§å›æº¯æ¬¡æ•°

    Returns:
        RVRBExecutor å®ä¾‹
    """
    cfg = config or ExecutorConfig()
    cfg.enable_backtrack = True
    cfg.max_backtrack_attempts = max_backtracks
    return RVRBExecutor(config=cfg)
