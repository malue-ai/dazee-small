"""
RVRExecutor - RVR æ‰§è¡Œç­–ç•¥

å®ç° React-Validate-Reflect å¾ªç¯ã€‚

V10.1 P1 è§£è€¦ï¼š
- æ‰€æœ‰æ–¹æ³•è¿ç§»åˆ° Executor ä¸­
- ä¸å†ä¾èµ– agent å¯¹è±¡
- ä» ExecutionContext è·å–æ‰€æœ‰ä¾èµ–

èŒè´£ï¼š
- æ ‡å‡† RVR ä¸»å¾ªç¯
- æµå¼ LLM å“åº”å¤„ç†
- å·¥å…·è°ƒç”¨å¤„ç†
- æ¶ˆæ¯æ„å»ºå’Œæ›´æ–°
- ä¸Šä¸‹æ–‡é•¿åº¦ç®¡ç†ï¼ˆToken è£å‰ªï¼‰

æ³¨æ„ï¼š
- æœ¬æ¨¡å—å®ç°æ ‡å‡† RVR å¾ªç¯
- å¦‚éœ€å›æº¯èƒ½åŠ›ï¼Œè¯·ä½¿ç”¨ RVRBExecutor
"""

from typing import TYPE_CHECKING, Any, AsyncGenerator, Dict, List, Optional

from core.agent.errors import create_fallback_tool_result, create_timeout_tool_results
from core.agent.execution.protocol import (
    BaseExecutor,
    ExecutionContext,
    ExecutorConfig,
)
from core.context import stable_json_dumps
from core.context.compaction import trim_by_token_budget
from core.llm.base import count_request_tokens
from logger import get_logger
from utils.message_utils import (
    append_assistant_message,
    append_user_message,
    dict_list_to_messages,
    messages_to_dict_list,
)

if TYPE_CHECKING:
    from core.context.runtime import RuntimeContext
    from core.events.broadcaster import EventBroadcaster
    from core.llm.base import BaseLLMService, LLMResponse
    from core.routing.types import IntentResult
    from core.tool.executor import ToolExecutor
    from models.usage import UsageTracker

logger = get_logger(__name__)

# ==================== å›¾ç‰‡å‰¥ç¦»è¾…åŠ©å‡½æ•° ====================

IMAGE_PLACEHOLDER = "[æˆªå›¾å·²çœç•¥ï¼Œä»…ä¿ç•™æœ€è¿‘è½®æ¬¡çš„æˆªå›¾]"


def _content_has_image(content) -> bool:
    """é€’å½’æ£€æŸ¥ content æ˜¯å¦åŒ…å« image block"""
    if isinstance(content, list):
        return any(
            (isinstance(b, dict) and b.get("type") == "image")
            or (isinstance(b, dict) and b.get("type") == "tool_result"
                and _content_has_image(b.get("content")))
            for b in content
        )
    if isinstance(content, dict):
        return content.get("type") == "image"
    return False


def _strip_images_from_blocks(blocks: list) -> list:
    """
    é€’å½’æ›¿æ¢ content blocks ä¸­çš„ image block ä¸ºæ–‡æœ¬å ä½ç¬¦

    ä¿ç•™ text block å’Œ tool_use/tool_result çš„ç»“æ„ï¼Œä»…æ›¿æ¢ imageã€‚
    """
    result = []
    for block in blocks:
        if not isinstance(block, dict):
            result.append(block)
            continue

        block_type = block.get("type", "")

        if block_type == "image":
            # æ›¿æ¢ base64 å›¾ç‰‡ä¸ºæ–‡æœ¬å ä½ç¬¦
            result.append({"type": "text", "text": IMAGE_PLACEHOLDER})
        elif block_type == "tool_result":
            inner = block.get("content")
            if isinstance(inner, list) and _content_has_image(inner):
                result.append({
                    **block,
                    "content": _strip_images_from_blocks(inner),
                })
            else:
                result.append(block)
        else:
            result.append(block)

    return result


class RVRExecutor(BaseExecutor):
    """
    RVR æ‰§è¡Œå™¨ï¼ˆV10.1 è§£è€¦ç‰ˆï¼‰

    å®ç°æ ‡å‡†çš„ React-Validate-Reflect-Repeat å¾ªç¯ã€‚
    æ‰€æœ‰æ–¹æ³•éƒ½åœ¨ Executor å†…éƒ¨å®ç°ï¼Œä¸ä¾èµ–å¤–éƒ¨ Agentã€‚

    ä½¿ç”¨æ–¹å¼ï¼š
        executor = RVRExecutor()

        async for event in executor.execute(
            messages=messages,
            context=ExecutionContext(
                llm=llm,
                session_id=session_id,
                tool_executor=tool_executor,
                broadcaster=broadcaster,
                ...
            )
        ):
            yield event
    """

    @property
    def name(self) -> str:
        return "RVRExecutor"

    def supports_backtrack(self) -> bool:
        return False

    # ==================== å·¥å…·æ–¹æ³• ====================

    def _extract_system_prompt_text(self, system_prompt) -> str:
        """
        ä» system_prompt ä¸­æå–çº¯æ–‡æœ¬ï¼ˆæ”¯æŒ string å’Œ list æ ¼å¼ï¼‰
        """
        if isinstance(system_prompt, list):
            parts = []
            for block in system_prompt:
                if isinstance(block, dict) and block.get("type") == "text":
                    parts.append(block.get("text", ""))
                elif isinstance(block, str):
                    parts.append(block)
            return "".join(parts)
        return system_prompt or ""

    @staticmethod
    def _strip_old_images(
        messages: List[Dict], preserve_last_n: int = 2
    ) -> List[Dict]:
        """
        å‰¥ç¦»éæœ€è¿‘ N æ¡æ¶ˆæ¯ä¸­çš„ base64 å›¾ç‰‡æ•°æ®

        observe_screen ç­‰å·¥å…·è¿”å›çš„æˆªå›¾ä»¥ base64 åµŒå…¥ tool_resultï¼Œ
        æ¯å¼ å›¾ç‰‡ ~0.6MBï¼Œä¼šå¿«é€Ÿè€—å°½ 200K token ä¸Šä¸‹æ–‡çª—å£ã€‚
        å°†æ—§æ¶ˆæ¯ä¸­çš„å›¾ç‰‡æ›¿æ¢ä¸ºæ–‡æœ¬å ä½ç¬¦ï¼Œä¿ç•™æœ€è¿‘æ¶ˆæ¯çš„å›¾ç‰‡ã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆdict æ ¼å¼ï¼‰
            preserve_last_n: ä¿ç•™æœ€è¿‘ N æ¡æ¶ˆæ¯çš„å›¾ç‰‡ä¸å‰¥ç¦»

        Returns:
            å¤„ç†åçš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæµ…æ‹·è´ï¼Œä»…ä¿®æ”¹å«å›¾ç‰‡çš„æ¶ˆæ¯ï¼‰
        """
        if not messages:
            return messages

        stripped_count = 0
        result = []

        # ä¿ç•™æœ€å N æ¡æ¶ˆæ¯çš„å›¾ç‰‡
        strip_boundary = len(messages) - preserve_last_n

        for i, msg in enumerate(messages):
            if i >= strip_boundary:
                result.append(msg)
                continue

            content = msg.get("content")
            if not isinstance(content, list):
                result.append(msg)
                continue

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ block
            has_image = any(
                isinstance(block, dict) and block.get("type") == "image"
                for block in content
            )

            if not has_image:
                # é€’å½’æ£€æŸ¥ tool_result å†…éƒ¨
                has_image = any(
                    isinstance(block, dict)
                    and block.get("type") == "tool_result"
                    and _content_has_image(block.get("content"))
                    for block in content
                )

            if not has_image:
                result.append(msg)
                continue

            # æ›¿æ¢å›¾ç‰‡ä¸ºå ä½ç¬¦
            new_content = _strip_images_from_blocks(content)
            stripped_count += 1
            result.append({**msg, "content": new_content})

        if stripped_count > 0:
            logger.info(f"ğŸ–¼ï¸ å·²å‰¥ç¦» {stripped_count} æ¡æ¶ˆæ¯ä¸­çš„ base64 å›¾ç‰‡")

        return result

    def _trim_messages_if_needed(
        self,
        llm_messages: List,
        system_prompt_text: str,
        safe_threshold: int,
        context_strategy,
        turn: int = 0,
        tools_for_llm: List[Dict] = None,  # å·¥å…·å®šä¹‰ï¼Œç”¨äºæ›´å‡†ç¡®çš„ token ä¼°ç®—
    ) -> List:
        """
        å¦‚æœæ¶ˆæ¯è¶…è¿‡å®‰å…¨é˜ˆå€¼ï¼Œæ‰§è¡Œè£å‰ª

        å§‹ç»ˆå‰¥ç¦»æ—§æ¶ˆæ¯ä¸­çš„ base64 å›¾ç‰‡ï¼Œé¿å…æˆªå›¾ç´¯ç§¯æ’‘çˆ†ä¸Šä¸‹æ–‡çª—å£ã€‚
        """
        from core.context.compaction import fast_prefilter_messages

        messages_for_estimate = [
            {"role": m.role, "content": m.content} if hasattr(m, "role") else m
            for m in llm_messages
        ]

        # å§‹ç»ˆå‰¥ç¦»æ—§æ¶ˆæ¯ä¸­çš„ base64 å›¾ç‰‡ï¼ˆä¿ç•™æœ€è¿‘ 2 æ¡æ¶ˆæ¯çš„å›¾ç‰‡ï¼‰
        messages_for_estimate = self._strip_old_images(messages_for_estimate)

        # å¿«é€Ÿå­—ç¬¦çº§é¢„è¿‡æ»¤ï¼ˆ<1msï¼‰ï¼šåœ¨æ˜‚è´µçš„ token è®¡ç®—å‰æˆªæ–­è¶…å¤§æ¶ˆæ¯
        messages_for_estimate = fast_prefilter_messages(messages_for_estimate)

        # ä½¿ç”¨ç»Ÿä¸€çš„ token è®¡ç®—æ–¹æ³•ï¼ˆåŒ…å«å·¥å…·å®šä¹‰ï¼‰
        estimated_tokens = count_request_tokens(
            messages_for_estimate, system_prompt_text, tools_for_llm
        )

        if estimated_tokens <= safe_threshold:
            if turn == 0:
                logger.debug(
                    f"ğŸ“Š ä¸Šä¸‹æ–‡é•¿åº¦æ­£å¸¸: ä¼°ç®— {estimated_tokens:,} tokens < å®‰å…¨é˜ˆå€¼ {safe_threshold:,}"
                )
            # è¿”å›å‰¥ç¦»å›¾ç‰‡åçš„æ¶ˆæ¯ï¼ˆè€ŒéåŸå§‹æ¶ˆæ¯ï¼‰ï¼Œé˜²æ­¢æˆªå›¾ç´¯ç§¯è¶…é™
            return dict_list_to_messages(messages_for_estimate)

        preserve_first = (
            getattr(context_strategy, "preserve_first_messages", 4) if context_strategy else 4
        )
        preserve_last = (
            getattr(context_strategy, "preserve_last_messages", 8) if context_strategy else 8
        )
        preserve_tool_results = (
            getattr(context_strategy, "preserve_tool_results", True) if context_strategy else True
        )

        logger.warning(
            f"âš ï¸ Turn {turn + 1}: ä¸Šä¸‹æ–‡é•¿åº¦è­¦å‘Š: ä¼°ç®— {estimated_tokens:,} tokens > å®‰å…¨é˜ˆå€¼ {safe_threshold:,}"
        )

        trimmed_messages, trim_stats = trim_by_token_budget(
            messages=messages_for_estimate,
            token_budget=safe_threshold,
            preserve_first_messages=preserve_first,
            preserve_last_messages=preserve_last,
            preserve_tool_results=preserve_tool_results,
            system_prompt=system_prompt_text,
        )
        trimmed_tokens = trim_stats.estimated_tokens

        logger.info(
            f"âœ‚ï¸ å†å²æ¶ˆæ¯å·²è£å‰ª: {len(messages_for_estimate)} â†’ {len(trimmed_messages)} æ¡æ¶ˆæ¯, "
            f"token ä¼°ç®—: {estimated_tokens:,} â†’ {trimmed_tokens:,}"
        )

        if trimmed_tokens > safe_threshold:
            logger.warning(f"âš ï¸ è£å‰ªåä»è¶…è¿‡é˜ˆå€¼ï¼Œè¿›è¡Œæ¿€è¿›è£å‰ª...")

            aggressive_budget = int(safe_threshold * 0.6)
            aggressively_trimmed, aggressive_stats = trim_by_token_budget(
                messages=trimmed_messages,
                token_budget=aggressive_budget,
                preserve_first_messages=2,
                preserve_last_messages=6,
                preserve_tool_results=False,
                system_prompt=system_prompt_text,
            )
            aggressive_tokens = aggressive_stats.estimated_tokens

            logger.info(
                f"âœ‚ï¸ æ¿€è¿›è£å‰ª: {len(trimmed_messages)} â†’ {len(aggressively_trimmed)} æ¡æ¶ˆæ¯, "
                f"token ä¼°ç®—: {trimmed_tokens:,} â†’ {aggressive_tokens:,}"
            )

            return dict_list_to_messages(aggressively_trimmed)

        return dict_list_to_messages(trimmed_messages)

    # ==================== LLM æµå¼å¤„ç† ====================

    async def _process_stream(
        self,
        llm: "BaseLLMService",
        messages: List,
        system_prompt,
        tools: List,
        ctx: "RuntimeContext",
        session_id: str,
        broadcaster: "EventBroadcaster",
        usage_tracker: "UsageTracker",
        is_first_turn: bool = False,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å¤„ç† LLM å“åº”ï¼ˆä» Agent è¿ç§»ï¼‰

        Args:
            llm: LLM æœåŠ¡
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·å®šä¹‰
            ctx: RuntimeContext
            session_id: Session ID
            broadcaster: äº‹ä»¶å¹¿æ’­å™¨
            usage_tracker: ä½¿ç”¨è·Ÿè¸ªå™¨
            is_first_turn: æ˜¯å¦é¦–è½®

        Yields:
            SSE äº‹ä»¶
        """
        from core.agent.content_handler import create_content_handler

        # åˆ›å»º ContentHandlerï¼ˆä¼ å…¥ session_id ç”¨äºå¿«æ·æ–¹æ³•ï¼‰
        content_handler = create_content_handler(broadcaster, ctx.block, session_id=session_id)

        final_response = None

        try:
            # è°ƒç”¨ LLM Stream
            async for response in llm.create_message_stream(
                messages=messages, system=system_prompt, tools=tools
            ):
                # LLMResponse å¯¹è±¡ â†’ æ ¹æ®å­—æ®µåˆ¤æ–­äº‹ä»¶ç±»å‹
                # æ³¨æ„ï¼šåªå¤„ç†æµå¼å¢é‡ï¼ˆis_stream=Trueï¼‰ï¼Œè·³è¿‡æœ€ç»ˆæ±‡æ€»å“åº”ï¼ˆis_stream=Falseï¼‰

                if response.thinking and response.is_stream:
                    # æ€è€ƒè¿‡ç¨‹ï¼ˆæµå¼å¢é‡ï¼‰
                    await content_handler.handle_thinking(response.thinking)
                    yield {"type": "thinking_delta", "data": {"thinking": response.thinking}}

                if response.content and response.is_stream:
                    # å†…å®¹å¢é‡ï¼ˆæµå¼å¢é‡ï¼‰
                    await content_handler.handle_text(response.content)
                    yield {"type": "content_delta", "data": {"text": response.content}}

                if response.tool_use_start:
                    # å·¥å…·è°ƒç”¨å¼€å§‹
                    await content_handler.handle_tool_use_start(
                        tool_id=response.tool_use_start.get("id"),
                        tool_name=response.tool_use_start.get("name"),
                    )
                    yield {"type": "tool_use_start", "data": response.tool_use_start}

                if response.input_delta:
                    # å·¥å…·è¾“å…¥å¢é‡
                    await content_handler.handle_tool_input_delta(response.input_delta)
                    yield {"type": "input_delta", "data": {"input": response.input_delta}}

                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆå“åº”ï¼ˆæœ‰ tool_calls æˆ– stop_reasonï¼Œæˆ–è€…æ˜¯éæµå¼æ±‡æ€»ï¼‰
                if (
                    response.tool_calls
                    or response.stop_reason != "end_turn"
                    or not response.is_stream
                ):
                    final_response = response

        finally:
            # å…³é—­æœ€åä¸€ä¸ª block
            await content_handler.stop_block(session_id)

        # ä¿å­˜æœ€ç»ˆå“åº”
        if final_response:
            ctx.last_llm_response = final_response
            ctx.touch_activity()  # æ›´æ–°æ´»åŠ¨æ—¶é—´ï¼ˆidle_timeout æ£€æµ‹ï¼‰
            # ç´¯ç§¯ usage
            usage_tracker.accumulate(final_response)

    # ==================== å·¥å…·å¤„ç† ====================

    async def _handle_tool_calls(
        self,
        response: "LLMResponse",
        llm_messages: List,
        session_id: str,
        conversation_id: str,
        ctx: "RuntimeContext",
        tool_executor: "ToolExecutor",
        broadcaster: "EventBroadcaster",
        context_engineering=None,
        plan_cache: Dict = None,
        plan_todo_tool=None,
        event_manager=None,
        state_manager=None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆæµå¼ï¼ŒV10.2 ä½¿ç”¨ ToolExecutionFlowï¼‰

        Args:
            response: LLM å“åº”
            llm_messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆä¼šè¢«ä¿®æ”¹ï¼‰
            session_id: Session ID
            conversation_id: Conversation ID
            ctx: RuntimeContext
            tool_executor: å·¥å…·æ‰§è¡Œå™¨
            broadcaster: äº‹ä»¶å¹¿æ’­å™¨
            context_engineering: ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ˆå¯é€‰ï¼‰
            plan_cache: Plan ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
            plan_todo_tool: Plan å·¥å…·ï¼ˆå¯é€‰ï¼‰
            event_manager: äº‹ä»¶ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            state_manager: çŠ¶æ€ä¸€è‡´æ€§ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼ŒV11ï¼‰

        Yields:
            SSE äº‹ä»¶
        """
        from core.agent.content_handler import create_content_handler
        from core.agent.tools.flow import (
            ToolExecutionContext,
            ToolExecutionFlow,
            create_tool_execution_flow,
        )

        tool_calls = [tc for tc in response.tool_calls if tc.get("type") == "tool_use"]

        if not tool_calls:
            return

        # åˆ›å»º ToolExecutionContext
        tool_context = ToolExecutionContext(
            session_id=session_id,
            conversation_id=conversation_id,
            tool_executor=tool_executor,
            broadcaster=broadcaster,
            event_manager=event_manager,
            context_engineering=context_engineering,
            plan_cache=plan_cache or {},
            plan_todo_tool=plan_todo_tool,
            state_manager=state_manager,
        )

        # åˆ›å»º ToolExecutionFlowï¼ˆå¸¦ç‰¹æ®Šå¤„ç†å™¨ï¼‰
        flow = create_tool_execution_flow()
        content_handler = create_content_handler(broadcaster, ctx.block, session_id=session_id)

        # ä½¿ç”¨ ToolExecutionFlow æ‰§è¡Œ
        tool_results = []
        async for event in flow.execute_stream(tool_calls, tool_context, content_handler):
            # è·³è¿‡ç©ºäº‹ä»¶
            if event is None:
                continue

            yield event

            # ä»äº‹ä»¶ä¸­æå–ç»“æœç”¨äºæ¶ˆæ¯æ„å»º
            if event.get("type") == "tool_result" or (
                isinstance(event.get("content"), dict) and "tool_use_id" in event.get("content", {})
            ):
                content = event.get("content", {})
                if content:
                    tool_results.append(
                        {
                            "type": "tool_result",
                            "tool_use_id": content.get("tool_use_id"),
                            "content": content.get("content", ""),
                            "is_error": content.get("is_error", False),
                        }
                    )

        # å¦‚æœæ²¡æœ‰ä»äº‹ä»¶æå–åˆ°ç»“æœï¼Œä½¿ç”¨åŒæ­¥æ‰§è¡Œç»“æœ
        if not tool_results:
            results = await flow.execute(tool_calls, tool_context)
            for tool_id, result_info in results.items():
                result = result_info.result
                result_content = result if isinstance(result, str) else stable_json_dumps(result)
                tool_results.append(
                    {
                        "type": "tool_result",
                        "tool_use_id": tool_id,
                        "content": result_content,
                        "is_error": result_info.is_error,
                    }
                )

        # æ›´æ–°æ¶ˆæ¯å†å²
        # æ·»åŠ  assistant æ¶ˆæ¯ï¼ˆåŒ…å« tool_useï¼‰
        assistant_content = (
            response.raw_content_blocks if hasattr(response, "raw_content_blocks") else []
        )
        if not assistant_content and response.tool_calls:
            # æ„å»º content blocks
            assistant_content = []
            if response.content:
                assistant_content.append({"type": "text", "text": response.content})
            for tc in response.tool_calls:
                if tc.get("type") == "tool_use":
                    assistant_content.append(
                        {
                            "type": "tool_use",
                            "id": tc["id"],
                            "name": tc["name"],
                            "input": tc.get("input", {}),
                        }
                    )

        append_assistant_message(llm_messages, assistant_content)

        # æ·»åŠ  user æ¶ˆæ¯ï¼ˆåŒ…å« tool_resultï¼‰
        append_user_message(llm_messages, tool_results)

        # æ›´æ–°è¿ç»­å¤±è´¥è®¡æ•°ï¼ˆä¾›ç»ˆæ­¢ç­–ç•¥ä¸è‡ªåŠ¨å›æ»šä½¿ç”¨ï¼‰
        if any(r.get("is_error") for r in tool_results):
            ctx.consecutive_failures += 1
        else:
            ctx.consecutive_failures = 0
        ctx.touch_activity()  # å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ›´æ–°æ´»åŠ¨æ—¶é—´ï¼ˆidle_timeout æ£€æµ‹ï¼‰

    async def _handle_last_turn_tools(
        self,
        response: "LLMResponse",
        llm_messages: List,
        system_prompt,
        ctx: "RuntimeContext",
        session_id: str,
        conversation_id: str,
        llm: "BaseLLMService",
        tool_executor: "ToolExecutor",
        broadcaster: "EventBroadcaster",
        usage_tracker: "UsageTracker",
        context_engineering=None,
        plan_cache: Dict = None,
        plan_todo_tool=None,
        event_manager=None,
        state_manager=None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†æœ€åä¸€è½®çš„å·¥å…·è°ƒç”¨

        åœ¨æœ€åä¸€è½®ï¼Œæ‰§è¡Œå·¥å…·åéœ€è¦å†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚
        """
        # å…ˆæ‰§è¡Œå·¥å…·ï¼ˆV10.2: ä½¿ç”¨ ToolExecutionFlowï¼‰
        async for event in self._handle_tool_calls(
            response,
            llm_messages,
            session_id,
            conversation_id,
            ctx,
            tool_executor,
            broadcaster,
            context_engineering=context_engineering,
            plan_cache=plan_cache,
            plan_todo_tool=plan_todo_tool,
            event_manager=event_manager,
            state_manager=state_manager,
        ):
            yield event

        # å†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå“åº”
        logger.info("ğŸ”„ æœ€åä¸€è½®å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆå“åº”...")

        async for event in self._process_stream(
            llm=llm,
            messages=llm_messages,
            system_prompt=system_prompt,
            tools=[],  # æœ€åä¸€è½®ä¸æä¾›å·¥å…·
            ctx=ctx,
            session_id=session_id,
            broadcaster=broadcaster,
            usage_tracker=usage_tracker,
            is_first_turn=False,
        ):
            yield event

        final_response = ctx.last_llm_response
        if final_response:
            ctx.set_completed(final_response.content, final_response.stop_reason)

    # ==================== ä¸»æ‰§è¡Œå¾ªç¯ ====================

    async def execute(
        self,
        messages: List[Dict[str, Any]],
        context: ExecutionContext,
        config: Optional[ExecutorConfig] = None,
        **kwargs,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æ‰§è¡Œ RVR ä¸»å¾ªç¯ï¼ˆV10.1 è§£è€¦ç‰ˆï¼‰

        Args:
            messages: åˆå§‹æ¶ˆæ¯åˆ—è¡¨ï¼ˆdict æ ¼å¼ï¼‰
            context: æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆåŒ…å«æ‰€æœ‰ä¾èµ–ï¼‰
            config: æ‰§è¡Œé…ç½®
            **kwargs: é¢å¤–å‚æ•°

        Yields:
            äº‹ä»¶å­—å…¸
        """
        cfg = config or self.config

        # ä» context è·å–ä¾èµ–ï¼ˆV10.1: ç›´æ¥ä» context è·å–ï¼Œä¸ä¾èµ– agentï¼‰
        llm = context.llm
        tool_executor = context.tool_executor
        broadcaster = context.broadcaster
        ctx = context.runtime_ctx
        session_id = context.session_id
        conversation_id = context.conversation_id
        system_prompt = context.system_prompt
        tools_for_llm = context.tools_for_llm
        intent = context.intent
        plan_cache = context.plan_cache

        # éªŒè¯å¿…éœ€ä¾èµ–
        if not llm:
            logger.error("âŒ RVRExecutor: llm æœªæä¾›")
            yield {"type": "error", "data": {"message": "æ‰§è¡Œå™¨é…ç½®é”™è¯¯: llm æœªæä¾›"}}
            return

        if not ctx:
            logger.error("âŒ RVRExecutor: runtime_ctx æœªæä¾›")
            yield {"type": "error", "data": {"message": "æ‰§è¡Œå™¨é…ç½®é”™è¯¯: runtime_ctx æœªæä¾›"}}
            return

        # è·å–é¢å¤–ä¾èµ–ï¼ˆä» context.extraï¼‰
        usage_tracker = context.extra.get("usage_tracker")
        if not usage_tracker:
            from models.usage import UsageTracker

            usage_tracker = UsageTracker()

        context_engineering = context.extra.get("context_engineering")
        plan_todo_tool = context.extra.get("plan_todo_tool")
        event_manager = context.extra.get("event_manager")
        state_manager = context.extra.get("state_manager")

        logger.info(f"ğŸš€ RVRExecutor å¼€å§‹æ‰§è¡Œ (signal-driven termination)")

        # è½¬æ¢æ¶ˆæ¯ä¸º Message å¯¹è±¡
        llm_messages = dict_list_to_messages(messages)

        # Context Engineering: Todo é‡å†™
        def _refresh_plan_injection(_llm_messages: List, *, inject_errors: bool) -> List:
            if not context_engineering or not plan_cache.get("plan"):
                return _llm_messages
            prepared_messages = context_engineering.prepare_messages_for_llm(
                messages=messages_to_dict_list(_llm_messages),
                plan=plan_cache.get("plan"),
                inject_plan=True,
                inject_errors=inject_errors,
            )
            return dict_list_to_messages(prepared_messages)

        # æå– system_prompt æ–‡æœ¬å¹¶è®¡ç®—å®‰å…¨é˜ˆå€¼
        system_prompt_text = self._extract_system_prompt_text(system_prompt)
        token_budget = (
            getattr(context.context_strategy, "token_budget", 180000)
            if context.context_strategy
            else 180000
        )
        safe_threshold = token_budget - 10000

        # è¿›å…¥å¾ªç¯å‰æ£€æŸ¥å¹¶è£å‰ªä¸Šä¸‹æ–‡
        llm_messages = self._trim_messages_if_needed(
            llm_messages, system_prompt_text, safe_threshold, context.context_strategy, turn=0
        )

        turn = 0
        while True:
            # æ¯è½®è°ƒç”¨ LLM å‰åˆ·æ–° Plan æ³¨å…¥ï¼ˆPlan å¯èƒ½åœ¨ä¸Šä¸€è½®å·¥å…·è°ƒç”¨ä¸­è¢«æ›´æ–°ï¼‰
            llm_messages = _refresh_plan_injection(llm_messages, inject_errors=(turn == 0))

            # æ¯è½®å¼€å§‹æ—¶æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦
            if turn > 0:
                llm_messages = self._trim_messages_if_needed(
                    llm_messages,
                    system_prompt_text,
                    safe_threshold,
                    context.context_strategy,
                    turn=turn,
                )

            ctx.next_turn()
            ctx.touch_activity()  # æ›´æ–°æ´»åŠ¨æ—¶é—´ï¼ˆç”¨äº idle_timeout æ£€æµ‹ï¼‰
            logger.info(f"{'='*60}")
            logger.info(f"ğŸ”„ Turn {turn + 1}")
            logger.info(f"{'='*60}")

            if cfg.enable_stream:
                # æµå¼å¤„ç†
                async for event in self._process_stream(
                    llm=llm,
                    messages=llm_messages,
                    system_prompt=system_prompt,
                    tools=tools_for_llm,
                    ctx=ctx,
                    session_id=session_id,
                    broadcaster=broadcaster,
                    usage_tracker=usage_tracker,
                    is_first_turn=(turn == 0),
                ):
                    yield event

                response = ctx.last_llm_response
                if response:
                    # é˜¶æ®µ 5 éªŒè¯ï¼šæ£€æŸ¥å¤æ‚ä»»åŠ¡æ˜¯å¦åˆ›å»º Plan
                    if turn == 0 and intent and intent.needs_plan and response.tool_calls:
                        self._validate_plan_creation(
                            response.tool_calls, context.extra.get("tracer")
                        )

                    # å¤„ç†å·¥å…·è°ƒç”¨
                    if response.stop_reason == "tool_use" and response.tool_calls:
                        # V11.1: HITL å±é™©æ“ä½œç¡®è®¤ï¼ˆæ‰§è¡Œå‰æ‹¦æˆªï¼Œç­‰å¾…ç”¨æˆ·å†³ç­–ï¼‰
                        hitl_rejected = False
                        if cfg.terminator:
                            try:
                                pending_names = [
                                    t.get("name") for t in response.tool_calls if t.get("name")
                                ]
                                from core.termination.protocol import TerminationAction

                                hitl_decision = cfg.terminator.evaluate(
                                    ctx,
                                    last_stop_reason="tool_use",
                                    pending_tool_names=pending_names,
                                )
                                if (
                                    hitl_decision.action == TerminationAction.ASK_USER
                                    and "hitl_confirm" in (hitl_decision.reason or "")
                                ):
                                    # é€šçŸ¥å‰ç«¯æ˜¾ç¤ºç¡®è®¤å¼¹çª—
                                    yield {
                                        "type": "hitl_confirm",
                                        "data": {
                                            "reason": hitl_decision.reason,
                                            "tools": pending_names,
                                            "message": "å±é™©æ“ä½œéœ€ç”¨æˆ·ç¡®è®¤",
                                        },
                                    }

                                    # ç­‰å¾…ç”¨æˆ·å†³ç­–ï¼ˆapprove / rejectï¼‰
                                    wait_fn = (context.extra or {}).get(
                                        "wait_hitl_confirm_async"
                                    )
                                    if callable(wait_fn):
                                        user_choice = await wait_fn()
                                        if user_choice == "approve":
                                            logger.info(
                                                f"HITL å·²æ‰¹å‡†: {pending_names}"
                                            )
                                            # ç”¨æˆ·æ‰¹å‡† â†’ ç»§ç»­æ‰§è¡Œå·¥å…·
                                        else:
                                            # ç”¨æˆ·æ‹’ç» â†’ æ‰§è¡Œ on_rejection ç­–ç•¥
                                            logger.info(
                                                f"HITL å·²æ‹’ç»: {pending_names}ï¼Œ"
                                                f"æ‰§è¡Œå›é€€ç­–ç•¥"
                                            )
                                            hitl_rejected = True
                                            async for evt in self._handle_hitl_rejection(
                                                context, ctx, cfg
                                            ):
                                                yield evt
                                            break
                                    else:
                                        # æ— ç­‰å¾…å‡½æ•°ï¼Œä¿å®ˆåœæ­¢ï¼ˆä¸æ‰§è¡Œå±é™©æ“ä½œï¼‰
                                        logger.warning(
                                            "HITL ç¡®è®¤: æ—  wait å‡½æ•°ï¼Œ"
                                            "ä¿å®ˆåœæ­¢ï¼ˆä¸æ‰§è¡Œå±é™©æ“ä½œï¼‰"
                                        )
                                        ctx.stop_reason = (
                                            hitl_decision.reason or "hitl_confirm"
                                        )
                                        break
                            except Exception as e:
                                logger.warning(
                                    f"HITL æ£€æŸ¥å¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ: {e}",
                                    exc_info=True,
                                )

                        if hitl_rejected:
                            break

                        # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆV10.2: ä½¿ç”¨ ToolExecutionFlowï¼‰
                        async for event in self._handle_tool_calls(
                            response,
                            llm_messages,
                            session_id,
                            conversation_id,
                            ctx,
                            tool_executor,
                            broadcaster,
                            context_engineering=context_engineering,
                            plan_cache=plan_cache,
                            plan_todo_tool=plan_todo_tool,
                            event_manager=event_manager,
                            state_manager=state_manager,
                        ):
                            yield event
                    else:
                        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä»»åŠ¡å®Œæˆ
                        ctx.set_completed(response.content, response.stop_reason)
                        break
            else:
                # éæµå¼å¤„ç†
                response = await llm.create_message_async(
                    messages=llm_messages, system=system_prompt, tools=tools_for_llm
                )

                usage_tracker.accumulate(response)

                if response.content:
                    yield {"type": "content", "data": {"text": response.content}}

                if response.stop_reason != "tool_use":
                    ctx.set_completed(response.content, response.stop_reason)
                    break

                # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆéæµå¼ï¼‰
                async for _ in self._handle_tool_calls(
                    response,
                    llm_messages,
                    session_id,
                    conversation_id,
                    ctx,
                    tool_executor,
                    broadcaster,
                    context_engineering=context_engineering,
                    plan_cache=plan_cache,
                    plan_todo_tool=plan_todo_tool,
                    event_manager=event_manager,
                    state_manager=state_manager,
                ):
                    pass  # éæµå¼ä¸ yield äº‹ä»¶

            turn += 1

            if ctx.is_completed():
                break

            # ç»ˆæ­¢ç­–ç•¥ï¼šå®Œå…¨ç”± AdaptiveTerminator ä¿¡å·é©±åŠ¨ï¼ˆæ— ç¡¬æ€§ max_turnsï¼‰
            if cfg.terminator and not ctx.is_completed():
                try:
                    from core.termination.protocol import TerminationAction

                    last_reason = (
                        getattr(ctx.last_llm_response, "stop_reason", None)
                        if ctx.last_llm_response
                        else None
                    )
                    # V11: ä¼ å…¥ stop_requestedï¼ˆå¤–éƒ¨åœæ­¢ä¿¡å·ï¼‰
                    _stop_requested = (
                        context.stop_event.is_set() if context.stop_event else False
                    )
                    decision = cfg.terminator.evaluate(
                        ctx,
                        last_stop_reason=last_reason,
                        stop_requested=_stop_requested,
                        pending_tool_names=None,
                    )
                    if decision.should_stop:
                        ctx.stop_reason = decision.reason or "terminator"
                        # V11.1: è¿ç»­å¤±è´¥ â†’ æ¨é€å›æ»šé€‰é¡¹ï¼ˆäº‹ä»¶ç±»å‹å¯¹é½å‰ç«¯ï¼‰
                        if decision.action == TerminationAction.ROLLBACK_OPTIONS:
                            _state_mgr = (context.extra or {}).get("state_manager")
                            _options = (
                                _state_mgr.get_rollback_options(session_id)
                                if _state_mgr
                                else []
                            )
                            yield {
                                "type": "rollback_options",
                                "data": {
                                    "task_id": session_id,
                                    "options": _options,
                                    "reason": decision.reason,
                                },
                            }
                        break
                    # é•¿ä»»åŠ¡ç¡®è®¤ï¼šyield äº‹ä»¶åç­‰å¾…ç”¨æˆ·ç‚¹å‡»ã€Œç»§ç»­ã€
                    if (
                        decision.action == TerminationAction.ASK_USER
                        and decision.reason == "long_running_confirm"
                    ):
                        wait_fn = (context.extra or {}).get("wait_long_run_confirm_async")
                        if callable(wait_fn):
                            yield {
                                "type": "long_running_confirm",
                                "data": {
                                    "turn": ctx.current_turn,
                                    "message": f"ä»»åŠ¡å·²æ‰§è¡Œ {ctx.current_turn} è½®ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ",
                                },
                            }
                            await wait_fn()
                            cfg.terminator.confirm_long_running()
                except Exception as e:
                    logger.warning(
                        f"terminator.evaluate() å¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ: {e}",
                        exc_info=True,
                    )

        logger.info(f"âœ… RVRExecutor æ‰§è¡Œå®Œæˆ: turns={ctx.current_turn}")

    # ==================== HITL æ‹’ç»å¤„ç†ï¼ˆV11.1ï¼‰====================

    async def _handle_hitl_rejection(
        self,
        context: ExecutionContext,
        ctx: "RuntimeContext",
        cfg: ExecutorConfig,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†ç”¨æˆ·æ‹’ç» HITL ç¡®è®¤åçš„å›é€€ç­–ç•¥

        æ ¹æ® HITLConfig.on_rejection é…ç½®æ‰§è¡Œï¼š
        - "rollback": è‡ªåŠ¨å›æ»šåˆ°ä»»åŠ¡å¿«ç…§
        - "stop": ç›´æ¥åœæ­¢ï¼Œä¸å›æ»š
        - "ask_rollback": æ¨é€å›æ»šé€‰é¡¹ï¼Œè®©ç”¨æˆ·å†³å®šæ˜¯å¦å›æ»š

        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            ctx: è¿è¡Œæ—¶ä¸Šä¸‹æ–‡
            cfg: æ‰§è¡Œå™¨é…ç½®
        """
        session_id = context.session_id
        state_mgr = (context.extra or {}).get("state_manager")

        # è¯»å– on_rejection ç­–ç•¥
        on_rejection = "ask_rollback"  # é»˜è®¤ï¼šè¯¢é—®ç”¨æˆ·
        if (
            cfg.terminator
            and hasattr(cfg.terminator, "config")
            and hasattr(cfg.terminator.config, "hitl")
        ):
            on_rejection = getattr(
                cfg.terminator.config.hitl, "on_rejection", "ask_rollback"
            )

        logger.info(
            f"HITL æ‹’ç»å¤„ç†: on_rejection={on_rejection}, session={session_id}"
        )

        if on_rejection == "rollback" and state_mgr:
            # è‡ªåŠ¨å›æ»š
            snapshot_id = state_mgr.get_snapshot_for_task(session_id)
            if snapshot_id:
                rollback_msgs = state_mgr.rollback(snapshot_id)
                logger.info(f"HITL æ‹’ç» â†’ è‡ªåŠ¨å›æ»šå®Œæˆ: {rollback_msgs}")
                yield {
                    "type": "rollback_completed",
                    "data": {
                        "task_id": session_id,
                        "messages": rollback_msgs,
                        "trigger": "hitl_rejection",
                    },
                }
            else:
                logger.warning("HITL æ‹’ç» â†’ å›æ»šå¤±è´¥: æœªæ‰¾åˆ°å¿«ç…§")
            ctx.stop_reason = "hitl_rejected_rollback"

        elif on_rejection == "ask_rollback":
            # æ¨é€å›æ»šé€‰é¡¹ç»™å‰ç«¯ï¼Œè®©ç”¨æˆ·å†³å®š
            options = (
                state_mgr.get_rollback_options(session_id) if state_mgr else []
            )
            yield {
                "type": "rollback_options",
                "data": {
                    "task_id": session_id,
                    "options": options,
                    "reason": "ç”¨æˆ·æ‹’ç»å±é™©æ“ä½œ",
                },
            }
            ctx.stop_reason = "hitl_rejected_ask_rollback"

        else:
            # "stop" æˆ–æœªçŸ¥ç­–ç•¥ â†’ ç›´æ¥åœæ­¢
            ctx.stop_reason = "hitl_rejected_stop"

    def _validate_plan_creation(self, tool_calls: List[Dict], tracer=None) -> None:
        """
        éªŒè¯å¤æ‚ä»»åŠ¡æ˜¯å¦åœ¨ç¬¬ä¸€è½®åˆ›å»º Plan
        """
        first_tool_name = tool_calls[0].get("name", "")
        if first_tool_name == "plan":
            first_action = tool_calls[0].get("input", {}).get("action", "")
            if first_action == "create":
                logger.info("âœ… é˜¶æ®µ 5 éªŒè¯é€šè¿‡: å¤æ‚ä»»åŠ¡ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨æ˜¯ plan(action='create')")
            else:
                logger.warning(f"âš ï¸ é˜¶æ®µ 5 å¼‚å¸¸: plan action ä¸æ˜¯ createï¼Œå®é™…: {first_action}")
        else:
            logger.warning(f"âš ï¸ é˜¶æ®µ 5 å¼‚å¸¸: å¤æ‚ä»»åŠ¡æœªåˆ›å»º Planï¼ç¬¬ä¸€ä¸ªå·¥å…·: {first_tool_name}")
            if tracer:
                tracer.add_warning(f"Plan Creation è·³è¿‡: ç¬¬ä¸€ä¸ªå·¥å…·æ˜¯ {first_tool_name}")


def create_rvr_executor(config: Optional[ExecutorConfig] = None) -> RVRExecutor:
    """åˆ›å»º RVR æ‰§è¡Œå™¨"""
    return RVRExecutor(config=config)
