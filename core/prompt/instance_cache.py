"""
å®ä¾‹çº§æç¤ºè¯ç¼“å­˜ç®¡ç†å™¨ - InstancePromptCache

ğŸ†• V5.5: åœºæ™¯åŒ–æç¤ºè¯åˆ†è§£ + prompt_results å¯è§†åŒ–è¾“å‡º

è®¾è®¡åŸåˆ™ï¼š
1. å®ä¾‹å¯åŠ¨æ—¶ä¸€æ¬¡æ€§åŠ è½½ï¼Œå…¨å±€ç¼“å­˜
2. ç”¨ç©ºé—´æ¢æ—¶é—´ï¼Œé¿å…é‡å¤åˆ†æ
3. æ‰€æœ‰æç¤ºè¯ç‰ˆæœ¬å¯åŠ¨æ—¶ç”Ÿæˆï¼Œè¿è¡Œæ—¶ç›´æ¥å–ç¼“å­˜
4. ğŸ†• V5.0: æ”¯æŒæŒä¹…åŒ–åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œé¿å…é‡å¤ LLM åˆ†æ
5. ğŸ†• V5.5: è¾“å‡ºåˆ° prompt_results/ ç›®å½•ä¾›è¿è¥æŸ¥çœ‹å’Œç¼–è¾‘

æ•°æ®æµï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å¯åŠ¨é˜¶æ®µï¼ˆä¼˜å…ˆåŠ è½½ prompt_results/ï¼‰                            â”‚
â”‚ 1. æ£€æŸ¥ prompt_results/ æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ                         â”‚
â”‚ 2. æ£€æµ‹æºæ–‡ä»¶å˜åŒ–ï¼ˆprompt.md / config.yamlï¼‰                   â”‚
â”‚ 3. æ£€æµ‹è¿è¥æ‰‹åŠ¨ç¼–è¾‘ï¼ˆä¿æŠ¤æ‰‹åŠ¨ä¿®æ”¹çš„æ–‡ä»¶ï¼‰                        â”‚
â”‚ 4. éœ€è¦é‡æ–°ç”Ÿæˆæ—¶ï¼šLLM åˆ†è§£ä»»åŠ¡ â†’ å†™å…¥ prompt_results/          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¿è¡Œé˜¶æ®µï¼ˆæ¯æ¬¡è¯·æ±‚ï¼Œæ¯«ç§’çº§ï¼‰                                     â”‚
â”‚ 1. ç›´æ¥ä»å†…å­˜ç¼“å­˜è·å– intent_prompt                            â”‚
â”‚ 2. æ„å›¾è¯†åˆ« â†’ å¤æ‚åº¦                                          â”‚
â”‚ 3. ç›´æ¥ä»å†…å­˜ç¼“å­˜è·å–å¯¹åº”ç‰ˆæœ¬ system_prompt                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ–‡ä»¶ç»“æ„ï¼š
â”œâ”€â”€ .cache/                 # äºŒè¿›åˆ¶ç¼“å­˜ï¼ˆJSONï¼‰
â”‚   â”œâ”€â”€ prompt_cache.json
â”‚   â”œâ”€â”€ agent_schema.json
â”‚   â””â”€â”€ cache_meta.json
â”‚
â””â”€â”€ prompt_results/         # ğŸ†• è¿è¥å¯è§å¯ç¼–è¾‘
    â”œâ”€â”€ README.md           # ä½¿ç”¨è¯´æ˜
    â”œâ”€â”€ agent_schema.yaml   # AgentSchema
    â”œâ”€â”€ intent_prompt.md    # æ„å›¾è¯†åˆ«æç¤ºè¯
    â”œâ”€â”€ simple_prompt.md    # ç®€å•ä»»åŠ¡æç¤ºè¯
    â”œâ”€â”€ medium_prompt.md    # ä¸­ç­‰ä»»åŠ¡æç¤ºè¯
    â”œâ”€â”€ complex_prompt.md   # å¤æ‚ä»»åŠ¡æç¤ºè¯
    â””â”€â”€ _metadata.json      # å…ƒæ•°æ®
"""

# 1. æ ‡å‡†åº“
import asyncio
import hashlib
import json
import time
from abc import ABC, abstractmethod
from dataclasses import asdict, dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Coroutine, Dict, List, Optional, Protocol

# Type alias for progress callback: async def callback(step: int, message: str) -> None
ProgressCallback = Optional[Callable[[int, str], Coroutine[Any, Any, None]]]

# 2. ç¬¬ä¸‰æ–¹åº“
import aiofiles

# 3. æœ¬åœ°æ¨¡å—
# æ³¨æ„ï¼šä¸ºé¿å…å¾ªç¯å¯¼å…¥ï¼ŒAgentFactory å»¶è¿Ÿå¯¼å…¥
from config.llm_config import get_llm_profile
from core.llm import create_llm_service
from core.llm.base import Message
from core.prompt.framework_rules import (
    get_complex_prompt_template,
    get_intent_prompt_template,
    get_medium_prompt_template,
    get_merge_prompts,
    get_simple_prompt_template,
)
from core.prompt.intent_prompt_generator import IntentPromptGenerator
from core.prompt.prompt_layer import PromptParser, PromptSchema, TaskComplexity, generate_prompt
from core.prompt.prompt_results_writer import PromptResults, PromptResultsWriter
from core.schemas import DEFAULT_AGENT_SCHEMA, AgentSchema
from logger import get_logger
from prompts.intent_recognition_prompt import get_intent_recognition_prompt

logger = get_logger("instance_cache")


# ============================================================
# ç¼“å­˜å­˜å‚¨åç«¯æŠ½è±¡ï¼ˆé¢„ç•™äº‘ç«¯åŒæ­¥æ‰©å±•ç‚¹ï¼‰
# ============================================================


class CacheStorageBackend(ABC):
    """
    ç¼“å­˜å­˜å‚¨åç«¯æŠ½è±¡æ¥å£ï¼ˆå¼‚æ­¥ï¼‰

    ğŸ†• V5.0: é¢„ç•™äº‘ç«¯åŒæ­¥æ‰©å±•ç‚¹
    å½“å‰å®ç°ï¼šLocalFileBackend
    æœªæ¥æ‰©å±•ï¼šCloudSyncBackendï¼ˆS3/OSS/æ•°æ®åº“ï¼‰
    """

    @abstractmethod
    async def save(self, key: str, data: Dict[str, Any]) -> bool:
        """ä¿å­˜ç¼“å­˜æ•°æ®ï¼ˆå¼‚æ­¥ï¼‰"""
        pass

    @abstractmethod
    async def load(self, key: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½ç¼“å­˜æ•°æ®ï¼ˆå¼‚æ­¥ï¼‰"""
        pass

    @abstractmethod
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        pass

    @abstractmethod
    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜ï¼ˆå¼‚æ­¥ï¼‰"""
        pass


class LocalFileBackend(CacheStorageBackend):
    """
    æœ¬åœ°æ–‡ä»¶å­˜å‚¨åç«¯ï¼ˆå¼‚æ­¥ï¼‰

    å­˜å‚¨ä½ç½®ï¼šinstances/xxx/.cache/
    """

    def __init__(self, cache_dir: Path):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_path(self, key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{key}.json"

    async def save(self, key: str, data: Dict[str, Any]) -> bool:
        """ä¿å­˜åˆ°æœ¬åœ° JSON æ–‡ä»¶ï¼ˆå¼‚æ­¥ï¼‰"""
        try:
            path = self._get_path(key)
            content = json.dumps(data, ensure_ascii=False, indent=2)
            async with aiofiles.open(path, "w", encoding="utf-8") as f:
                await f.write(content)
            logger.debug(f"ğŸ’¾ å·²ä¿å­˜ç¼“å­˜: {path}")
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            return False

    async def load(self, key: str) -> Optional[Dict[str, Any]]:
        """ä»æœ¬åœ° JSON æ–‡ä»¶åŠ è½½ï¼ˆå¼‚æ­¥ï¼‰"""
        try:
            path = self._get_path(key)
            if not path.exists():
                return None
            async with aiofiles.open(path, "r", encoding="utf-8") as f:
                content = await f.read()
                return json.loads(content)
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None

    def exists(self, key: str) -> bool:
        """æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return self._get_path(key).exists()

    async def delete(self, key: str) -> bool:
        """åˆ é™¤æœ¬åœ°ç¼“å­˜æ–‡ä»¶"""
        try:
            path = self._get_path(key)
            if path.exists():
                path.unlink()
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False


# ============================================================
# ç¼“å­˜æ•°æ®ç»“æ„
# ============================================================


@dataclass
class CacheMetrics:
    """ç¼“å­˜æ€§èƒ½æŒ‡æ ‡"""

    load_time_ms: float = 0
    llm_analysis_time_ms: float = 0
    prompt_generation_time_ms: float = 0
    disk_load_time_ms: float = 0  # ğŸ†• V5.0: ç£ç›˜åŠ è½½è€—æ—¶
    cache_hits: int = 0
    cache_misses: int = 0
    disk_hits: int = 0  # ğŸ†• V5.0: ç£ç›˜ç¼“å­˜å‘½ä¸­æ¬¡æ•°
    disk_misses: int = 0  # ğŸ†• V5.0: ç£ç›˜ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°


@dataclass
class CacheMeta:
    """
    ç¼“å­˜å…ƒæ•°æ®

    ç”¨äºåˆ¤æ–­ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰
    """

    prompt_hash: str  # prompt.md çš„å“ˆå¸Œ
    config_hash: str  # config.yaml çš„å“ˆå¸Œ
    combined_hash: str  # ç»„åˆå“ˆå¸Œ
    created_at: str  # åˆ›å»ºæ—¶é—´
    version: str = "5.0"  # ç¼“å­˜ç‰ˆæœ¬

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "CacheMeta":
        return cls(
            prompt_hash=data.get("prompt_hash", ""),
            config_hash=data.get("config_hash", ""),
            combined_hash=data.get("combined_hash", ""),
            created_at=data.get("created_at", ""),
            version=data.get("version", "5.0"),
        )


class InstancePromptCache:
    """
    å®ä¾‹çº§æç¤ºè¯ç¼“å­˜ç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    æ ¸å¿ƒèŒè´£ï¼š
    1. å®ä¾‹å¯åŠ¨æ—¶ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æç¤ºè¯ç‰ˆæœ¬
    2. è¿è¡Œæ—¶æä¾›æ¯«ç§’çº§çš„æç¤ºè¯è®¿é—®
    3. ç®¡ç†ç¼“å­˜ç”Ÿå‘½å‘¨æœŸï¼ˆåŒ…æ‹¬å¤±æ•ˆæ£€æµ‹ï¼‰
    4. ğŸ†• V5.0: æ”¯æŒæœ¬åœ°æ–‡ä»¶æŒä¹…åŒ–

    ä½¿ç”¨æ–¹å¼ï¼š
    ```python
    # è·å–ç¼“å­˜å®ä¾‹ï¼ˆå•ä¾‹ï¼‰
    cache = InstancePromptCache.get_instance("test_agent")

    # è®¾ç½®ç¼“å­˜ç›®å½•ï¼ˆæŒä¹…åŒ–ï¼‰
    cache.set_cache_dir("/path/to/instances/test_agent/.cache")

    # å¯åŠ¨æ—¶ä¸€æ¬¡æ€§åŠ è½½ï¼ˆä¼˜å…ˆåŠ è½½ç£ç›˜ç¼“å­˜ï¼‰
    await cache.load_once(raw_prompt, config, force_refresh=False)

    # è¿è¡Œæ—¶è·å–æç¤ºè¯
    intent_prompt = cache.get_intent_prompt()
    system_prompt = cache.get_system_prompt(TaskComplexity.MEDIUM)
    agent_schema = cache.agent_schema
    ```
    """

    # ç±»çº§åˆ«çš„å®ä¾‹å­˜å‚¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    _instances: Dict[str, "InstancePromptCache"] = {}

    # ç¼“å­˜æ–‡ä»¶ key
    CACHE_KEY_PROMPTS = "prompt_cache"
    CACHE_KEY_SCHEMA = "agent_schema"
    CACHE_KEY_META = "cache_meta"

    def __init__(self, instance_name: str):
        """
        åˆå§‹åŒ–ç¼“å­˜å®ä¾‹

        Args:
            instance_name: å®ä¾‹åç§°ï¼ˆå¦‚ "test_agent"ï¼‰
        """
        self.instance_name = instance_name

        # è§£æåçš„ Schema
        self.prompt_schema: Optional[Any] = None  # PromptSchema
        self.agent_schema: Optional[Any] = None  # AgentSchema

        # ä¸‰ä¸ªç‰ˆæœ¬çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯åŠ¨æ—¶ä¸€æ¬¡æ€§ç”Ÿæˆï¼‰
        self.system_prompt_simple: Optional[str] = None
        self.system_prompt_medium: Optional[str] = None
        self.system_prompt_complex: Optional[str] = None

        # æ„å›¾è¯†åˆ«æç¤ºè¯ï¼ˆå¯åŠ¨æ—¶ä¸€æ¬¡æ€§ç”Ÿæˆï¼‰
        self.intent_prompt: Optional[str] = None

        # åŸå§‹æç¤ºè¯ï¼ˆç”¨äºç¼“å­˜å¤±æ•ˆæ£€æµ‹ï¼‰
        self._raw_prompt: str = ""
        self._raw_prompt_hash: str = ""
        self._config_hash: str = ""

        # åŠ è½½çŠ¶æ€
        self.is_loaded: bool = False
        self._load_lock = asyncio.Lock()

        # ğŸ†• V5.0: æŒä¹…åŒ–å­˜å‚¨åç«¯
        self._storage_backend: Optional[CacheStorageBackend] = None
        self._cache_dir: Optional[Path] = None

        # ğŸ†• V5.5: å®ä¾‹è·¯å¾„ï¼ˆç”¨äº prompt_results è¾“å‡ºï¼‰
        self._instance_path: Optional[Path] = None
        self._prompt_results_writer: Optional[Any] = None  # PromptResultsWriter

        # ğŸ†• V5.1: è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼ˆAPIs + framework_promptï¼‰
        # ç”± instance_loader è®¾ç½®ï¼ŒAgent è¿è¡Œæ—¶è¿½åŠ åˆ°ç¼“å­˜ç‰ˆæœ¬
        self.runtime_context: Dict[str, str] = {}

        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = CacheMetrics()

        logger.debug(f"ğŸ“¦ åˆ›å»º InstancePromptCache: {instance_name}")

    def set_cache_dir(self, cache_dir: str) -> None:
        """
        è®¾ç½®ç¼“å­˜ç›®å½•ï¼ˆå¯ç”¨æŒä¹…åŒ–ï¼‰

        ğŸ†• V5.0: è®¾ç½®åå°†ä½¿ç”¨ LocalFileBackend è¿›è¡ŒæŒä¹…åŒ–
        ğŸ†• V5.5: åŒæ—¶åˆå§‹åŒ– PromptResultsWriter

        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼ˆå¦‚ instances/test_agent/.cacheï¼‰
        """
        self._cache_dir = Path(cache_dir)
        self._storage_backend = LocalFileBackend(self._cache_dir)

        # ğŸ†• V5.5: ä» .cache ç›®å½•æ¨æ–­å®ä¾‹è·¯å¾„å¹¶åˆå§‹åŒ– PromptResultsWriter
        self._instance_path = self._cache_dir.parent

        self._prompt_results_writer = PromptResultsWriter(self._instance_path)

        logger.debug(f"ğŸ“ è®¾ç½®ç¼“å­˜ç›®å½•: {cache_dir}")
        logger.debug(f"ğŸ“ å®ä¾‹è·¯å¾„: {self._instance_path}")

    @classmethod
    def get_instance(cls, instance_name: str) -> "InstancePromptCache":
        """
        è·å–å®ä¾‹ç¼“å­˜ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

        Args:
            instance_name: å®ä¾‹åç§°

        Returns:
            InstancePromptCache å®ä¾‹
        """
        if instance_name not in cls._instances:
            cls._instances[instance_name] = cls(instance_name)
        return cls._instances[instance_name]

    @classmethod
    def clear_all(cls):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜å®ä¾‹ï¼ˆæµ‹è¯•ç”¨ï¼‰"""
        cls._instances.clear()
        logger.info("ğŸ§¹ å·²æ¸…é™¤æ‰€æœ‰ InstancePromptCache å®ä¾‹")

    async def load_once(
        self,
        raw_prompt: str,
        config: Optional[Dict[str, Any]] = None,
        force_refresh: bool = False,
        progress_callback: ProgressCallback = None,
    ) -> bool:
        """
        ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æç¤ºè¯ç‰ˆæœ¬ï¼ˆå¹‚ç­‰ï¼‰

        ğŸ†• V5.5 åŠ è½½æµç¨‹ï¼š
        1. æ£€æŸ¥æ˜¯å¦å·²åŠ è½½ï¼ˆå¹‚ç­‰ï¼‰
        2. ğŸ†• å°è¯•ä» prompt_results/ åŠ è½½ï¼ˆè¿è¥å¯ç¼–è¾‘ç‰ˆæœ¬ï¼‰
        3. æ£€æµ‹æºæ–‡ä»¶å˜åŒ–ï¼Œå†³å®šæ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ
        4. ğŸ†• åˆ†è§£ LLM ä»»åŠ¡ç”Ÿæˆåœºæ™¯åŒ–æç¤ºè¯
        5. å†™å…¥ prompt_results/ ä¾›è¿è¥æŸ¥çœ‹

        Args:
            raw_prompt: è¿è¥å†™çš„åŸå§‹ç³»ç»Ÿæç¤ºè¯
            config: å®ä¾‹é…ç½®ï¼ˆæ¥è‡ª config.yamlï¼‰
            force_refresh: å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            progress_callback: async callback(step, message) for progress reporting

        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        start_time = time.time()

        async with self._load_lock:
            # è®¡ç®—å†…å®¹å“ˆå¸Œï¼ˆç”¨äºå¤±æ•ˆæ£€æµ‹ï¼‰
            prompt_hash = self._compute_hash(raw_prompt)
            config_hash = self._compute_hash(json.dumps(config or {}, sort_keys=True))
            combined_hash = self._compute_hash(prompt_hash + config_hash)

            # æ£€æŸ¥å†…å­˜ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
            if self.is_loaded and not force_refresh:
                if combined_hash == self._compute_hash(self._raw_prompt_hash + self._config_hash):
                    self.metrics.cache_hits += 1
                    logger.debug(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­: {self.instance_name}")
                    return True
                else:
                    logger.info(f"âš ï¸ é…ç½®å·²å˜åŒ–ï¼Œé‡æ–°åŠ è½½: {self.instance_name}")

            # ä¿å­˜åŸå§‹æç¤ºè¯å’Œå“ˆå¸Œ
            self._raw_prompt = raw_prompt
            self._raw_prompt_hash = prompt_hash
            self._config_hash = config_hash

            # ğŸ†• V5.5: ä¼˜å…ˆä» prompt_results/ åŠ è½½ï¼ˆè¿è¥å¯ç¼–è¾‘ç‰ˆæœ¬ï¼‰
            if not force_refresh and self._prompt_results_writer:
                disk_start = time.time()
                if await self._try_load_from_prompt_results():
                    self.metrics.disk_hits += 1
                    self.metrics.disk_load_time_ms = (time.time() - disk_start) * 1000
                    self.metrics.load_time_ms = (time.time() - start_time) * 1000
                    self.is_loaded = True

                    # ğŸ†• V7.10: ä»ç£ç›˜åŠ è½½åä¹Ÿè¦åº”ç”¨ config.yaml çš„è¦†ç›–é…ç½®
                    # ç¡®ä¿ thinking_mode ç­‰è¿è¡Œæ—¶é…ç½®ç”Ÿæ•ˆ
                    if config:
                        self._merge_config_overrides(config)

                    logger.info(f"âœ… ä» prompt_results/ åŠ è½½: {self.instance_name}")
                    logger.info(f"   åŠ è½½è€—æ—¶: {self.metrics.disk_load_time_ms:.0f}ms")
                    return True
                else:
                    self.metrics.disk_misses += 1
                    logger.debug(f"ğŸ“ prompt_results/ æœªå‘½ä¸­æˆ–éœ€è¦æ›´æ–°")

            # ğŸ†• V5.0: å°è¯•ä» .cache/ ç£ç›˜åŠ è½½ç¼“å­˜ï¼ˆfallbackï¼‰
            if not force_refresh and self._storage_backend:
                disk_start = time.time()
                if await self._try_load_from_disk(combined_hash):
                    self.metrics.disk_hits += 1
                    self.metrics.disk_load_time_ms = (time.time() - disk_start) * 1000
                    self.metrics.load_time_ms = (time.time() - start_time) * 1000
                    self.is_loaded = True

                    logger.info(f"âœ… ä»ç£ç›˜ç¼“å­˜åŠ è½½: {self.instance_name}")
                    logger.info(f"   ç£ç›˜åŠ è½½è€—æ—¶: {self.metrics.disk_load_time_ms:.0f}ms")
                    return True
                else:
                    self.metrics.disk_misses += 1
                    logger.debug(f"ğŸ“ ç£ç›˜ç¼“å­˜æœªå‘½ä¸­æˆ–å·²å¤±æ•ˆ")

            # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œ LLM åˆ†è§£ä»»åŠ¡
            self.metrics.cache_misses += 1
            logger.info(f"ğŸ”„ å¼€å§‹ LLM åœºæ™¯åŒ–åˆ†è§£: {self.instance_name}")

            try:
                # ğŸ†• V5.5: åˆ†è§£ LLM ä»»åŠ¡ç”Ÿæˆåœºæ™¯åŒ–æç¤ºè¯
                llm_start = time.time()
                await self._generate_decomposed_prompts(raw_prompt, config, progress_callback)
                self.metrics.llm_analysis_time_ms = (time.time() - llm_start) * 1000

                self.is_loaded = True
                self.metrics.load_time_ms = (time.time() - start_time) * 1000

                # ğŸ†• V5.5: å†™å…¥ prompt_results/ ä¾›è¿è¥æŸ¥çœ‹
                if self._prompt_results_writer:
                    await self._save_to_prompt_results()

                # ğŸ†• V5.0: åŒæ—¶å†™å…¥ .cache/ ç£ç›˜ç¼“å­˜
                if self._storage_backend:
                    await self._save_to_disk(combined_hash)

                logger.info(f"âœ… InstancePromptCache åŠ è½½å®Œæˆ: {self.instance_name}")
                logger.info(f"   LLM åˆ†è§£ç”Ÿæˆ: {self.metrics.llm_analysis_time_ms:.0f}ms")
                logger.info(f"   æ€»è€—æ—¶: {self.metrics.load_time_ms:.0f}ms")

                return True

            except Exception as e:
                logger.error(f"âŒ åŠ è½½ InstancePromptCache å¤±è´¥: {e}", exc_info=True)
                # ä½¿ç”¨ fallback
                await self._load_fallback(raw_prompt)
                return False

    # ============================================================
    # ğŸ†• V5.5: prompt_results/ ç›®å½•åŠ è½½å’Œä¿å­˜
    # ============================================================

    async def _try_load_from_prompt_results(self) -> bool:
        """
        ğŸ†• V5.5: å°è¯•ä» prompt_results/ åŠ è½½ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰

        ä¼˜å…ˆä½¿ç”¨è¿è¥æ‰‹åŠ¨ç¼–è¾‘çš„ç‰ˆæœ¬

        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if not self._prompt_results_writer:
            return False

        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆï¼ˆå¼‚æ­¥è°ƒç”¨ï¼‰
            regen_flags = await self._prompt_results_writer.should_regenerate()

            # å¦‚æœæ‰€æœ‰æ–‡ä»¶éƒ½ä¸éœ€è¦é‡æ–°ç”Ÿæˆï¼Œç›´æ¥åŠ è½½
            if not any(regen_flags.values()):
                existing = await self._prompt_results_writer.load_existing()
                if existing:
                    self._load_from_prompt_results(existing)
                    logger.debug(f"ğŸ“‚ ä» prompt_results/ åŠ è½½å®Œæˆï¼ˆæ— éœ€æ›´æ–°ï¼‰")
                    return True

            # å¦‚æœéƒ¨åˆ†æ–‡ä»¶éœ€è¦é‡æ–°ç”Ÿæˆï¼Œå…ˆåŠ è½½ç°æœ‰çš„ï¼ˆä¿æŠ¤æ‰‹åŠ¨ç¼–è¾‘çš„ï¼‰
            if self._prompt_results_writer.is_valid():
                existing = await self._prompt_results_writer.load_existing()
                if existing:
                    # åªåŠ è½½ä¸éœ€è¦é‡æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                    self._load_partial_from_prompt_results(existing, regen_flags)
                    logger.debug(f"ğŸ“‚ ä» prompt_results/ éƒ¨åˆ†åŠ è½½ï¼ˆéœ€è¦æ›´æ–°éƒ¨åˆ†æ–‡ä»¶ï¼‰")
                    # è¿”å› False è§¦å‘é‡æ–°ç”Ÿæˆç¼ºå¤±çš„éƒ¨åˆ†
                    return False

            return False

        except Exception as e:
            logger.warning(f"âš ï¸ ä» prompt_results/ åŠ è½½å¤±è´¥: {e}")
            return False

    def _load_from_prompt_results(self, results) -> None:
        """ä» PromptResults åŠ è½½åˆ°å†…å­˜"""
        # åŠ è½½ AgentSchema
        if results.agent_schema:
            try:
                self.agent_schema = AgentSchema(**results.agent_schema)
            except Exception as e:
                logger.warning(f"âš ï¸ AgentSchema åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤")
                self.agent_schema = DEFAULT_AGENT_SCHEMA

        # åŠ è½½åœºæ™¯åŒ–æç¤ºè¯
        self.intent_prompt = results.intent_prompt
        self.system_prompt_simple = results.simple_prompt
        self.system_prompt_medium = results.medium_prompt
        self.system_prompt_complex = results.complex_prompt

        # åˆ›å»ºç®€å•çš„ PromptSchema
        self.prompt_schema = PromptSchema(raw_prompt=self._raw_prompt)

    def _load_partial_from_prompt_results(self, results, regen_flags: Dict[str, bool]) -> None:
        """éƒ¨åˆ†åŠ è½½ï¼ˆä¿æŠ¤æ‰‹åŠ¨ç¼–è¾‘çš„æ–‡ä»¶ï¼‰"""
        # åŠ è½½ AgentSchemaï¼ˆå¦‚æœä¸éœ€è¦é‡æ–°ç”Ÿæˆï¼‰
        if not regen_flags.get("agent_schema", True) and results.agent_schema:
            try:
                self.agent_schema = AgentSchema(**results.agent_schema)
            except Exception:
                pass

        # åŠ è½½ä¸éœ€è¦é‡æ–°ç”Ÿæˆçš„æç¤ºè¯
        if not regen_flags.get("intent_prompt", True):
            self.intent_prompt = results.intent_prompt
        if not regen_flags.get("simple_prompt", True):
            self.system_prompt_simple = results.simple_prompt
        if not regen_flags.get("medium_prompt", True):
            self.system_prompt_medium = results.medium_prompt
        if not regen_flags.get("complex_prompt", True):
            self.system_prompt_complex = results.complex_prompt

    async def _save_to_prompt_results(self) -> bool:
        """
        ğŸ†• V5.5: ä¿å­˜åˆ° prompt_results/ ç›®å½•

        Returns:
            æ˜¯å¦æˆåŠŸä¿å­˜
        """
        if not self._prompt_results_writer:
            return False

        try:
            # æ„å»ºç»“æœ
            results = PromptResults(
                agent_schema=(
                    self._agent_schema_to_dict(self.agent_schema) if self.agent_schema else {}
                ),
                intent_prompt=self.intent_prompt or "",
                simple_prompt=self.system_prompt_simple or "",
                medium_prompt=self.system_prompt_medium or "",
                complex_prompt=self.system_prompt_complex or "",
            )

            # å†™å…¥
            success = await self._prompt_results_writer.write_all(results)

            if success:
                logger.info(f"ğŸ“‚ å·²å†™å…¥ prompt_results/ ç›®å½•")

            return success

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ° prompt_results/ å¤±è´¥: {e}")
            return False

    # ============================================================
    # ğŸ†• V5.5: åˆ†è§£ LLM ä»»åŠ¡ç”Ÿæˆåœºæ™¯åŒ–æç¤ºè¯
    # ============================================================

    async def _generate_decomposed_prompts(
        self,
        raw_prompt: str,
        config: Optional[Dict[str, Any]] = None,
        progress_callback: ProgressCallback = None,
    ):
        """
        ğŸ†• V5.5: åˆ†è§£ LLM ä»»åŠ¡ç”Ÿæˆåœºæ™¯åŒ–æç¤ºè¯

        å°†å•æ¬¡è¶…é•¿ä»»åŠ¡åˆ†è§£ä¸º 5 ä¸ªç‹¬ç«‹ä»»åŠ¡ï¼š
        1. ç”Ÿæˆ AgentSchema
        2. ç”Ÿæˆæ„å›¾è¯†åˆ«æç¤ºè¯
        3. ç”Ÿæˆç®€å•ä»»åŠ¡æç¤ºè¯
        4. ç”Ÿæˆä¸­ç­‰ä»»åŠ¡æç¤ºè¯
        5. ç”Ÿæˆå¤æ‚ä»»åŠ¡æç¤ºè¯

        æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹æ‰§è¡Œï¼Œé¿å…å•æ¬¡ä»»åŠ¡è¿‡é‡å¯¼è‡´è¶…æ—¶
        """
        logger.info("   ğŸ“‹ å¼€å§‹åˆ†è§£ LLM ä»»åŠ¡...")

        # æ£€æŸ¥å“ªäº›éœ€è¦é‡æ–°ç”Ÿæˆ
        regen_flags = {
            "agent_schema": True,
            "intent_prompt": True,
            "simple_prompt": True,
            "medium_prompt": True,
            "complex_prompt": True,
        }

        if self._prompt_results_writer:
            regen_flags = await self._prompt_results_writer.should_regenerate()

        # Task 1: ç”Ÿæˆ AgentSchema
        if regen_flags.get("agent_schema", True) or not self.agent_schema:
            if progress_callback:
                await progress_callback(2, "åˆ†æè§’è‰²å®šä¹‰...")
            logger.info("   Task 1/5: ç”Ÿæˆ AgentSchema...")
            await self._generate_agent_schema(raw_prompt, config)
            logger.info(
                f"   âœ… AgentSchema: {self.agent_schema.name if self.agent_schema else 'Default'}"
            )
        else:
            logger.info("   Task 1/5: AgentSchemaï¼ˆå·²å­˜åœ¨ï¼Œè·³è¿‡ï¼‰")

        # Task 2: ç”Ÿæˆæ„å›¾è¯†åˆ«æç¤ºè¯
        if regen_flags.get("intent_prompt", True) or not self.intent_prompt:
            if progress_callback:
                await progress_callback(3, "ç”Ÿæˆæ„å›¾è¯†åˆ«...")
            logger.info("   Task 2/5: ç”Ÿæˆæ„å›¾è¯†åˆ«æç¤ºè¯...")
            await self._generate_intent_prompt_decomposed(raw_prompt)
            logger.info(f"   âœ… æ„å›¾è¯†åˆ«æç¤ºè¯: {len(self.intent_prompt or '')} å­—ç¬¦")
        else:
            logger.info("   Task 2/5: æ„å›¾è¯†åˆ«æç¤ºè¯ï¼ˆå·²å­˜åœ¨ï¼Œè·³è¿‡ï¼‰")

        # Task 3: ç”Ÿæˆç®€å•ä»»åŠ¡æç¤ºè¯
        if regen_flags.get("simple_prompt", True) or not self.system_prompt_simple:
            if progress_callback:
                await progress_callback(4, "ç”Ÿæˆåœºæ™¯æç¤ºè¯(1/3)...")
            logger.info("   Task 3/5: ç”Ÿæˆç®€å•ä»»åŠ¡æç¤ºè¯...")
            await self._generate_simple_prompt_decomposed(raw_prompt)
            logger.info(f"   âœ… ç®€å•ä»»åŠ¡æç¤ºè¯: {len(self.system_prompt_simple or '')} å­—ç¬¦")
        else:
            logger.info("   Task 3/5: ç®€å•ä»»åŠ¡æç¤ºè¯ï¼ˆå·²å­˜åœ¨ï¼Œè·³è¿‡ï¼‰")

        # Task 4: ç”Ÿæˆä¸­ç­‰ä»»åŠ¡æç¤ºè¯
        if regen_flags.get("medium_prompt", True) or not self.system_prompt_medium:
            if progress_callback:
                await progress_callback(5, "ç”Ÿæˆåœºæ™¯æç¤ºè¯(2/3)...")
            logger.info("   Task 4/5: ç”Ÿæˆä¸­ç­‰ä»»åŠ¡æç¤ºè¯...")
            await self._generate_medium_prompt_decomposed(raw_prompt)
            logger.info(f"   âœ… ä¸­ç­‰ä»»åŠ¡æç¤ºè¯: {len(self.system_prompt_medium or '')} å­—ç¬¦")
        else:
            logger.info("   Task 4/5: ä¸­ç­‰ä»»åŠ¡æç¤ºè¯ï¼ˆå·²å­˜åœ¨ï¼Œè·³è¿‡ï¼‰")

        # Task 5: ç”Ÿæˆå¤æ‚ä»»åŠ¡æç¤ºè¯
        if regen_flags.get("complex_prompt", True) or not self.system_prompt_complex:
            if progress_callback:
                await progress_callback(6, "ç”Ÿæˆåœºæ™¯æç¤ºè¯(3/3)...")
            logger.info("   Task 5/5: ç”Ÿæˆå¤æ‚ä»»åŠ¡æç¤ºè¯...")
            await self._generate_complex_prompt_decomposed(raw_prompt)
            logger.info(f"   âœ… å¤æ‚ä»»åŠ¡æç¤ºè¯: {len(self.system_prompt_complex or '')} å­—ç¬¦")
        else:
            logger.info("   Task 5/5: å¤æ‚ä»»åŠ¡æç¤ºè¯ï¼ˆå·²å­˜åœ¨ï¼Œè·³è¿‡ï¼‰")

        # åˆ›å»º PromptSchema
        self.prompt_schema = PromptSchema(raw_prompt=raw_prompt)

        logger.info("   âœ… æ‰€æœ‰åˆ†è§£ä»»åŠ¡å®Œæˆ")

    async def _generate_intent_prompt_decomposed(self, raw_prompt: str):
        """
        ç”Ÿæˆæ„å›¾è¯†åˆ«æç¤ºè¯ï¼ˆåˆ†è§£ä»»åŠ¡ï¼‰

        ğŸ†• V6.1: å¦‚æœ AgentSchema å·²ç”Ÿæˆï¼Œæ³¨å…¥èƒ½åŠ›æ‘˜è¦ç¡®ä¿æ„å›¾åˆ†ç±»ä¸ Agent èƒ½åŠ›ä¸€è‡´
        """
        try:
            # è·å– LLM Profile
            try:
                profile = await get_llm_profile("prompt_decomposer")
            except KeyError:
                profile = await get_llm_profile("llm_analyzer")

            llm_service = create_llm_service(**profile)

            # ğŸ†• V6.1: è·å– AgentSchema èƒ½åŠ›æ‘˜è¦ï¼ˆå¦‚æœå·²ç”Ÿæˆï¼‰
            schema_summary = self._build_schema_summary()

            # æ„å»ºæç¤ºè¯ï¼ˆä¼ å…¥å®Œæ•´ prompt ç”¨äºæå–æ„å›¾å®šä¹‰ï¼Œæ¨¡æ¿å†…éƒ¨ä¼šé™åˆ¶é•¿åº¦ï¼‰
            prompt_template = await get_intent_prompt_template(raw_prompt, schema_summary)

            # è°ƒç”¨ LLMï¼ˆä½¿ç”¨ Message å¯¹è±¡è€Œéå­—å…¸ï¼‰
            response = await llm_service.create_message_async(
                messages=[Message(role="user", content=prompt_template)],
                max_tokens=8000,
            )

            self.intent_prompt = response.content.strip()

        except Exception as e:
            logger.warning(f"âš ï¸ æ„å›¾è¯†åˆ«æç¤ºè¯ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤")
            from core.prompt.intent_prompt_generator import IntentPromptGenerator

            self.intent_prompt = IntentPromptGenerator.get_default()

    def _build_schema_summary(self) -> str:
        """
        ğŸ†• V6.1 æ„å»º AgentSchema èƒ½åŠ›æ‘˜è¦

        ç”¨äºæ³¨å…¥æ„å›¾è¯†åˆ«æç¤ºè¯ï¼Œç¡®ä¿ task_type åˆ†ç±»ä¸ Agent å®é™…èƒ½åŠ›ä¸€è‡´ã€‚

        Returns:
            Schema èƒ½åŠ›æ‘˜è¦æ–‡æœ¬ï¼ˆMarkdown æ ¼å¼ï¼‰ï¼Œå¦‚æœ Schema æœªç”Ÿæˆåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not self.agent_schema:
            return ""

        try:
            schema = self.agent_schema

            # æå–å·²å¯ç”¨çš„å·¥å…·
            tools = schema.tools if schema.tools else []
            tools_str = ", ".join(tools) if tools else "æ— "

            # æå–å·²å¯ç”¨çš„æŠ€èƒ½
            skills = []
            if schema.skills:
                for s in schema.skills:
                    if hasattr(s, "name"):
                        skills.append(s.name)
                    elif isinstance(s, dict):
                        skills.append(s.get("name", str(s)))
                    else:
                        skills.append(str(s))
            skills_str = ", ".join(skills) if skills else "æ— "

            # è§„åˆ’èƒ½åŠ›
            plan_enabled = schema.plan_manager.enabled if schema.plan_manager else False
            plan_str = "å¯ç”¨" if plan_enabled else "ç¦ç”¨"

            return f"""
---

## Agent èƒ½åŠ›å‚è€ƒ

æ„å›¾åˆ†ç±»æ—¶ç¡®ä¿ä¸ Agent å®é™…èƒ½åŠ›åŒ¹é…ï¼š

- **å·²å¯ç”¨å·¥å…·**: {tools_str}
- **å·²å¯ç”¨æŠ€èƒ½**: {skills_str}
- **è§„åˆ’èƒ½åŠ›**: {plan_str}

å¦‚æœç”¨æˆ·è¯·æ±‚æ¶‰åŠä¸Šè¿°æœªå¯ç”¨çš„èƒ½åŠ›ï¼Œåº”å°† complexity æ ‡è®°ä¸ºè¾ƒé«˜ã€‚
"""
        except Exception as e:
            logger.warning(f"âš ï¸ æ„å»º Schema æ‘˜è¦å¤±è´¥: {e}")
            return ""

    async def _generate_simple_prompt_decomposed(self, raw_prompt: str):
        """ç”Ÿæˆç®€å•ä»»åŠ¡æç¤ºè¯ï¼ˆåˆ†è§£ä»»åŠ¡ï¼‰"""
        try:
            try:
                profile = await get_llm_profile("prompt_decomposer")
            except KeyError:
                profile = await get_llm_profile("llm_analyzer")

            llm_service = create_llm_service(**profile)

            # æ„å»ºæç¤ºè¯ï¼ˆä¼ å…¥å®Œæ•´çš„ raw_promptï¼‰
            prompt_template = await get_simple_prompt_template(raw_prompt)

            response = await llm_service.create_message_async(
                messages=[Message(role="user", content=prompt_template)],
                max_tokens=20000,
            )

            self.system_prompt_simple = response.content.strip()

        except Exception as e:
            logger.warning(f"âš ï¸ ç®€å•ä»»åŠ¡æç¤ºè¯ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
            # Fallback: æå–æ ¸å¿ƒéƒ¨åˆ†
            self.system_prompt_simple = self._build_fallback_prompt(
                self._extract_core_sections(raw_prompt), "ç®€å•æŸ¥è¯¢", max_size=15000
            )

    async def _generate_medium_prompt_decomposed(self, raw_prompt: str):
        """ç”Ÿæˆä¸­ç­‰ä»»åŠ¡æç¤ºè¯ï¼ˆåˆ†è§£ä»»åŠ¡ï¼‰"""
        try:
            try:
                profile = await get_llm_profile("prompt_decomposer")
            except KeyError:
                profile = await get_llm_profile("llm_analyzer")

            llm_service = create_llm_service(**profile)

            prompt_template = await get_medium_prompt_template(raw_prompt)

            response = await llm_service.create_message_async(
                messages=[Message(role="user", content=prompt_template)],
                max_tokens=50000,
            )

            self.system_prompt_medium = response.content.strip()

        except Exception as e:
            logger.warning(f"âš ï¸ ä¸­ç­‰ä»»åŠ¡æç¤ºè¯ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
            self.system_prompt_medium = self._build_fallback_prompt(
                raw_prompt[:40000] if len(raw_prompt) > 40000 else raw_prompt,
                "ä¸­ç­‰ä»»åŠ¡",
                max_size=40000,
            )

    async def _generate_complex_prompt_decomposed(self, raw_prompt: str):
        """ç”Ÿæˆå¤æ‚ä»»åŠ¡æç¤ºè¯ï¼ˆåˆ†è§£ä»»åŠ¡ï¼‰"""
        try:
            try:
                profile = await get_llm_profile("prompt_decomposer")
            except KeyError:
                profile = await get_llm_profile("llm_analyzer")

            llm_service = create_llm_service(**profile)

            prompt_template = await get_complex_prompt_template(raw_prompt)

            response = await llm_service.create_message_async(
                messages=[Message(role="user", content=prompt_template)],
                max_tokens=32000,  # å…¼å®¹æ‰€æœ‰æ¨¡å‹ï¼ˆClaude/Qwenï¼‰
            )

            self.system_prompt_complex = response.content.strip()

        except Exception as e:
            logger.warning(f"âš ï¸ å¤æ‚ä»»åŠ¡æç¤ºè¯ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
            self.system_prompt_complex = self._build_fallback_prompt(
                raw_prompt[:80000] if len(raw_prompt) > 80000 else raw_prompt,
                "å¤æ‚ä»»åŠ¡",
                max_size=80000,
            )

    # ============================================================
    # ğŸ†• V5.0: ç£ç›˜æŒä¹…åŒ–æ–¹æ³•
    # ============================================================

    async def _try_load_from_disk(self, expected_hash: str) -> bool:
        """
        å°è¯•ä»ç£ç›˜åŠ è½½ç¼“å­˜ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            expected_hash: æœŸæœ›çš„å†…å®¹å“ˆå¸Œï¼ˆç”¨äºéªŒè¯ç¼“å­˜æœ‰æ•ˆæ€§ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if not self._storage_backend:
            return False

        try:
            # 1. åŠ è½½å¹¶éªŒè¯ç¼“å­˜å…ƒæ•°æ®
            meta_data = await self._storage_backend.load(self.CACHE_KEY_META)
            if not meta_data:
                logger.debug("ğŸ“ ç¼“å­˜å…ƒæ•°æ®ä¸å­˜åœ¨")
                return False

            meta = CacheMeta.from_dict(meta_data)

            # éªŒè¯å“ˆå¸Œæ˜¯å¦åŒ¹é…
            if meta.combined_hash != expected_hash:
                logger.debug(
                    f"ğŸ“ ç¼“å­˜å“ˆå¸Œä¸åŒ¹é…: {meta.combined_hash[:8]}... != {expected_hash[:8]}..."
                )
                return False

            # éªŒè¯ç‰ˆæœ¬å…¼å®¹æ€§
            if meta.version != "5.0":
                logger.debug(f"ğŸ“ ç¼“å­˜ç‰ˆæœ¬ä¸å…¼å®¹: {meta.version}")
                return False

            # 2. åŠ è½½æç¤ºè¯ç¼“å­˜
            prompt_data = await self._storage_backend.load(self.CACHE_KEY_PROMPTS)
            if not prompt_data:
                logger.debug("ğŸ“ æç¤ºè¯ç¼“å­˜ä¸å­˜åœ¨")
                return False

            self.system_prompt_simple = prompt_data.get("system_prompt_simple")
            self.system_prompt_medium = prompt_data.get("system_prompt_medium")
            self.system_prompt_complex = prompt_data.get("system_prompt_complex")
            self.intent_prompt = prompt_data.get("intent_prompt")

            # 3. åŠ è½½ AgentSchema ç¼“å­˜
            schema_data = await self._storage_backend.load(self.CACHE_KEY_SCHEMA)
            if schema_data:
                from core.schemas import AgentSchema

                try:
                    self.agent_schema = AgentSchema(**schema_data)
                except Exception as e:
                    logger.warning(f"âš ï¸ AgentSchema ååºåˆ—åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤")
                    from core.schemas import DEFAULT_AGENT_SCHEMA

                    self.agent_schema = DEFAULT_AGENT_SCHEMA

            # 4. é‡å»º PromptSchemaï¼ˆç®€åŒ–ç‰ˆï¼Œä¸éœ€è¦å®Œæ•´è§£æï¼‰
            from core.prompt import PromptSchema

            self.prompt_schema = PromptSchema(raw_prompt=self._raw_prompt)

            logger.debug(f"ğŸ“ ä»ç£ç›˜åŠ è½½ç¼“å­˜æˆåŠŸ")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸ ä»ç£ç›˜åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return False

    async def _save_to_disk(self, combined_hash: str) -> bool:
        """
        ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            combined_hash: å†…å®¹ç»„åˆå“ˆå¸Œ

        Returns:
            æ˜¯å¦æˆåŠŸä¿å­˜
        """
        if not self._storage_backend:
            return False

        try:
            # 1. ä¿å­˜ç¼“å­˜å…ƒæ•°æ®
            meta = CacheMeta(
                prompt_hash=self._raw_prompt_hash,
                config_hash=self._config_hash,
                combined_hash=combined_hash,
                created_at=datetime.now().isoformat(),
                version="5.0",
            )
            await self._storage_backend.save(self.CACHE_KEY_META, meta.to_dict())

            # 2. ä¿å­˜æç¤ºè¯ç¼“å­˜
            prompt_data = {
                "system_prompt_simple": self.system_prompt_simple,
                "system_prompt_medium": self.system_prompt_medium,
                "system_prompt_complex": self.system_prompt_complex,
                "intent_prompt": self.intent_prompt,
            }
            await self._storage_backend.save(self.CACHE_KEY_PROMPTS, prompt_data)

            # 3. ä¿å­˜ AgentSchema ç¼“å­˜
            if self.agent_schema:
                try:
                    # AgentSchema æ˜¯ dataclassï¼Œéœ€è¦è½¬æ¢ä¸º dict
                    schema_dict = self._agent_schema_to_dict(self.agent_schema)
                    await self._storage_backend.save(self.CACHE_KEY_SCHEMA, schema_dict)
                except Exception as e:
                    logger.warning(f"âš ï¸ AgentSchema åºåˆ—åŒ–å¤±è´¥: {e}")

            logger.info(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜åˆ°ç£ç›˜: {self._cache_dir}")
            return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜å¤±è´¥: {e}")
            return False

    def _agent_schema_to_dict(self, schema) -> Dict[str, Any]:
        """å°† AgentSchema è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
        from dataclasses import asdict, is_dataclass
        from enum import Enum

        def make_serializable(obj):
            """é€’å½’å¤„ç†å¯¹è±¡ä½¿å…¶å¯ JSON åºåˆ—åŒ–"""
            if obj is None:
                return None
            elif isinstance(obj, (str, int, float, bool)):
                return obj
            elif isinstance(obj, Enum):
                return obj.value
            elif isinstance(obj, dict):
                return {k: make_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [make_serializable(item) for item in obj]
            elif is_dataclass(obj):
                try:
                    return make_serializable(asdict(obj))
                except Exception:
                    # asdict å¤±è´¥æ—¶æ‰‹åŠ¨å¤„ç†
                    result = {}
                    for key in obj.__dataclass_fields__.keys():
                        value = getattr(obj, key, None)
                        result[key] = make_serializable(value)
                    return result
            elif hasattr(obj, "__dict__"):
                # æ™®é€šå¯¹è±¡ï¼Œè·³è¿‡ä¸å¯åºåˆ—åŒ–çš„å±æ€§
                result = {}
                for key, value in obj.__dict__.items():
                    if not key.startswith("_"):  # è·³è¿‡ç§æœ‰å±æ€§
                        try:
                            serialized = make_serializable(value)
                            result[key] = serialized
                        except Exception:
                            pass  # è·³è¿‡æ— æ³•åºåˆ—åŒ–çš„å±æ€§
                return result
            else:
                # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                try:
                    return str(obj)
                except Exception:
                    return None

        try:
            return make_serializable(schema)
        except Exception as e:
            logger.warning(f"âš ï¸ Schema åºåˆ—åŒ–éƒ¨åˆ†å¤±è´¥: {e}")
            # è¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                "name": getattr(schema, "name", "Unknown"),
                "model": getattr(schema, "model", None),
            }

    async def clear_disk_cache(self) -> bool:
        """
        æ¸…é™¤ç£ç›˜ç¼“å­˜ï¼ˆå¼‚æ­¥ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸæ¸…é™¤
        """
        if not self._storage_backend:
            return False

        try:
            await self._storage_backend.delete(self.CACHE_KEY_META)
            await self._storage_backend.delete(self.CACHE_KEY_PROMPTS)
            await self._storage_backend.delete(self.CACHE_KEY_SCHEMA)
            logger.info(f"ğŸ§¹ å·²æ¸…é™¤ç£ç›˜ç¼“å­˜: {self.instance_name}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¸…é™¤ç£ç›˜ç¼“å­˜å¤±è´¥: {e}")
            return False

    async def _analyze_with_llm(self, raw_prompt: str, config: Optional[Dict[str, Any]] = None):
        """
        ğŸ†• V5.2: ä½¿ç”¨ LLM è¯­ä¹‰åˆ†æå¹¶æ™ºèƒ½åˆå¹¶æ¡†æ¶è§„åˆ™

        æµç¨‹ï¼š
        1. æ¡†æ¶è§„åˆ™ + è¿è¥ prompt â†’ LLM æ™ºèƒ½åˆå¹¶ â†’ æœ€ç»ˆç³»ç»Ÿæç¤ºè¯
        2. åˆ†ææœ€ç»ˆæç¤ºè¯ â†’ PromptSchema
        3. ç”Ÿæˆ Agent é…ç½® â†’ AgentSchema

        æ¶æ„å‚è€ƒï¼šdocs/15-FRAMEWORK_PROMPT_CONTRACT.md
        """
        # ğŸ†• V5.4: è·³è¿‡ LLM åˆå¹¶æ­¥éª¤ï¼ˆæ¶æ„ä¿®æ­£ï¼‰
        #
        # åŸå› åˆ†æï¼ˆåŸºäºå®é™…æ—¥å¿—ï¼‰ï¼š
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 1. Input: 82k å­—ç¬¦ prompt.md + 5k æ¡†æ¶è§„åˆ™ â‰ˆ 27k tokens
        # 2. Task: LLM éœ€è¦"æ™ºèƒ½åˆå¹¶"ï¼ˆè¯­ä¹‰èåˆï¼Œéæ‹¼æ¥ï¼‰
        # 3. Output: ç”Ÿæˆæ–°çš„å®Œæ•´ç³»ç»Ÿæç¤ºè¯ â‰ˆ 25k tokens
        # 4. ç»“æœ: æ¯æ¬¡è¯·æ±‚éƒ½è¶…æ—¶ï¼ˆ600ç§’ï¼‰ï¼Œé‡è¯• 3 æ¬¡ï¼Œå…± 2.5 å°æ—¶å¤±è´¥
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        #
        # æ¶æ„é—®é¢˜ï¼ˆè¿å 15-FRAMEWORK_PROMPT_CONTRACT.mdï¼‰ï¼š
        # - prompt.md å·²ç»æ˜¯è¿è¥ç²¾å¿ƒç¼–å†™çš„å®Œæ•´ç³»ç»Ÿæç¤ºè¯
        # - è®© LLM "æ™ºèƒ½åˆå¹¶" = è®© LLM é‡å†™æ•´ä¸ªç³»ç»Ÿæç¤ºè¯
        # - ä»»åŠ¡è¿‡äºå¤æ‚ï¼ŒSonnet æ— æ³•åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        #
        # æ­£ç¡®æ¶æ„ï¼š
        # - æ¡†æ¶è§„åˆ™é€šè¿‡ Schema å’Œç»„ä»¶ä½“ç°ï¼ˆå·²å®ç°ï¼‰
        # - è¿è¡Œæ—¶åŠ¨æ€è¿½åŠ ï¼ˆå·²å®ç°ï¼šprompt_cache.runtime_contextï¼‰
        # - ä¸åº”è¯¥åœ¨å¯åŠ¨æ—¶åˆå¹¶

        logger.info("   Step 1: ä½¿ç”¨è¿è¥æç¤ºè¯ï¼ˆè·³è¿‡ LLM åˆå¹¶ï¼Œç›´æ¥åˆ†æï¼‰...")
        merged_prompt = raw_prompt
        self._raw_user_prompt = raw_prompt
        self._merged_prompt = raw_prompt
        logger.info(f"   âœ… æç¤ºè¯é•¿åº¦: {len(merged_prompt):,} å­—ç¬¦")

        # Step 2: è§£æ PromptSchemaï¼ˆä½¿ç”¨åˆå¹¶åçš„æç¤ºè¯ï¼‰
        logger.info("   Step 2: è§£æ PromptSchema...")
        self.prompt_schema = await PromptParser.parse_async(merged_prompt, use_llm=True)
        logger.info(
            f"   PromptSchema: {self.prompt_schema.agent_name} ({len(self.prompt_schema.modules)} æ¨¡å—)"
        )

        # 2. ç”Ÿæˆ AgentSchemaï¼ˆä½¿ç”¨é«˜è´¨é‡ Prompt + few-shotï¼‰
        await self._generate_agent_schema(raw_prompt, config)
        logger.info(f"   AgentSchema: {self.agent_schema.name if self.agent_schema else 'Default'}")

    async def _generate_agent_schema(
        self, raw_prompt: str, config: Optional[Dict[str, Any]] = None
    ):
        """
        ä½¿ç”¨é«˜è´¨é‡ Prompt + few-shot ç”Ÿæˆ AgentSchema

        ğŸ†• V5.0: åº”ç”¨çº§é‡è¯•é€»è¾‘

        æ ¸å¿ƒå“²å­¦ï¼šè§„åˆ™å†™åœ¨é«˜è´¨é‡ Prompt é‡Œï¼Œä¸å†™åœ¨ä»£ç é‡Œ
        """
        # ğŸ†• V5.0: åº”ç”¨çº§é‡è¯•é…ç½®
        max_retries = 2
        retry_delay = 1.0  # ç§’

        for attempt in range(max_retries + 1):
            try:
                # å»¶è¿Ÿå¯¼å…¥ AgentFactoryï¼Œé¿å…å¾ªç¯ä¾èµ–
                from core.agent.factory import AgentFactory

                # è°ƒç”¨ LLM ç”Ÿæˆ Schemaï¼ˆä½¿ç”¨é«˜è´¨é‡ Prompt + few-shotï¼‰
                self.agent_schema = await AgentFactory._generate_schema(raw_prompt)

                # åˆå¹¶å®ä¾‹é…ç½®ï¼ˆconfig.yaml ä¸­çš„è¦†ç›–ï¼‰
                if config:
                    self._merge_config_overrides(config)

                return  # æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯

            except Exception as e:
                if attempt < max_retries:
                    logger.warning(
                        f"âš ï¸ AgentSchema ç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}"
                    )
                    await asyncio.sleep(retry_delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                else:
                    logger.warning(f"âš ï¸ AgentSchema ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                    self.agent_schema = DEFAULT_AGENT_SCHEMA

    def _merge_config_overrides(self, config: Dict[str, Any]):
        """åˆå¹¶ config.yaml ä¸­çš„è¦†ç›–é…ç½®"""
        if not self.agent_schema:
            return

        # åˆå¹¶ agent é…ç½®
        agent_config = config.get("agent", {})
        if agent_config:
            if "model" in agent_config:
                self.agent_schema.model = agent_config["model"]
            if "max_turns" in agent_config:
                self.agent_schema.max_turns = agent_config["max_turns"]
            if "plan_manager_enabled" in agent_config:
                self.agent_schema.plan_manager.enabled = agent_config["plan_manager_enabled"]

        # åˆå¹¶ prompts é…ç½®ï¼ˆå¿…é¡»åœ¨ thinking_mode ä¹‹å‰ï¼ŒéªŒè¯å™¨ä¾èµ–å®ƒï¼‰
        prompts_config = config.get("prompts", {})
        logger.debug(
            f"ğŸ“‹ config keys: {list(config.keys())}, prompts_config: {bool(prompts_config)}"
        )
        if prompts_config:
            from core.schemas.validator import PrefaceConfig, PromptsConfig, SimulatedThinkingConfig

            # æ„å»º PromptsConfig
            preface_cfg = prompts_config.get("preface")
            simulated_thinking_cfg = prompts_config.get("simulated_thinking")

            preface = None
            if preface_cfg:
                preface = PrefaceConfig(
                    enabled=preface_cfg.get("enabled", True),
                    max_tokens=preface_cfg.get("max_tokens", 150),
                    template=preface_cfg.get("template", ""),
                )

            simulated_thinking = None
            if simulated_thinking_cfg:
                simulated_thinking = SimulatedThinkingConfig(
                    guide=simulated_thinking_cfg.get("guide", "")
                )

            self.agent_schema.prompts = PromptsConfig(
                preface=preface, simulated_thinking=simulated_thinking
            )
            logger.info("ğŸ“ prompts é…ç½®å·²åº”ç”¨")

        # åˆå¹¶ LLM è¶…å‚æ•°ï¼ˆthinking_mode å¿…é¡»åœ¨ prompts ä¹‹åï¼‰
        llm_config = agent_config.get("llm", {})
        if llm_config:
            # å¤„ç† thinking_modeï¼ˆç›´æ¥åœ¨ AgentSchema ä¸Šï¼‰
            if "thinking_mode" in llm_config:
                self.agent_schema.thinking_mode = llm_config["thinking_mode"]
                logger.info(f"ğŸ§  thinking_mode é…ç½®å·²åº”ç”¨: {llm_config['thinking_mode']}")

            # å¤„ç†å…¶ä»– LLM é…ç½®ï¼ˆå¦‚æœæœ‰ llm_config å±æ€§ï¼‰
            if hasattr(self.agent_schema, "llm_config"):
                for key, value in llm_config.items():
                    if key != "thinking_mode" and hasattr(self.agent_schema.llm_config, key):
                        setattr(self.agent_schema.llm_config, key, value)

    async def _generate_all_prompts(self):
        """
        ğŸ†• V5.2: ç”Ÿæˆä¸‰ä¸ªç‰ˆæœ¬çš„ç³»ç»Ÿæç¤ºè¯

        åŸºäº LLM æ™ºèƒ½åˆå¹¶åçš„æç¤ºè¯ï¼ŒæŒ‰å¤æ‚åº¦è£å‰ªç”Ÿæˆä¸‰ä¸ªç‰ˆæœ¬
        """
        if not self.prompt_schema:
            logger.warning("âš ï¸ PromptSchema æœªåŠ è½½ï¼Œè·³è¿‡æç¤ºè¯ç”Ÿæˆ")
            return

        # ğŸ†• V5.2: ç¡®ä¿ PromptSchema åŒ…å«åˆå¹¶åçš„æç¤ºè¯
        if hasattr(self, "_merged_prompt") and self._merged_prompt:
            self.prompt_schema.raw_prompt = self._merged_prompt
            logger.info(f"   ä½¿ç”¨ LLM åˆå¹¶åçš„æç¤ºè¯ä½œä¸ºåŸºç¡€: {len(self._merged_prompt)} å­—ç¬¦")

        # æ›´æ–°æ’é™¤æ¨¡å—ï¼ˆæ ¹æ® AgentSchemaï¼‰
        self.prompt_schema.update_exclusions(self.agent_schema)

        # ç”Ÿæˆä¸‰ä¸ªç‰ˆæœ¬ï¼ˆåŸºäºåˆå¹¶åçš„æç¤ºè¯æŒ‰å¤æ‚åº¦è£å‰ªï¼‰
        self.system_prompt_simple = generate_prompt(
            self.prompt_schema, TaskComplexity.SIMPLE, self.agent_schema
        )

        self.system_prompt_medium = generate_prompt(
            self.prompt_schema, TaskComplexity.MEDIUM, self.agent_schema
        )

        self.system_prompt_complex = generate_prompt(
            self.prompt_schema, TaskComplexity.COMPLEX, self.agent_schema
        )

        logger.info(f"   ç³»ç»Ÿæç¤ºè¯ç‰ˆæœ¬:")
        logger.info(f"     Simple: {len(self.system_prompt_simple)} å­—ç¬¦")
        logger.info(f"     Medium: {len(self.system_prompt_medium)} å­—ç¬¦")
        logger.info(f"     Complex: {len(self.system_prompt_complex)} å­—ç¬¦")

    async def _generate_intent_prompt(self):
        """ç”Ÿæˆæ„å›¾è¯†åˆ«æç¤ºè¯"""
        if self.prompt_schema:
            # ä» PromptSchema åŠ¨æ€ç”Ÿæˆï¼ˆç”¨æˆ·é…ç½®ä¼˜å…ˆï¼‰
            self.intent_prompt = IntentPromptGenerator.generate(self.prompt_schema)
            logger.info(f"   æ„å›¾è¯†åˆ«æç¤ºè¯: {len(self.intent_prompt)} å­—ç¬¦ (åŠ¨æ€ç”Ÿæˆ)")
        else:
            # ä½¿ç”¨é«˜è´¨é‡é»˜è®¤
            self.intent_prompt = IntentPromptGenerator.get_default()
            logger.info(f"   æ„å›¾è¯†åˆ«æç¤ºè¯: {len(self.intent_prompt)} å­—ç¬¦ (é»˜è®¤)")

    async def _load_fallback(self, raw_prompt: str):
        """
        åŠ è½½å¤±è´¥æ—¶çš„ fallback

        ğŸ†• V5.1: å³ä½¿ fallback ä¹Ÿè¦ç”Ÿæˆåˆç†å¤§å°çš„æç¤ºè¯ç‰ˆæœ¬
        """
        logger.warning("âš ï¸ ä½¿ç”¨ fallback åŠ è½½")

        # ä½¿ç”¨æœ€ç®€å•çš„é…ç½®
        self.prompt_schema = PromptSchema(raw_prompt=raw_prompt)
        self.agent_schema = DEFAULT_AGENT_SCHEMA

        # ğŸ†• V5.1: å³ä½¿ fallback ä¹Ÿè¦ç”Ÿæˆç²¾ç®€ç‰ˆæœ¬
        # æå–æ ¸å¿ƒå†…å®¹ï¼ˆè§’è‰²å®šä¹‰ + ç¦ä»¤ï¼‰
        core_sections = self._extract_core_sections(raw_prompt)

        # Simple: ä»…æ ¸å¿ƒè§„åˆ™ï¼ˆé™åˆ¶ 15k å­—ç¬¦ï¼‰
        self.system_prompt_simple = self._build_fallback_prompt(
            core_sections, "ç®€å•æŸ¥è¯¢", max_size=15000
        )

        # Medium: æ ¸å¿ƒ + éƒ¨åˆ†æ‰©å±•ï¼ˆé™åˆ¶ 40k å­—ç¬¦ï¼‰
        self.system_prompt_medium = self._build_fallback_prompt(
            raw_prompt[:40000] if len(raw_prompt) > 40000 else raw_prompt,
            "ä¸­ç­‰ä»»åŠ¡",
            max_size=40000,
        )

        # Complex: å®Œæ•´ç‰ˆæœ¬ï¼ˆé™åˆ¶ 80k å­—ç¬¦ï¼‰
        self.system_prompt_complex = self._build_fallback_prompt(
            raw_prompt[:80000] if len(raw_prompt) > 80000 else raw_prompt,
            "å¤æ‚ä»»åŠ¡",
            max_size=80000,
        )

        logger.info(
            f"   Fallback ç‰ˆæœ¬: Simple={len(self.system_prompt_simple)}, "
            f"Medium={len(self.system_prompt_medium)}, "
            f"Complex={len(self.system_prompt_complex)} å­—ç¬¦"
        )

        # ä½¿ç”¨é»˜è®¤æ„å›¾è¯†åˆ«æç¤ºè¯
        self.intent_prompt = get_intent_recognition_prompt()

        self.is_loaded = True

    def _extract_core_sections(self, raw_prompt: str) -> str:
        """
        ğŸ†• V5.1: ä»åŸå§‹æç¤ºè¯ä¸­æå–æ ¸å¿ƒéƒ¨åˆ†

        æå–å†…å®¹ï¼š
        - è§’è‰²å®šä¹‰ï¼ˆå¼€å¤´åˆ°ç¬¬ä¸€ä¸ªä¸»è¦åˆ†éš”ç¬¦ï¼‰
        - ç»å¯¹ç¦ä»¤ï¼ˆ<absolute_prohibitions> æ ‡ç­¾å†…å®¹ï¼‰
        - è¾“å‡ºæ ¼å¼åŸºç¡€è§„åˆ™
        """
        import re

        parts = []

        # 1. æå–è§’è‰²å®šä¹‰ï¼ˆå¼€å¤´éƒ¨åˆ†ï¼‰
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸»è¦åˆ†éš”ç¬¦çš„ä½ç½®
        separators = ["<absolute_prohibitions", "## ç»å¯¹ç¦ä»¤", "---\n\n#", "==="]
        end_pos = len(raw_prompt)
        for sep in separators:
            pos = raw_prompt.find(sep)
            if pos > 0 and pos < end_pos:
                end_pos = pos

        role_section = raw_prompt[: min(end_pos, 3000)].strip()
        if role_section:
            parts.append(role_section)

        # 2. æå–ç»å¯¹ç¦ä»¤
        prohibitions_match = re.search(
            r"<absolute_prohibitions.*?>.*?</absolute_prohibitions>", raw_prompt, re.DOTALL
        )
        if prohibitions_match:
            parts.append(prohibitions_match.group(0)[:3000])  # é™åˆ¶å¤§å°

        # 3. æå–è¾“å‡ºæ ¼å¼æ ¸å¿ƒè§„åˆ™
        output_patterns = [
            r"## \d*\.?\s*æ ¸å¿ƒæ¶æ„.*?(?=^## \d|^# |\Z)",
            r"ä¸‰æ®µå¼.*?è¾“å‡ºæ ¼å¼.*?(?=\n\n\n|\Z)",
        ]
        for pattern in output_patterns:
            match = re.search(pattern, raw_prompt, re.MULTILINE | re.DOTALL)
            if match:
                parts.append(match.group(0)[:5000])
                break

        return "\n\n---\n\n".join(parts)

    def _build_fallback_prompt(self, content: str, mode: str, max_size: int) -> str:
        """
        ğŸ†• V5.1: æ„å»º fallback ç‰ˆæœ¬çš„æç¤ºè¯
        """
        header = f"""# GeneralAgent

---

## å½“å‰ä»»åŠ¡æ¨¡å¼ï¼š{mode}

"""

        # ç¡®ä¿ä¸è¶…è¿‡å¤§å°é™åˆ¶
        available_size = max_size - len(header) - 100  # é¢„ç•™ç¼“å†²
        if len(content) > available_size:
            content = content[:available_size].rsplit("\n", 1)[0]
            content += "\n\n<!-- å†…å®¹å·²ç²¾ç®€ -->"

        return header + content

    def get_system_prompt(self, complexity) -> str:
        """
        è·å–å¯¹åº”å¤æ‚åº¦çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆç›´æ¥ä»ç¼“å­˜å–ï¼‰

        Args:
            complexity: TaskComplexity æšä¸¾

        Returns:
            å¯¹åº”ç‰ˆæœ¬çš„ç³»ç»Ÿæç¤ºè¯
        """
        if not self.is_loaded:
            logger.warning("âš ï¸ ç¼“å­˜æœªåŠ è½½ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
            return ""

        if complexity == TaskComplexity.SIMPLE:
            return self.system_prompt_simple or ""
        elif complexity == TaskComplexity.MEDIUM:
            return self.system_prompt_medium or ""
        else:
            return self.system_prompt_complex or ""

    def get_full_system_prompt(self, complexity) -> str:
        """
        ğŸ†• V5.1: è·å–å®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆç¼“å­˜ç‰ˆæœ¬ + è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼‰

        è¿è¡Œæ—¶åŠ¨æ€ç»„è£…ï¼š
        1. ä»ç¼“å­˜è·å–å¯¹åº”å¤æ‚åº¦çš„ç²¾ç®€ç‰ˆæœ¬
        2. è¿½åŠ è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼ˆAPIs æè¿° + æ¡†æ¶åè®®ï¼‰

        Args:
            complexity: TaskComplexity æšä¸¾

        Returns:
            å®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆç¼“å­˜ç‰ˆæœ¬ + è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼‰
        """
        # 1. è·å–ç¼“å­˜çš„ç²¾ç®€ç‰ˆæœ¬
        base_prompt = self.get_system_prompt(complexity)

        if not base_prompt:
            logger.warning(f"âš ï¸ ç¼“å­˜ç‰ˆæœ¬ä¸ºç©º: complexity={complexity}")
            return ""

        # 2. å¦‚æœæ²¡æœ‰è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼Œç›´æ¥è¿”å›ç¼“å­˜ç‰ˆæœ¬
        if not self.runtime_context:
            return base_prompt

        # 3. è¿½åŠ è¿è¡Œæ—¶ä¸Šä¸‹æ–‡
        apis_prompt = self.runtime_context.get("apis_prompt", "")
        framework_prompt = self.runtime_context.get("framework_prompt", "")
        environment_prompt = self.runtime_context.get("environment_prompt", "")  # ğŸ†• V6.0

        # ç»„è£…å®Œæ•´æç¤ºè¯
        parts = [base_prompt]

        # ğŸ†• V6.0: ç¯å¢ƒä¿¡æ¯ä¼˜å…ˆæ³¨å…¥ï¼ˆè®© Agent äº†è§£è¿è¡Œç¯å¢ƒï¼‰
        if environment_prompt:
            parts.append(f"\n\n---\n\n{environment_prompt}")

        if apis_prompt:
            parts.append(f"\n\n---\n\n{apis_prompt}")

        if framework_prompt:
            parts.append(f"\n\n---\n\n# æ¡†æ¶èƒ½åŠ›åè®®\n\n{framework_prompt}")

        full_prompt = "".join(parts)

        runtime_len = len(apis_prompt) + len(framework_prompt) + len(environment_prompt)
        logger.debug(
            f"âœ… ç»„è£…å®Œæ•´ç³»ç»Ÿæç¤ºè¯: ç¼“å­˜={len(base_prompt)} + è¿è¡Œæ—¶={runtime_len} = {len(full_prompt)} å­—ç¬¦"
        )

        return full_prompt

    def get_intent_prompt(self) -> str:
        """
        è·å–æ„å›¾è¯†åˆ«æç¤ºè¯ï¼ˆç”¨æˆ·é…ç½® or é»˜è®¤ï¼‰

        Returns:
            æ„å›¾è¯†åˆ«æç¤ºè¯
        """
        if self.intent_prompt:
            return self.intent_prompt

        # fallback åˆ°é»˜è®¤
        return get_intent_recognition_prompt()

    def get_cached_system_blocks(
        self, complexity, user_profile: Optional[str] = None, tools_context: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        æ„å»ºå¤šå±‚ç¼“å­˜çš„ system blocksï¼ˆç”¨äº Claude Prompt Cachingï¼‰

        ğŸ†• å‰ç¼€ç¼“å­˜ä¼˜åŒ–ç­–ç•¥ï¼š
        Claude çš„ç¼“å­˜æ˜¯ç´¯ç§¯å¼å‰ç¼€åŒ¹é…ï¼Œä»å¼€å¤´åˆ°æ–­ç‚¹çš„æ•´ä¸ªå‰ç¼€åºåˆ—ä¼šè¢«ç¼“å­˜ã€‚
        å¤šä¸ªæ–­ç‚¹å¯ä»¥å®ç°åˆ†çº§ç¼“å­˜ï¼Œæé«˜ä¸åŒåœºæ™¯ä¸‹çš„å‘½ä¸­ç‡ã€‚

        ç¼“å­˜å±‚çº§ï¼ˆæŒ‰ç¨³å®šæ€§ä»é«˜åˆ°ä½æ’åºï¼‰ï¼š
        - Layer 1: æ¡†æ¶è§„åˆ™ï¼ˆ1h ç¼“å­˜ï¼‰- è·¨ Agent å…±äº«ï¼Œå‘½ä¸­ç‡æœ€é«˜
        - Layer 2: å®ä¾‹æç¤ºè¯ï¼ˆ1h ç¼“å­˜ï¼‰- åŒ Agent å…±äº«
        - Layer 3: Skills + å·¥å…·ï¼ˆ1h ç¼“å­˜ï¼‰- è¿è¡ŒæœŸç¨³å®š
        - Layer 4: Mem0 ç”¨æˆ·ç”»åƒï¼ˆä¸ç¼“å­˜ï¼‰- æ¯æ¬¡æ£€ç´¢ç»“æœä¸åŒ

        æ–­ç‚¹ç­–ç•¥ï¼ˆClaude æœ€å¤šæ”¯æŒ 4 ä¸ªæ–­ç‚¹ï¼‰ï¼š
        - æ–­ç‚¹ 1: æ¡†æ¶è§„åˆ™å â†’ è·¨ Agentã€è·¨ç”¨æˆ·å…±äº«
        - æ–­ç‚¹ 2: å®ä¾‹æç¤ºè¯å â†’ åŒ Agent ä¸åŒç”¨æˆ·å…±äº«
        - æ–­ç‚¹ 3: Skills + å·¥å…·å â†’ åŒ Agent åŒç”¨æˆ·ä¸åŒè½®æ¬¡å…±äº«
        - ç”¨æˆ·ç”»åƒä¸ç¼“å­˜ â†’ åŠ¨æ€å†…å®¹æ”¾æœ€å

        Args:
            complexity: TaskComplexity æšä¸¾ï¼ˆSIMPLE/MEDIUM/COMPLEXï¼‰
            user_profile: Mem0 ç”¨æˆ·ç”»åƒï¼ˆå¯é€‰ï¼Œä¸ç¼“å­˜ï¼‰
            tools_context: Skills + å·¥å…·å®šä¹‰ï¼ˆå¯é€‰ï¼Œ1h ç¼“å­˜ï¼‰

        Returns:
            List[Dict] - Claude API çš„ system blocks æ ¼å¼ï¼ˆå¸¦ _cache_layer å…ƒæ•°æ®ï¼‰

        Example:
            system_blocks = cache.get_cached_system_blocks(
                complexity=TaskComplexity.MEDIUM,
                user_profile=mem0_profile,
                tools_context=skills_metadata
            )
            response = await llm.create_message_async(messages, system=system_blocks)
        """
        system_blocks = []

        # Layer 1: æ¡†æ¶è§„åˆ™ï¼ˆæœ€ç¨³å®šï¼Œè·¨ Agent å…±äº«ï¼‰
        # æ¡†æ¶å‡çº§ â†’ é‡å¯ â†’ è¿è¡ŒæœŸç¨³å®š
        # ğŸ”§ æ–­ç‚¹ 1ï¼šæ‰€æœ‰ Agent å…±äº«æ¡†æ¶è§„åˆ™
        framework_prompt = self.runtime_context.get("framework_prompt", "")
        if framework_prompt:
            system_blocks.append(
                {
                    "type": "text",
                    "text": f"# æ¡†æ¶èƒ½åŠ›åè®®\n\n{framework_prompt}",
                    "_cache_layer": 1,  # ğŸ†• å…ƒæ•°æ®ï¼šæ ‡è®°ä¸ºç¬¬ 1 å±‚ç¼“å­˜
                }
            )
            logger.debug(f"ğŸ“¦ Layer 1 (æ¡†æ¶è§„åˆ™): {len(framework_prompt)} å­—ç¬¦ [cache_layer=1]")

        # Layer 2: å®ä¾‹æ ¸å¿ƒæç¤ºè¯ï¼ˆåŒ Agent å…±äº«ï¼‰
        # è¿è¥ä¼˜åŒ– â†’ é‡å¯ â†’ è¿è¡ŒæœŸç¨³å®š
        # ğŸ”§ æ–­ç‚¹ 2ï¼šåŒ Agent çš„ä¸åŒç”¨æˆ·/ä¼šè¯å…±äº«
        instance_prompt = self.get_system_prompt(complexity)
        if instance_prompt:
            system_blocks.append(
                {
                    "type": "text",
                    "text": instance_prompt,
                    "_cache_layer": 2,  # ğŸ†• å…ƒæ•°æ®ï¼šæ ‡è®°ä¸ºç¬¬ 2 å±‚ç¼“å­˜
                }
            )
            logger.debug(f"ğŸ“¦ Layer 2 (å®ä¾‹æç¤ºè¯): {len(instance_prompt)} å­—ç¬¦ [cache_layer=2]")

        # Layer 3: APIs + å·¥å…·å®šä¹‰ï¼ˆè¿è¡ŒæœŸç¨³å®šï¼‰
        # å·¥å…·æ›´æ–° â†’ é‡å¯ â†’ è¿è¡ŒæœŸç¨³å®š
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ tools_contextï¼Œå¦åˆ™ä½¿ç”¨ runtime_context ä¸­çš„ apis_prompt
        tools_text = tools_context or self.runtime_context.get("apis_prompt", "")
        if tools_text:
            system_blocks.append(
                {
                    "type": "text",
                    "text": tools_text,
                    "_cache_layer": 3,  # ğŸ†• å…ƒæ•°æ®ï¼šæ ‡è®°ä¸ºç¬¬ 3 å±‚ç¼“å­˜ï¼ˆä¸ Skills åˆå¹¶ï¼‰
                }
            )
            logger.debug(f"ğŸ“¦ Layer 3 (APIs+å·¥å…·): {len(tools_text)} å­—ç¬¦ [cache_layer=3]")

        # Layer 3.5: Skills Promptï¼ˆä¸å·¥å…·åˆå¹¶ä¸ºåŒä¸€å±‚ç¼“å­˜ï¼‰
        # å°† <available_skills> XML æ³¨å…¥åˆ°æç¤ºè¯ï¼ŒAgent é€šè¿‡ read å·¥å…·è¯»å– SKILL.md
        # ğŸ”§ æ–­ç‚¹ 3ï¼šåœ¨ Skills åæ·»åŠ ï¼ŒåŒ Agent åŒç”¨æˆ·ä¸åŒè½®æ¬¡å…±äº«
        skills_prompt = self.runtime_context.get("skills_prompt", "")
        if skills_prompt:
            system_blocks.append(
                {"type": "text", "text": skills_prompt, "_cache_layer": 3}  # ğŸ†• ä¸å·¥å…·åˆå¹¶ä¸ºç¬¬ 3 å±‚
            )
            logger.debug(f"ğŸ“¦ Layer 3.5 (Skills Prompt): {len(skills_prompt)} å­—ç¬¦ [cache_layer=3]")

        # Layer 4: Mem0 ç”¨æˆ·ç”»åƒï¼ˆä¸ç¼“å­˜ï¼‰
        # åŸºäºè¯­ä¹‰æ£€ç´¢ï¼Œæ¯æ¬¡ query ä¸åŒ â†’ ç»“æœä¸åŒ â†’ ä¸èƒ½ç¼“å­˜
        # ğŸ”§ åŠ¨æ€å†…å®¹æ”¾æœ€åï¼Œä¸å½±å“å‰ç¼€ç¼“å­˜å‘½ä¸­
        if user_profile:
            system_blocks.append(
                {
                    "type": "text",
                    "text": f"# ç”¨æˆ·ç”»åƒ\n\n{user_profile}",
                    "_cache_layer": 0,  # ğŸ†• å…ƒæ•°æ®ï¼š0 è¡¨ç¤ºä¸ç¼“å­˜
                }
            )
            logger.debug(f"ğŸ“¦ Layer 4 (ç”¨æˆ·ç”»åƒ): {len(user_profile)} å­—ç¬¦ [ä¸ç¼“å­˜]")

        # ç»Ÿè®¡ç¼“å­˜å±‚
        cached_layers = len([b for b in system_blocks if b.get("_cache_layer", 0) > 0])
        logger.info(
            f"ğŸ—‚ï¸ æ„å»ºå¤šå±‚ç¼“å­˜ system blocks: {len(system_blocks)} å±‚, "
            f"å…¶ä¸­ {cached_layers} å±‚å¯ç”¨ç¼“å­˜"
        )

        return system_blocks

    def get_cached_intent_blocks(self) -> List[Dict[str, Any]]:
        """
        æ„å»ºæ„å›¾è¯†åˆ«çš„ system blocksï¼ˆç”¨äº Claude Prompt Cachingï¼‰

        æ„å›¾è¯†åˆ«æç¤ºè¯åœ¨è¿è¡ŒæœŸåªè¯»ï¼Œå¯ç”¨ç¼“å­˜ï¼ˆ5åˆ†é’Ÿ TTLï¼ŒClaude å›ºå®šï¼‰

        Returns:
            List[Dict] - Claude API çš„ system blocks æ ¼å¼
        """
        intent_prompt = self.get_intent_prompt()

        if not intent_prompt:
            return []

        # ğŸ”§ ä¸åœ¨è¿™é‡Œæ·»åŠ  cache_controlï¼Œç”± claude.py ç»Ÿä¸€å¤„ç†
        system_blocks = [{"type": "text", "text": intent_prompt}]

        logger.debug(f"ğŸ—‚ï¸ æ„å»ºæ„å›¾è¯†åˆ« system blocks: {len(intent_prompt)} å­—ç¬¦")

        return system_blocks

    @staticmethod
    def _compute_hash(content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        return hashlib.md5(content.encode()).hexdigest()

    def get_status(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çŠ¶æ€ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
        return {
            "instance_name": self.instance_name,
            "is_loaded": self.is_loaded,
            "prompt_schema": (
                self.prompt_schema.agent_name
                if self.prompt_schema and hasattr(self.prompt_schema, "agent_name")
                else None
            ),
            "agent_schema": (
                self.agent_schema.name
                if self.agent_schema and hasattr(self.agent_schema, "name")
                else None
            ),
            "system_prompts": {
                "simple": len(self.system_prompt_simple or ""),
                "medium": len(self.system_prompt_medium or ""),
                "complex": len(self.system_prompt_complex or ""),
            },
            "intent_prompt": len(self.intent_prompt or ""),
            # ğŸ†• V5.0: æŒä¹…åŒ–çŠ¶æ€
            "persistence": {
                "enabled": self._storage_backend is not None,
                "cache_dir": str(self._cache_dir) if self._cache_dir else None,
                "has_disk_cache": (
                    self._storage_backend.exists(self.CACHE_KEY_META)
                    if self._storage_backend
                    else False
                ),
            },
            "metrics": {
                "load_time_ms": self.metrics.load_time_ms,
                "disk_load_time_ms": self.metrics.disk_load_time_ms,
                "llm_analysis_time_ms": self.metrics.llm_analysis_time_ms,
                "cache_hits": self.metrics.cache_hits,
                "cache_misses": self.metrics.cache_misses,
                "disk_hits": self.metrics.disk_hits,
                "disk_misses": self.metrics.disk_misses,
            },
        }


# ============================================================
# ä¾¿æ·å‡½æ•°
# ============================================================


def get_instance_cache(instance_name: str) -> InstancePromptCache:
    """è·å–å®ä¾‹ç¼“å­˜ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    return InstancePromptCache.get_instance(instance_name)


async def load_instance_cache(
    instance_name: str,
    raw_prompt: str,
    config: Optional[Dict[str, Any]] = None,
    cache_dir: Optional[str] = None,
    force_refresh: bool = False,
    progress_callback: ProgressCallback = None,
) -> InstancePromptCache:
    """
    åŠ è½½å®ä¾‹ç¼“å­˜ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    ğŸ†• V5.0: æ”¯æŒè®¾ç½®ç¼“å­˜ç›®å½•å®ç°æŒä¹…åŒ–

    Args:
        instance_name: å®ä¾‹åç§°
        raw_prompt: åŸå§‹æç¤ºè¯
        config: å®ä¾‹é…ç½®
        cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼ˆå¯ç”¨æŒä¹…åŒ–ï¼‰
        force_refresh: å¼ºåˆ¶åˆ·æ–°
        progress_callback: async callback(step, message) for progress reporting

    Returns:
        åŠ è½½å®Œæˆçš„ InstancePromptCache
    """
    cache = get_instance_cache(instance_name)

    # ğŸ†• V5.0: è®¾ç½®ç¼“å­˜ç›®å½•ï¼ˆå¯ç”¨æŒä¹…åŒ–ï¼‰
    if cache_dir:
        cache.set_cache_dir(cache_dir)

    await cache.load_once(raw_prompt, config, force_refresh, progress_callback)
    return cache
