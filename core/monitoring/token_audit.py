"""
Token å®¡è®¡æ¨¡å—

åŠŸèƒ½ï¼š
1. è®°å½•æ¯æ¬¡ Agent æ‰§è¡Œçš„ Token æ¶ˆè€—
2. æ”¯æŒå¤šç»´åº¦ç»Ÿè®¡ï¼ˆæ™ºèƒ½ä½“çº§ã€ä¼šè¯çº§ã€ç”¨æˆ·çº§ï¼‰
3. ä¸Žè¯„ä¼°ç³»ç»Ÿé›†æˆï¼Œç”¨äºŽæˆæœ¬åˆ†æž
4. å®‰å…¨ä¿æŠ¤ï¼šå¼‚å¸¸æ¶ˆè€—å‘Šè­¦
5. è®¡è´¹æ—¥å¿—ï¼šæŒ‰ç”¨æˆ·+æ—¥æœŸå†™å…¥ JSON Lines æ–‡ä»¶

æž¶æž„ä½ç½®ï¼šcore/monitoring/token_audit.py
ä¾èµ–ï¼ševaluation/models.py (TokenUsage)
"""

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

from utils.app_paths import get_logs_dir

import aiofiles
from pydantic import BaseModel, Field

from evaluation.models import TokenUsage
from logger import get_logger

logger = get_logger(__name__)


class AuditLevel(str, Enum):
    """å®¡è®¡çº§åˆ«"""

    TURN = "turn"  # å•è½®å¯¹è¯
    SESSION = "session"  # å•æ¬¡ä¼šè¯
    CONVERSATION = "conversation"  # æ•´ä¸ªå¯¹è¯
    USER = "user"  # ç”¨æˆ·çº§åˆ«
    AGENT = "agent"  # æ™ºèƒ½ä½“çº§åˆ«


class TokenAuditRecord(BaseModel):
    """Token å®¡è®¡è®°å½•"""

    record_id: str = Field(..., description="è®°å½•å”¯ä¸€æ ‡è¯†")
    level: AuditLevel = Field(..., description="å®¡è®¡çº§åˆ«")

    # å…³è” ID
    session_id: Optional[str] = Field(None, description="ä¼šè¯ ID")
    conversation_id: Optional[str] = Field(None, description="å¯¹è¯ ID")
    user_id: Optional[str] = Field(None, description="ç”¨æˆ· ID")
    agent_id: Optional[str] = Field(None, description="æ™ºèƒ½ä½“ ID")
    turn_number: Optional[int] = Field(None, description="å¯¹è¯è½®æ¬¡")

    # Token ä½¿ç”¨è¯¦æƒ…
    usage: TokenUsage = Field(default_factory=TokenUsage, description="Token ä½¿ç”¨ç»Ÿè®¡")

    # å…ƒæ•°æ®
    model: str = Field("unknown", description="ä½¿ç”¨çš„æ¨¡åž‹")
    timestamp: datetime = Field(default_factory=datetime.now)
    duration_ms: int = Field(0, description="æ‰§è¡Œè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰")

    # ç”¨æˆ·æŸ¥è¯¢ä¿¡æ¯ï¼ˆè„±æ•åŽï¼‰
    query_length: int = Field(0, description="ç”¨æˆ·æŸ¥è¯¢é•¿åº¦")
    query_hash: Optional[str] = Field(None, description="ç”¨æˆ·æŸ¥è¯¢å“ˆå¸Œï¼ˆç”¨äºŽåŽ»é‡ï¼‰")

    # å®‰å…¨æ ‡è®°
    is_anomaly: bool = Field(False, description="æ˜¯å¦ä¸ºå¼‚å¸¸æ¶ˆè€—")
    anomaly_reason: Optional[str] = Field(None, description="å¼‚å¸¸åŽŸå› ")


class TokenAuditStats(BaseModel):
    """Token å®¡è®¡ç»Ÿè®¡"""

    total_records: int = 0

    # æ€»è®¡
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_thinking_tokens: int = 0
    total_cache_read_tokens: int = 0
    total_cache_write_tokens: int = 0

    # å¹³å‡å€¼
    avg_input_tokens: float = 0.0
    avg_output_tokens: float = 0.0
    avg_thinking_tokens: float = 0.0

    # å³°å€¼
    max_input_tokens: int = 0
    max_output_tokens: int = 0
    max_thinking_tokens: int = 0

    # å¼‚å¸¸ç»Ÿè®¡
    anomaly_count: int = 0
    anomaly_rate: float = 0.0

    # æ—¶é—´èŒƒå›´
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None

    @property
    def total_tokens(self) -> int:
        """æ€» Token æ•°"""
        return self.total_input_tokens + self.total_output_tokens + self.total_thinking_tokens

    @property
    def cache_hit_rate(self) -> float:
        """ç¼“å­˜å‘½ä¸­çŽ‡"""
        total_read = self.total_input_tokens
        if total_read == 0:
            return 0.0
        return self.total_cache_read_tokens / total_read


class TokenAuditor:
    """
    Token å®¡è®¡å™¨

    åŠŸèƒ½ï¼š
    1. è®°å½•æ¯æ¬¡æ‰§è¡Œçš„ Token æ¶ˆè€—
    2. å¤šç»´åº¦ç»Ÿè®¡åˆ†æž
    3. å¼‚å¸¸æ£€æµ‹ä¸Žå‘Šè­¦
    4. ä¸Žè¯„ä¼°ç³»ç»Ÿé›†æˆ
    5. è®¡è´¹æ—¥å¿—ï¼ˆJSON Lines æ ¼å¼ï¼ŒæŒ‰ç”¨æˆ·+æ—¥æœŸåˆ†æ–‡ä»¶ï¼‰

    æ³¨æ„ï¼š
    - ä¼ å…¥çš„ usage å¯èƒ½æ˜¯å•æ¬¡ LLM è°ƒç”¨æˆ–æ•´ä¸ªä¼šè¯çš„ç´¯è®¡å€¼
    - å‘Šè­¦é˜ˆå€¼åŒºåˆ†ã€Œå•æ¬¡è°ƒç”¨é˜ˆå€¼ã€å’Œã€Œä¼šè¯ç´¯è®¡é˜ˆå€¼ã€
    """

    def __init__(
        self,
        # å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ï¼ˆå•æ¬¡ LLM è°ƒç”¨ï¼‰
        max_input_tokens_per_call: int = 180_000,  # å•æ¬¡è°ƒç”¨æœ€å¤§è¾“å…¥ Token
        max_output_tokens_per_call: int = 50_000,  # å•æ¬¡è°ƒç”¨æœ€å¤§è¾“å‡º Token
        max_thinking_tokens_per_call: int = 100_000,  # å•æ¬¡è°ƒç”¨æœ€å¤§ Thinking Token
        # å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ï¼ˆä¼šè¯ç´¯è®¡ï¼‰
        max_session_input_tokens: int = 1_000_000,  # ä¼šè¯ç´¯è®¡æœ€å¤§è¾“å…¥ Token
        max_session_output_tokens: int = 200_000,  # ä¼šè¯ç´¯è®¡æœ€å¤§è¾“å‡º Token
        # å­˜å‚¨é…ç½®
        max_records: int = 10_000,  # æœ€å¤§ä¿ç•™è®°å½•æ•°
        enable_persistence: bool = False,  # æ˜¯å¦æŒä¹…åŒ–ï¼ˆé»˜è®¤å†…å­˜ï¼‰
        # è®¡è´¹æ—¥å¿—é…ç½®
        log_dir: str = "",  # æ—¥å¿—ç›®å½•ï¼ˆç©ºåˆ™ä½¿ç”¨ get_logs_dir()/tokensï¼‰
        enable_billing_log: bool = True,  # æ˜¯å¦å¯ç”¨è®¡è´¹æ—¥å¿—
    ):
        # å•æ¬¡è°ƒç”¨é˜ˆå€¼
        self.max_input_tokens_per_call = max_input_tokens_per_call
        self.max_output_tokens_per_call = max_output_tokens_per_call
        self.max_thinking_tokens_per_call = max_thinking_tokens_per_call
        # ä¼šè¯ç´¯è®¡é˜ˆå€¼
        self.max_session_input_tokens = max_session_input_tokens
        self.max_session_output_tokens = max_session_output_tokens

        self.max_records = max_records
        self.enable_persistence = enable_persistence

        # è®¡è´¹æ—¥å¿—é…ç½®
        self.log_dir = Path(log_dir) if log_dir else get_logs_dir() / "tokens"
        self.enable_billing_log = enable_billing_log
        if enable_billing_log:
            self.log_dir.mkdir(parents=True, exist_ok=True)

        # å†…å­˜å­˜å‚¨
        self._records: List[TokenAuditRecord] = []

        # æŒ‰ç»´åº¦ç´¢å¼•ï¼ˆå¿«é€ŸæŸ¥è¯¢ï¼‰
        self._by_session: Dict[str, List[str]] = {}  # session_id -> record_ids
        self._by_conversation: Dict[str, List[str]] = {}  # conversation_id -> record_ids
        self._by_user: Dict[str, List[str]] = {}  # user_id -> record_ids
        self._by_agent: Dict[str, List[str]] = {}  # agent_id -> record_ids

        logger.info(
            f"âœ… TokenAuditor åˆå§‹åŒ–: per_call_input={max_input_tokens_per_call:,}, "
            f"session_input={max_session_input_tokens:,}, "
            f"billing_log={'enabled' if enable_billing_log else 'disabled'}"
        )

    async def record(
        self,
        session_id: str,
        usage: TokenUsage,
        *,
        conversation_id: Optional[str] = None,
        user_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        turn_number: Optional[int] = None,
        model: str = "unknown",
        duration_ms: int = 0,
        query_length: int = 0,
        query_hash: Optional[str] = None,
        is_session_cumulative: bool = True,  # ðŸ†• æ ‡è®°æ˜¯å¦ä¸ºä¼šè¯ç´¯è®¡å€¼
    ) -> TokenAuditRecord:
        """
        è®°å½•ä¸€æ¬¡ Token æ¶ˆè€—

        Args:
            session_id: ä¼šè¯ ID
            usage: Token ä½¿ç”¨ç»Ÿè®¡
            conversation_id: å¯¹è¯ ID
            user_id: ç”¨æˆ· ID
            agent_id: æ™ºèƒ½ä½“ ID
            turn_number: å¯¹è¯è½®æ¬¡
            model: ä½¿ç”¨çš„æ¨¡åž‹
            duration_ms: æ‰§è¡Œè€—æ—¶
            query_length: ç”¨æˆ·æŸ¥è¯¢é•¿åº¦
            query_hash: ç”¨æˆ·æŸ¥è¯¢å“ˆå¸Œ
            is_session_cumulative: æ˜¯å¦ä¸ºä¼šè¯ç´¯è®¡å€¼ï¼ˆé»˜è®¤ Trueï¼‰
                - True: ä½¿ç”¨ä¼šè¯ç´¯è®¡é˜ˆå€¼æ£€æµ‹ï¼ˆé€‚ç”¨äºŽæ•´ä¸ªä¼šè¯çš„ usage æ±‡æ€»ï¼‰
                - False: ä½¿ç”¨å•æ¬¡è°ƒç”¨é˜ˆå€¼æ£€æµ‹ï¼ˆé€‚ç”¨äºŽå•æ¬¡ LLM è°ƒç”¨ï¼‰

        Returns:
            TokenAuditRecord è®°å½•
        """
        record_id = f"audit_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(self._records)}"

        # å¼‚å¸¸æ£€æµ‹ï¼šæ ¹æ® is_session_cumulative é€‰æ‹©é˜ˆå€¼
        is_anomaly = False
        anomaly_reason = None

        if is_session_cumulative:
            # ä¼šè¯ç´¯è®¡å€¼ï¼šä½¿ç”¨ä¼šè¯çº§é˜ˆå€¼ï¼ˆæ›´å®½æ¾ï¼‰
            if usage.input_tokens > self.max_session_input_tokens:
                is_anomaly = True
                anomaly_reason = f"ä¼šè¯ç´¯è®¡è¾“å…¥ Token è¶…é™: {usage.input_tokens:,} > {self.max_session_input_tokens:,}"
            elif usage.output_tokens > self.max_session_output_tokens:
                is_anomaly = True
                anomaly_reason = f"ä¼šè¯ç´¯è®¡è¾“å‡º Token è¶…é™: {usage.output_tokens:,} > {self.max_session_output_tokens:,}"
        else:
            # å•æ¬¡è°ƒç”¨ï¼šä½¿ç”¨å•æ¬¡è°ƒç”¨é˜ˆå€¼ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            if usage.input_tokens > self.max_input_tokens_per_call:
                is_anomaly = True
                anomaly_reason = f"å•æ¬¡è°ƒç”¨è¾“å…¥ Token è¶…é™: {usage.input_tokens:,} > {self.max_input_tokens_per_call:,}"
            elif usage.output_tokens > self.max_output_tokens_per_call:
                is_anomaly = True
                anomaly_reason = f"å•æ¬¡è°ƒç”¨è¾“å‡º Token è¶…é™: {usage.output_tokens:,} > {self.max_output_tokens_per_call:,}"
            elif usage.thinking_tokens > self.max_thinking_tokens_per_call:
                is_anomaly = True
                anomaly_reason = f"Thinking Token è¶…é™: {usage.thinking_tokens:,} > {self.max_thinking_tokens_per_call:,}"

        if is_anomaly:
            logger.warning(f"âš ï¸ Token å¼‚å¸¸å‘Šè­¦: {anomaly_reason} (session={session_id})")

        # åˆ›å»ºè®°å½•
        record = TokenAuditRecord(
            record_id=record_id,
            level=AuditLevel.TURN,
            session_id=session_id,
            conversation_id=conversation_id,
            user_id=user_id,
            agent_id=agent_id,
            turn_number=turn_number,
            usage=usage,
            model=model,
            duration_ms=duration_ms,
            query_length=query_length,
            query_hash=query_hash,
            is_anomaly=is_anomaly,
            anomaly_reason=anomaly_reason,
        )

        # å­˜å‚¨
        self._records.append(record)

        # æ›´æ–°ç´¢å¼•
        if session_id:
            self._by_session.setdefault(session_id, []).append(record_id)
        if conversation_id:
            self._by_conversation.setdefault(conversation_id, []).append(record_id)
        if user_id:
            self._by_user.setdefault(user_id, []).append(record_id)
        if agent_id:
            self._by_agent.setdefault(agent_id, []).append(record_id)

        # é™åˆ¶è®°å½•æ•°
        if len(self._records) > self.max_records:
            self._cleanup_old_records()

        logger.debug(
            f"ðŸ“Š Token è®°å½•: input={usage.input_tokens:,}, output={usage.output_tokens:,}, "
            f"thinking={usage.thinking_tokens:,}, cache_read={usage.cache_read_tokens:,}"
        )

        # å†™å…¥è®¡è´¹æ—¥å¿—ï¼ˆå¼‚æ­¥ï¼‰
        if self.enable_billing_log:
            await self._write_to_log(record)

        return record

    def get_stats(
        self,
        *,
        session_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
        user_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> TokenAuditStats:
        """
        èŽ·å– Token å®¡è®¡ç»Ÿè®¡

        Args:
            session_id: æŒ‰ä¼šè¯ç­›é€‰
            conversation_id: æŒ‰å¯¹è¯ç­›é€‰
            user_id: æŒ‰ç”¨æˆ·ç­›é€‰
            agent_id: æŒ‰æ™ºèƒ½ä½“ç­›é€‰
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´

        Returns:
            TokenAuditStats ç»Ÿè®¡
        """
        # ç­›é€‰è®°å½•
        records = self._filter_records(
            session_id=session_id,
            conversation_id=conversation_id,
            user_id=user_id,
            agent_id=agent_id,
            start_time=start_time,
            end_time=end_time,
        )

        if not records:
            return TokenAuditStats()

        # è®¡ç®—ç»Ÿè®¡
        total_input = sum(r.usage.input_tokens for r in records)
        total_output = sum(r.usage.output_tokens for r in records)
        total_thinking = sum(r.usage.thinking_tokens for r in records)
        total_cache_read = sum(r.usage.cache_read_tokens for r in records)
        total_cache_write = sum(r.usage.cache_write_tokens for r in records)

        max_input = max(r.usage.input_tokens for r in records)
        max_output = max(r.usage.output_tokens for r in records)
        max_thinking = max(r.usage.thinking_tokens for r in records)

        anomaly_count = sum(1 for r in records if r.is_anomaly)

        n = len(records)

        return TokenAuditStats(
            total_records=n,
            total_input_tokens=total_input,
            total_output_tokens=total_output,
            total_thinking_tokens=total_thinking,
            total_cache_read_tokens=total_cache_read,
            total_cache_write_tokens=total_cache_write,
            avg_input_tokens=total_input / n,
            avg_output_tokens=total_output / n,
            avg_thinking_tokens=total_thinking / n,
            max_input_tokens=max_input,
            max_output_tokens=max_output,
            max_thinking_tokens=max_thinking,
            anomaly_count=anomaly_count,
            anomaly_rate=anomaly_count / n,
            start_time=min(r.timestamp for r in records),
            end_time=max(r.timestamp for r in records),
        )

    def get_comparison(self, agent_ids: List[str]) -> Dict[str, TokenAuditStats]:
        """
        æ¯”è¾ƒå¤šä¸ªæ™ºèƒ½ä½“çš„ Token æ¶ˆè€—

        Args:
            agent_ids: æ™ºèƒ½ä½“ ID åˆ—è¡¨

        Returns:
            Dict[agent_id, TokenAuditStats]
        """
        result = {}
        for agent_id in agent_ids:
            result[agent_id] = self.get_stats(agent_id=agent_id)
        return result

    def export_for_evaluation(self, session_id: str) -> Dict[str, Any]:
        """
        å¯¼å‡ºè¯„ä¼°ç³»ç»Ÿæ‰€éœ€çš„ Token æ•°æ®

        Args:
            session_id: ä¼šè¯ ID

        Returns:
            ä¸Ž evaluation/models.py TokenUsage å…¼å®¹çš„å­—å…¸
        """
        stats = self.get_stats(session_id=session_id)

        return {
            "input_tokens": stats.total_input_tokens,
            "output_tokens": stats.total_output_tokens,
            "thinking_tokens": stats.total_thinking_tokens,
            "cache_read_tokens": stats.total_cache_read_tokens,
            "cache_write_tokens": stats.total_cache_write_tokens,
            "total_tokens": stats.total_tokens,
            "cache_hit_rate": stats.cache_hit_rate,
            "anomaly_count": stats.anomaly_count,
            "anomaly_rate": stats.anomaly_rate,
        }

    def _filter_records(
        self,
        *,
        session_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
        user_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> List[TokenAuditRecord]:
        """ç­›é€‰è®°å½•"""
        records = self._records

        if session_id:
            record_ids = set(self._by_session.get(session_id, []))
            records = [r for r in records if r.record_id in record_ids]

        if conversation_id:
            record_ids = set(self._by_conversation.get(conversation_id, []))
            records = [r for r in records if r.record_id in record_ids]

        if user_id:
            record_ids = set(self._by_user.get(user_id, []))
            records = [r for r in records if r.record_id in record_ids]

        if agent_id:
            record_ids = set(self._by_agent.get(agent_id, []))
            records = [r for r in records if r.record_id in record_ids]

        if start_time:
            records = [r for r in records if r.timestamp >= start_time]

        if end_time:
            records = [r for r in records if r.timestamp <= end_time]

        return records

    def _cleanup_old_records(self):
        """æ¸…ç†æ—§è®°å½•ï¼ˆä¿ç•™æœ€è¿‘çš„ max_records æ¡ï¼‰"""
        if len(self._records) <= self.max_records:
            return

        # æŒ‰æ—¶é—´æŽ’åºï¼Œä¿ç•™æœ€æ–°çš„
        self._records.sort(key=lambda r: r.timestamp, reverse=True)
        removed = self._records[self.max_records :]
        self._records = self._records[: self.max_records]

        # æ›´æ–°ç´¢å¼•
        removed_ids = {r.record_id for r in removed}

        for key in list(self._by_session.keys()):
            self._by_session[key] = [rid for rid in self._by_session[key] if rid not in removed_ids]
            if not self._by_session[key]:
                del self._by_session[key]

        # åŒæ ·å¤„ç†å…¶ä»–ç´¢å¼•...
        logger.info(f"ðŸ§¹ æ¸…ç†æ—§å®¡è®¡è®°å½•: ç§»é™¤ {len(removed)} æ¡")

 

# ============================================================
# å…¨å±€å®žä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
# ============================================================

_auditor: Optional[TokenAuditor] = None


def get_token_auditor() -> TokenAuditor:
    """èŽ·å–å…¨å±€ Token å®¡è®¡å™¨å®žä¾‹"""
    global _auditor
    if _auditor is None:
        _auditor = TokenAuditor()
    return _auditor


def create_token_auditor(**kwargs) -> TokenAuditor:
    """åˆ›å»ºæ–°çš„ Token å®¡è®¡å™¨å®žä¾‹"""
    return TokenAuditor(**kwargs)
