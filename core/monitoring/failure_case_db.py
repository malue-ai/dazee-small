"""
å¤±è´¥æ¡ˆä¾‹åº“ï¼ˆFailure Case DBï¼‰

å­˜å‚¨å’Œç®¡ç†å¤±è´¥æ¡ˆä¾‹ï¼Œæ”¯æŒï¼š
1. æŒä¹…åŒ–å­˜å‚¨ï¼ˆæ–‡ä»¶/æ•°æ®åº“ï¼‰
2. æŸ¥è¯¢å’Œæ£€ç´¢
3. å¯¼å‡ºä¸ºè¯„ä¼°ä»»åŠ¡
4. ç»Ÿè®¡åˆ†æ
"""

import asyncio
import json
import os
from dataclasses import asdict
from datetime import datetime, timedelta

from utils.app_paths import get_user_data_dir
from pathlib import Path
from typing import Any, Dict, List, Optional

import aiofiles

from core.monitoring.failure_detector import FailureCase, FailureSeverity, FailureType
from logger import get_logger

logger = get_logger(__name__)


class FailureCaseDB:
    """
    å¤±è´¥æ¡ˆä¾‹æ•°æ®åº“

    ä½¿ç”¨æ–¹å¼ï¼š
        db = FailureCaseDB(storage_path="data/failure_cases")

        # ä¿å­˜æ¡ˆä¾‹
        db.save(failure_case)

        # æŸ¥è¯¢æ¡ˆä¾‹
        cases = db.query(failure_type=FailureType.CONTEXT_OVERFLOW)

        # å¯¼å‡ºä¸ºè¯„ä¼°ä»»åŠ¡
        tasks = db.export_as_eval_tasks(case_ids=["case_001", "case_002"])
    """

    def __init__(self, storage_path: str = "", retention_days: int = 30):
        """
        åˆå§‹åŒ–å¤±è´¥æ¡ˆä¾‹æ•°æ®åº“

        Args:
            storage_path: å­˜å‚¨è·¯å¾„
            retention_days: ä¿ç•™å¤©æ•°

        æ³¨æ„ï¼šéœ€è¦è°ƒç”¨ await initialize() å®Œæˆå¼‚æ­¥åˆå§‹åŒ–
        """
        self.storage_path = Path(storage_path) if storage_path else get_user_data_dir() / "data" / "failure_cases"
        self.retention_days = retention_days

        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        self.storage_path.mkdir(parents=True, exist_ok=True)

        # å†…å­˜ç¼“å­˜
        self._cache: Dict[str, FailureCase] = {}

        # åˆå§‹åŒ–æ ‡è®°
        self._initialized: bool = False

    async def initialize(self) -> None:
        """
        å¼‚æ­¥åˆå§‹åŒ–ï¼šåŠ è½½ç°æœ‰æ¡ˆä¾‹

        ä½¿ç”¨æ–¹å¼ï¼š
            db = FailureCaseDB()
            await db.initialize()
        """
        if self._initialized:
            return

        await self._load_all_async()
        self._initialized = True

    # ===================
    # å­˜å‚¨æ“ä½œ
    # ===================

    async def save(self, case: FailureCase) -> None:
        """
        ä¿å­˜å¤±è´¥æ¡ˆä¾‹ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            case: å¤±è´¥æ¡ˆä¾‹
        """
        # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
        self._cache[case.id] = case

        # æŒä¹…åŒ–åˆ°æ–‡ä»¶ï¼ˆå¼‚æ­¥ï¼‰
        file_path = self._get_file_path(case.id)
        content = json.dumps(case.to_dict(), ensure_ascii=False, indent=2)

        async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
            await f.write(content)

        logger.debug(f"ğŸ’¾ ä¿å­˜å¤±è´¥æ¡ˆä¾‹: {case.id}")

    def delete(self, case_id: str) -> bool:
        """
        åˆ é™¤å¤±è´¥æ¡ˆä¾‹

        Args:
            case_id: æ¡ˆä¾‹ID

        Returns:
            bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        # ä»ç¼“å­˜åˆ é™¤
        if case_id in self._cache:
            del self._cache[case_id]

        # ä»æ–‡ä»¶åˆ é™¤
        file_path = self._get_file_path(case_id)
        if file_path.exists():
            file_path.unlink()
            logger.debug(f"ğŸ—‘ï¸ åˆ é™¤å¤±è´¥æ¡ˆä¾‹: {case_id}")
            return True

        return False

    def get(self, case_id: str) -> Optional[FailureCase]:
        """
        è·å–å¤±è´¥æ¡ˆä¾‹

        Args:
            case_id: æ¡ˆä¾‹ID

        Returns:
            FailureCase: å¤±è´¥æ¡ˆä¾‹
        """
        return self._cache.get(case_id)

    async def update(self, case: FailureCase) -> None:
        """
        æ›´æ–°å¤±è´¥æ¡ˆä¾‹ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            case: å¤±è´¥æ¡ˆä¾‹
        """
        await self.save(case)

    # ===================
    # æŸ¥è¯¢æ“ä½œ
    # ===================

    def query(
        self,
        failure_type: Optional[FailureType] = None,
        severity: Optional[FailureSeverity] = None,
        status: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
        user_id: Optional[str] = None,
        limit: int = 100,
        offset: int = 0,
    ) -> List[FailureCase]:
        """
        æŸ¥è¯¢å¤±è´¥æ¡ˆä¾‹

        Args:
            failure_type: å¤±è´¥ç±»å‹
            severity: ä¸¥é‡ç¨‹åº¦
            status: çŠ¶æ€
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            user_id: ç”¨æˆ·ID
            limit: è¿”å›æ•°é‡é™åˆ¶
            offset: åç§»é‡

        Returns:
            List[FailureCase]: å¤±è´¥æ¡ˆä¾‹åˆ—è¡¨
        """
        cases = list(self._cache.values())

        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if failure_type:
            cases = [c for c in cases if c.failure_type == failure_type]

        if severity:
            cases = [c for c in cases if c.severity == severity]

        if status:
            cases = [c for c in cases if c.status == status]

        if start_date:
            cases = [c for c in cases if c.timestamp >= start_date]

        if end_date:
            cases = [c for c in cases if c.timestamp <= end_date]

        if user_id:
            cases = [c for c in cases if c.user_id == user_id]

        # æŒ‰æ—¶é—´å€’åºæ’åº
        cases.sort(key=lambda c: c.timestamp, reverse=True)

        # åˆ†é¡µ
        return cases[offset : offset + limit]

    def count(
        self,
        failure_type: Optional[FailureType] = None,
        severity: Optional[FailureSeverity] = None,
        status: Optional[str] = None,
    ) -> int:
        """
        ç»Ÿè®¡å¤±è´¥æ¡ˆä¾‹æ•°é‡

        Args:
            failure_type: å¤±è´¥ç±»å‹
            severity: ä¸¥é‡ç¨‹åº¦
            status: çŠ¶æ€

        Returns:
            int: æ•°é‡
        """
        cases = self.query(
            failure_type=failure_type,
            severity=severity,
            status=status,
            limit=100000,  # å¤§æ•°å€¼è·å–å…¨éƒ¨
        )
        return len(cases)

    def get_pending_review(self, limit: int = 100) -> List[FailureCase]:
        """
        è·å–å¾…å®¡æŸ¥çš„æ¡ˆä¾‹

        Args:
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            List[FailureCase]: å¾…å®¡æŸ¥æ¡ˆä¾‹åˆ—è¡¨
        """
        return self.query(status="new", limit=limit)

    # ===================
    # å¯¼å‡ºæ“ä½œ
    # ===================

    def export_as_eval_tasks(
        self,
        case_ids: Optional[List[str]] = None,
        failure_types: Optional[List[FailureType]] = None,
        limit: int = 50,
    ) -> List[Dict[str, Any]]:
        """
        å¯¼å‡ºä¸ºè¯„ä¼°ä»»åŠ¡ï¼ˆYAMLæ ¼å¼çš„å­—å…¸ï¼‰

        Args:
            case_ids: æŒ‡å®šæ¡ˆä¾‹IDåˆ—è¡¨
            failure_types: å¤±è´¥ç±»å‹åˆ—è¡¨
            limit: æ•°é‡é™åˆ¶

        Returns:
            List[Dict]: è¯„ä¼°ä»»åŠ¡åˆ—è¡¨ï¼ˆå¯åºåˆ—åŒ–ä¸ºYAMLï¼‰
        """
        if case_ids:
            cases = [self.get(cid) for cid in case_ids if self.get(cid)]
        elif failure_types:
            cases = []
            for ft in failure_types:
                cases.extend(self.query(failure_type=ft, limit=limit))
            cases = cases[:limit]
        else:
            cases = self.query(limit=limit)

        tasks = []
        for case in cases:
            task = {
                "id": f"regression_{case.id}",
                "description": f"å›å½’æµ‹è¯•: {case.error_message[:100]}",
                "category": "regression",
                "source_case_id": case.id,
                "input": {
                    "user_query": case.user_query,
                    "conversation_history": case.conversation_history,
                    "context": case.context,
                },
                "expected_outcome": {
                    # æ ¹æ®å¤±è´¥ç±»å‹ç”Ÿæˆé¢„æœŸç»“æœ
                    "should_not_fail": True,
                    "original_failure_type": case.failure_type.value,
                },
                "graders": self._generate_graders_for_case(case),
                "trials": 3,
                "timeout_seconds": 60,
                "tags": ["regression", case.failure_type.value],
                "metadata": {
                    "source_case": case.id,
                    "original_timestamp": case.timestamp.isoformat(),
                    "severity": case.severity.value,
                },
            }
            tasks.append(task)

        return tasks

    def _generate_graders_for_case(self, case: FailureCase) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å¤±è´¥ç±»å‹ç”Ÿæˆè¯„åˆ†å™¨é…ç½®

        Args:
            case: å¤±è´¥æ¡ˆä¾‹

        Returns:
            List[Dict]: è¯„åˆ†å™¨é…ç½®åˆ—è¡¨
        """
        graders = []

        # é€šç”¨è¯„åˆ†å™¨ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨é”™è¯¯
        graders.append(
            {
                "type": "code",
                "name": "check_no_tool_errors",
                "check": "check_no_tool_errors()",
            }
        )

        # æ ¹æ®å¤±è´¥ç±»å‹æ·»åŠ ç‰¹å®šè¯„åˆ†å™¨
        if case.failure_type == FailureType.CONTEXT_OVERFLOW:
            max_tokens = case.token_usage.get("max", 200000)
            graders.append(
                {
                    "type": "code",
                    "name": "check_token_limit",
                    "check": f"check_token_limit({max_tokens})",
                }
            )

        elif case.failure_type == FailureType.TIMEOUT:
            timeout = case.context.get("timeout_seconds", 60) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            graders.append(
                {
                    "type": "code",
                    "name": "check_execution_time",
                    "check": f"check_execution_time({timeout})",
                }
            )

        elif case.failure_type == FailureType.USER_NEGATIVE_FEEDBACK:
            graders.append(
                {
                    "type": "model",
                    "rubric": "grade_response_quality",
                    "min_score": 4,
                }
            )

        elif case.failure_type == FailureType.INTENT_MISMATCH:
            graders.append(
                {
                    "type": "model",
                    "rubric": "grade_intent_understanding",
                    "min_score": 4,
                }
            )

        return graders

    async def export_to_yaml_file(
        self,
        output_path: str,
        case_ids: Optional[List[str]] = None,
        failure_types: Optional[List[FailureType]] = None,
    ) -> str:
        """
        å¯¼å‡ºä¸ºYAMLæ–‡ä»¶ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            case_ids: æŒ‡å®šæ¡ˆä¾‹IDåˆ—è¡¨
            failure_types: å¤±è´¥ç±»å‹åˆ—è¡¨

        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import yaml

        tasks = self.export_as_eval_tasks(case_ids=case_ids, failure_types=failure_types)

        suite = {
            "id": f"regression_suite_{datetime.now().strftime('%Y%m%d')}",
            "name": "å›å½’æµ‹è¯•å¥—ä»¶ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰",
            "description": "ä»å¤±è´¥æ¡ˆä¾‹è‡ªåŠ¨ç”Ÿæˆçš„å›å½’æµ‹è¯•",
            "category": "regression",
            "default_trials": 3,
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "source": "failure_case_db",
                "case_count": len(tasks),
            },
            "tasks": tasks,
        }

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        content = yaml.dump(suite, allow_unicode=True, default_flow_style=False)
        async with aiofiles.open(output_file, "w", encoding="utf-8") as f:
            await f.write(content)

        logger.info(f"ğŸ“„ å¯¼å‡ºå›å½’æµ‹è¯•å¥—ä»¶: {output_path} ({len(tasks)} ä¸ªä»»åŠ¡)")

        return str(output_file)

    # ===================
    # ç»Ÿè®¡åˆ†æ
    # ===================

    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        cases = list(self._cache.values())

        # æŒ‰ç±»å‹ç»Ÿè®¡
        by_type = {}
        for ft in FailureType:
            by_type[ft.value] = sum(1 for c in cases if c.failure_type == ft)

        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        by_severity = {}
        for s in FailureSeverity:
            by_severity[s.value] = sum(1 for c in cases if c.severity == s)

        # æŒ‰çŠ¶æ€ç»Ÿè®¡
        statuses = set(c.status for c in cases)
        by_status = {s: sum(1 for c in cases if c.status == s) for s in statuses}

        # æ—¶é—´åˆ†å¸ƒï¼ˆæœ€è¿‘7å¤©ï¼‰
        daily_counts = {}
        for i in range(7):
            date = datetime.now() - timedelta(days=i)
            date_str = date.strftime("%Y-%m-%d")
            daily_counts[date_str] = sum(
                1 for c in cases if c.timestamp.strftime("%Y-%m-%d") == date_str
            )

        return {
            "total_cases": len(cases),
            "by_type": by_type,
            "by_severity": by_severity,
            "by_status": by_status,
            "daily_counts": daily_counts,
            "pending_review": by_status.get("new", 0),
            "converted_to_tasks": by_status.get("converted", 0),
        }

    # ===================
    # æ¸…ç†æ“ä½œ
    # ===================

    def cleanup_old_cases(self) -> int:
        """
        æ¸…ç†è¿‡æœŸæ¡ˆä¾‹

        Returns:
            int: æ¸…ç†çš„æ¡ˆä¾‹æ•°é‡
        """
        cutoff = datetime.now() - timedelta(days=self.retention_days)

        deleted_count = 0
        for case_id, case in list(self._cache.items()):
            if case.timestamp < cutoff and case.status in ["resolved", "converted"]:
                self.delete(case_id)
                deleted_count += 1

        if deleted_count > 0:
            logger.info(f"ğŸ§¹ æ¸…ç†äº† {deleted_count} ä¸ªè¿‡æœŸæ¡ˆä¾‹")

        return deleted_count

    # ===================
    # å†…éƒ¨æ–¹æ³•
    # ===================

    def _get_file_path(self, case_id: str) -> Path:
        """è·å–æ¡ˆä¾‹æ–‡ä»¶è·¯å¾„"""
        return self.storage_path / f"{case_id}.json"

    async def _load_all_async(self) -> None:
        """å¼‚æ­¥åŠ è½½æ‰€æœ‰æ¡ˆä¾‹åˆ°å†…å­˜"""
        # ä½¿ç”¨ asyncio.to_thread åŒ…è£…åŒæ­¥çš„ glob æ“ä½œ
        file_paths = await asyncio.to_thread(list, self.storage_path.glob("*.json"))
        for file_path in file_paths:
            try:
                async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
                    content = await f.read()
                    data = json.loads(content)

                case = self._dict_to_case(data)
                self._cache[case.id] = case

            except Exception as e:
                logger.error(f"åŠ è½½æ¡ˆä¾‹å¤±è´¥ {file_path}: {e}")

        logger.info(f"ğŸ“‚ åŠ è½½äº† {len(self._cache)} ä¸ªå¤±è´¥æ¡ˆä¾‹")

    def _dict_to_case(self, data: Dict[str, Any]) -> FailureCase:
        """å­—å…¸è½¬æ¢ä¸ºFailureCase"""
        return FailureCase(
            id=data["id"],
            failure_type=FailureType(data["failure_type"]),
            severity=FailureSeverity(data["severity"]),
            conversation_id=data["conversation_id"],
            user_id=data.get("user_id"),
            timestamp=datetime.fromisoformat(data["timestamp"]),
            user_query=data.get("user_query", ""),
            conversation_history=data.get("conversation_history", []),
            error_message=data.get("error_message", ""),
            stack_trace=data.get("stack_trace"),
            tool_calls=data.get("tool_calls", []),
            agent_response=data.get("agent_response", ""),
            token_usage=data.get("token_usage", {}),
            context=data.get("context", {}),
            status=data.get("status", "new"),
            reviewed_by=data.get("reviewed_by"),
            reviewed_at=(
                datetime.fromisoformat(data["reviewed_at"]) if data.get("reviewed_at") else None
            ),
        )
