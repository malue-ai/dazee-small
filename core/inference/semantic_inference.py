"""
V5.0 ç»Ÿä¸€è¯­ä¹‰æ¨ç†æ¨¡å—

æ ¸å¿ƒç†å¿µï¼š
- æ‰€æœ‰æ¨ç†éƒ½é€šè¿‡ LLM è¯­ä¹‰ç†è§£å®Œæˆ
- ä½¿ç”¨ Few-Shot æç¤ºè¯æ•™ä¼š LLM æ¨ç†æ¨¡å¼
- ä»£ç åªåšè°ƒç”¨å’Œè§£æï¼Œä¸åšè§„åˆ™åˆ¤æ–­
- ä¿å®ˆçš„ fallbackï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œä¸åšå…³é”®è¯çŒœæµ‹

è®¾è®¡åŸåˆ™ï¼š
- è¿è¥æ— éœ€é…ç½®ä»»ä½•æ¨ç†è§„åˆ™
- æ¡†æ¶å†…ç½® Few-Shot ç¤ºä¾‹
- å¯¹è¿è¥å®Œå…¨é€æ˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·è¾“å…¥ â†’ LLM è¯­ä¹‰æ¨ç† â†’ ç»“æ„åŒ–ç»“æœ                    â”‚
â”‚                                                        â”‚
â”‚  Few-Shot ç¤ºä¾‹æ•™ä¼š LLM:                                 â”‚
â”‚  â€¢ "æ„é€ CRMç³»ç»Ÿ" â†’ Complex (build + å®Œæ•´æ¶æ„)           â”‚
â”‚  â€¢ "è¿™ä¸ªç³»ç»Ÿæ€ä¹ˆç”¨" â†’ Simple (è¯¢é—®ï¼Œéæ„å»º)             â”‚
â”‚  â€¢ "åˆ†æé”€å”®è¶‹åŠ¿" â†’ Medium (åˆ†æï¼Œå•ä¸€ä»»åŠ¡)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import asyncio
import json
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional

from logger import get_logger

logger = get_logger("semantic_inference")


# ============================================================
# æ¨ç†ç±»å‹æšä¸¾
# ============================================================


class InferenceType(str, Enum):
    """æ¨ç†ç±»å‹"""

    COMPLEXITY = "complexity"  # å¤æ‚åº¦æ¨ç†
    INTENT = "intent"  # æ„å›¾æ¨ç†
    CAPABILITY = "capability"  # èƒ½åŠ›æ¨ç†
    SCHEMA = "schema"  # Schema æ¨ç†


# ============================================================
# æ¨ç†ç»“æœæ•°æ®ç±»
# ============================================================


@dataclass
class InferenceResult:
    """æ¨ç†ç»“æœ"""

    inference_type: InferenceType
    result: Dict[str, Any]
    confidence: float = 1.0
    reasoning: str = ""
    is_fallback: bool = False  # æ˜¯å¦ä½¿ç”¨äº† fallback


# ============================================================
# Few-Shot æç¤ºè¯ï¼ˆæ¡†æ¶å†…ç½®ï¼Œè¿è¥æ— éœ€é…ç½®ï¼‰
# ============================================================

COMPLEXITY_FEW_SHOT = """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡å¤æ‚åº¦åˆ†æä¸“å®¶ã€‚åˆ†æç”¨æˆ·çš„è¯·æ±‚ï¼Œåˆ¤æ–­å…¶å¤æ‚åº¦çº§åˆ«ã€‚

## å¤æ‚åº¦å®šä¹‰

- **simple**: å•ä¸€é—®ç­”ã€ç®€å•æŸ¥è¯¢ã€æ‰“æ‹›å‘¼ã€è·å–åŸºæœ¬ä¿¡æ¯ã€‚æ— éœ€å¤šæ­¥éª¤å¤„ç†ã€‚
- **medium**: éœ€è¦åˆ†æã€å¯¹æ¯”ã€ç”ŸæˆæŠ¥å‘Šæˆ–æ–‡æ¡£ã€æä¾›å»ºè®®ã€‚éœ€è¦ 3-5 æ­¥å¤„ç†ã€‚
- **complex**: ç³»ç»Ÿè®¾è®¡ã€æ¶æ„è§„åˆ’ã€ä¸šåŠ¡æµç¨‹æ„å»ºã€å¤šå®ä½“å…³ç³»å»ºæ¨¡ã€‚éœ€è¦å®Œæ•´è§„åˆ’å’Œå¤šè½®è¿­ä»£ã€‚

## å­¦ä¹ ç¤ºä¾‹ï¼ˆç†è§£è¯­ä¹‰ï¼Œä¸æ˜¯åŒ¹é…å…³é”®è¯ï¼‰

### ç¤ºä¾‹ 1
ç”¨æˆ·: "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
åˆ†æ: å•ä¸€ä¿¡æ¯æŸ¥è¯¢ï¼Œç›´æ¥è·å–å³å¯
è¾“å‡º: {"complexity": "simple", "reasoning": "å¤©æ°”æŸ¥è¯¢ï¼Œå•æ­¥å®Œæˆ"}

### ç¤ºä¾‹ 2
ç”¨æˆ·: "æ„é€ CRMç³»ç»Ÿ"
åˆ†æ: "æ„é€ "æ˜¯ build åŠ¨ä½œï¼Œ"ç³»ç»Ÿ"è¡¨ç¤ºå®Œæ•´æ¶æ„ï¼Œéœ€è¦å¤šè½®è®¾è®¡å’Œå»ºæ¨¡
è¾“å‡º: {"complexity": "complex", "reasoning": "ç³»ç»Ÿæ„å»ºï¼Œéœ€è¦éœ€æ±‚åˆ†æã€æ¶æ„è®¾è®¡ã€å®ä½“å»ºæ¨¡ç­‰å¤šä¸ªé˜¶æ®µ"}

### ç¤ºä¾‹ 3
ç”¨æˆ·: "è¿™ä¸ªç³»ç»Ÿæ€ä¹ˆç”¨ï¼Ÿ"
åˆ†æ: è™½ç„¶æåˆ°"ç³»ç»Ÿ"ï¼Œä½†è¿™æ˜¯è¯¢é—®ä½¿ç”¨æ–¹æ³•ï¼Œä¸æ˜¯æ„å»º
è¾“å‡º: {"complexity": "simple", "reasoning": "è¯¢é—®ä½¿ç”¨æ–¹æ³•ï¼Œç›´æ¥å›ç­”å³å¯"}

### ç¤ºä¾‹ 4
ç”¨æˆ·: "å¸®æˆ‘åˆ†æä¸€ä¸‹é”€å”®æ•°æ®è¶‹åŠ¿"
åˆ†æ: éœ€è¦è·å–æ•°æ®ã€åˆ†æè¶‹åŠ¿ã€æ€»ç»“æ´å¯Ÿï¼Œä½†æ˜¯å•ä¸€åˆ†æä»»åŠ¡
è¾“å‡º: {"complexity": "medium", "reasoning": "æ•°æ®åˆ†æä»»åŠ¡ï¼Œéœ€è¦è·å–æ•°æ®å’Œåˆ†æï¼Œä½†éç³»ç»Ÿçº§"}

### ç¤ºä¾‹ 5
ç”¨æˆ·: "åšä¸ªç®€å•çš„è‡ªæˆ‘ä»‹ç»PPT"
åˆ†æ: è™½ç„¶æ¶‰åŠ PPT ç”Ÿæˆï¼Œä½†"ç®€å•"é™å®šäº†èŒƒå›´ï¼Œä¸éœ€è¦æ·±åº¦è§„åˆ’
è¾“å‡º: {"complexity": "medium", "reasoning": "PPT ç”Ÿæˆä½†èŒƒå›´ç®€å•ï¼Œä¸­ç­‰å¤æ‚åº¦"}

### ç¤ºä¾‹ 6
ç”¨æˆ·: "è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ç”µå•†å¹³å°"
åˆ†æ: å®Œæ•´å¹³å°è®¾è®¡ï¼Œæ¶‰åŠå¤šä¸ªå­ç³»ç»Ÿã€å¤šå®ä½“å…³ç³»ã€ä¸šåŠ¡æµç¨‹
è¾“å‡º: {"complexity": "complex", "reasoning": "å®Œæ•´å¹³å°è®¾è®¡ï¼Œéœ€è¦å…¨é¢è§„åˆ’å’Œå¤šè½®è¿­ä»£"}

### ç¤ºä¾‹ 7
ç”¨æˆ·: "ä½ å¥½"
åˆ†æ: æ‰“æ‹›å‘¼ï¼Œé—²èŠ
è¾“å‡º: {"complexity": "simple", "reasoning": "é—²èŠé—®å€™"}

### ç¤ºä¾‹ 8
ç”¨æˆ·: "å¸®æˆ‘å†™ä¸€ä»½å¸‚åœºè°ƒç ”æŠ¥å‘Š"
åˆ†æ: éœ€è¦è°ƒç ”ã€æ”¶é›†ä¿¡æ¯ã€åˆ†æã€æ’°å†™ï¼Œä½†æ˜¯å•ä¸€ä»»åŠ¡
è¾“å‡º: {"complexity": "medium", "reasoning": "æŠ¥å‘Šæ’°å†™ï¼Œéœ€è¦å¤šæ­¥ä½†éç³»ç»Ÿè®¾è®¡"}

## ç°åœ¨åˆ†æ

ç”¨æˆ·: "{query}"

è¯·ç›´æ¥è¾“å‡º JSONï¼ˆä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š"""


INTENT_FEW_SHOT = """ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·æ„å›¾åˆ†æä¸“å®¶ã€‚åˆ†æç”¨æˆ·çš„è¯·æ±‚ï¼Œè¯†åˆ«å…¶ä»»åŠ¡ç±»å‹ã€‚

## ä»»åŠ¡ç±»å‹å®šä¹‰

- **information_query**: ä¿¡æ¯æŸ¥è¯¢ã€æœç´¢ã€è·å–çŸ¥è¯†
- **content_generation**: ç”Ÿæˆæ–‡æ¡£ã€æŠ¥å‘Šã€PPTã€ä»£ç ç­‰å†…å®¹
- **data_analysis**: æ•°æ®åˆ†æã€ç»Ÿè®¡ã€å¯è§†åŒ–
- **system_design**: ç³»ç»Ÿè®¾è®¡ã€æ¶æ„è§„åˆ’ã€ä¸šåŠ¡å»ºæ¨¡
- **conversation**: é—²èŠã€é—®å€™ã€éä»»åŠ¡å¯¹è¯
- **task_execution**: æ‰§è¡Œå…·ä½“æ“ä½œã€è¿è¡Œå‘½ä»¤

## å­¦ä¹ ç¤ºä¾‹

### ç¤ºä¾‹ 1
ç”¨æˆ·: "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
åˆ†æ: è·å–å¤©æ°”ä¿¡æ¯
è¾“å‡º: {"task_type": "information_query", "reasoning": "æŸ¥è¯¢å¤©æ°”ä¿¡æ¯"}

### ç¤ºä¾‹ 2
ç”¨æˆ·: "å¸®æˆ‘å†™ä¸€ä»½å‘¨æŠ¥"
åˆ†æ: ç”Ÿæˆæ–‡æ¡£å†…å®¹
è¾“å‡º: {"task_type": "content_generation", "reasoning": "ç”Ÿæˆå‘¨æŠ¥æ–‡æ¡£"}

### ç¤ºä¾‹ 3
ç”¨æˆ·: "åˆ†æè¿™ä»½ Excel ä¸­çš„é”€å”®è¶‹åŠ¿"
åˆ†æ: æ•°æ®åˆ†æä»»åŠ¡
è¾“å‡º: {"task_type": "data_analysis", "reasoning": "åˆ†æé”€å”®æ•°æ®"}

### ç¤ºä¾‹ 4
ç”¨æˆ·: "è®¾è®¡ä¸€ä¸ªåº“å­˜ç®¡ç†ç³»ç»Ÿ"
åˆ†æ: ç³»ç»Ÿè®¾è®¡ä»»åŠ¡
è¾“å‡º: {"task_type": "system_design", "reasoning": "è®¾è®¡ç®¡ç†ç³»ç»Ÿ"}

### ç¤ºä¾‹ 5
ç”¨æˆ·: "ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ"
åˆ†æ: é—²èŠé—®å€™
è¾“å‡º: {"task_type": "conversation", "reasoning": "æ—¥å¸¸é—®å€™"}

## ç°åœ¨åˆ†æ

ç”¨æˆ·: "{query}"

è¯·ç›´æ¥è¾“å‡º JSONï¼ˆä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š"""


CAPABILITY_FEW_SHOT = """ä½ æ˜¯ä¸€ä¸ªå·¥å…·èƒ½åŠ›åˆ†æä¸“å®¶ã€‚æ ¹æ®å·¥å…·çš„åç§°å’Œæè¿°ï¼Œæ¨æ–­å®ƒèƒ½å¤Ÿæ»¡è¶³çš„ç”¨æˆ·æ„å›¾ç±»åˆ«ã€‚

## èƒ½åŠ›ç±»åˆ«

- **document_creation**: åˆ›å»ºæ–‡æ¡£ã€å›¾è¡¨ã€æµç¨‹å›¾
- **ppt_generation**: ç”Ÿæˆæ¼”ç¤ºæ–‡ç¨¿
- **web_search**: ç½‘ç»œæœç´¢ã€ä¿¡æ¯æ£€ç´¢
- **data_analysis**: æ•°æ®åˆ†æã€ç»Ÿè®¡è®¡ç®—
- **image_generation**: ç”Ÿæˆå›¾ç‰‡
- **code_execution**: æ‰§è¡Œä»£ç ã€è„šæœ¬
- **notification**: å‘é€é€šçŸ¥ã€é‚®ä»¶
- **crm_integration**: CRM ç³»ç»Ÿé›†æˆ

## å­¦ä¹ ç¤ºä¾‹

### ç¤ºä¾‹ 1
å·¥å…·: {"name": "diagram_skill", "description": "å°†æ–‡æœ¬æè¿°è½¬æ¢ä¸ºæµç¨‹å›¾æˆ–å›¾è¡¨"}
åˆ†æ: ç”Ÿæˆæµç¨‹å›¾æ˜¯æ–‡æ¡£/å›¾è¡¨åˆ›å»ºèƒ½åŠ›
è¾“å‡º: {"capabilities": ["document_creation"], "reasoning": "ç”Ÿæˆæµç¨‹å›¾å±äºæ–‡æ¡£åˆ›å»º"}

### ç¤ºä¾‹ 2
å·¥å…·: {"name": "search_skill", "description": "é€šè¿‡æœç´¢ç±» Skill è¿›è¡Œäº’è”ç½‘ä¿¡æ¯æ£€ç´¢"}
åˆ†æ: ç½‘ç»œæœç´¢èƒ½åŠ›
è¾“å‡º: {"capabilities": ["web_search"], "reasoning": "äº’è”ç½‘æœç´¢"}

### ç¤ºä¾‹ 3
å·¥å…·: {"name": "ppt_skill", "description": "æ ¹æ®é…ç½®ç”Ÿæˆ PPT æ¼”ç¤ºæ–‡ç¨¿"}
åˆ†æ: PPT ç”Ÿæˆèƒ½åŠ›
è¾“å‡º: {"capabilities": ["ppt_generation", "document_creation"], "reasoning": "ç”Ÿæˆ PPT æ¼”ç¤ºæ–‡ç¨¿"}

### ç¤ºä¾‹ 4
å·¥å…·: {"name": "python_executor", "description": "å®‰å…¨ç¯å¢ƒæ‰§è¡Œ Python ä»£ç "}
åˆ†æ: ä»£ç æ‰§è¡Œèƒ½åŠ›ï¼Œä¹Ÿå¯ç”¨äºæ•°æ®åˆ†æ
è¾“å‡º: {"capabilities": ["code_execution", "data_analysis"], "reasoning": "æ‰§è¡Œä»£ç ï¼Œæ”¯æŒæ•°æ®åˆ†æ"}

## ç°åœ¨åˆ†æ

å·¥å…·: {tool_info}

è¯·ç›´æ¥è¾“å‡º JSONï¼ˆä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š"""


# ============================================================
# è¯­ä¹‰æ¨ç†æ ¸å¿ƒç±»
# ============================================================


class SemanticInference:
    """
    ç»Ÿä¸€çš„ LLM è¯­ä¹‰æ¨ç†æ¥å£

    èŒè´£ï¼š
    1. å¤æ‚åº¦æ¨ç†ï¼ˆæ›¿ä»£å…³é”®è¯åŒ¹é…ï¼‰
    2. æ„å›¾æ¨ç†ï¼ˆæ›¿ä»£å…³é”®è¯è§„åˆ™ï¼‰
    3. èƒ½åŠ›æ¨æ–­ï¼ˆæ›¿ä»£ keyword_mapï¼‰

    è®¾è®¡åŸåˆ™ï¼š
    - è¿è¥æ— éœ€é…ç½®ï¼Œæ¡†æ¶å†…ç½® Few-Shot ç¤ºä¾‹
    - LLM å­¦ä¹ ç¤ºä¾‹æ¨¡å¼ï¼Œè¿›è¡Œè¯­ä¹‰æ³›åŒ–æ¨ç†
    - ä¿å®ˆçš„ fallbackï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œä¸åšå…³é”®è¯çŒœæµ‹
    """

    # Few-Shot æç¤ºè¯æ˜ å°„
    FEW_SHOT_PROMPTS = {
        InferenceType.COMPLEXITY: COMPLEXITY_FEW_SHOT,
        InferenceType.INTENT: INTENT_FEW_SHOT,
        InferenceType.CAPABILITY: CAPABILITY_FEW_SHOT,
    }

    # ä¿å®ˆçš„é»˜è®¤å€¼ï¼ˆä¸åšæ™ºèƒ½çŒœæµ‹ï¼‰
    CONSERVATIVE_DEFAULTS = {
        InferenceType.COMPLEXITY: {
            "complexity": "medium",
            "reasoning": "LLM æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨ä¿å®ˆé»˜è®¤å€¼",
        },
        InferenceType.INTENT: {"task_type": "other", "reasoning": "LLM æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨ä¿å®ˆé»˜è®¤å€¼"},
        InferenceType.CAPABILITY: {"capabilities": [], "reasoning": "LLM æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨ä¿å®ˆé»˜è®¤å€¼"},
    }

    def __init__(self, llm_service=None):
        """
        åˆå§‹åŒ–è¯­ä¹‰æ¨ç†å™¨

        Args:
            llm_service: LLM æœåŠ¡ï¼ˆå¯é€‰ï¼Œæ‡’åŠ è½½ï¼‰
        """
        self._llm_service = llm_service
        self._cache: Dict[str, InferenceResult] = {}  # ç®€å•å†…å­˜ç¼“å­˜

    async def _get_llm_service(self):
        """è·å– LLM æœåŠ¡ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._llm_service is None:
            try:
                # ğŸ†• ä½¿ç”¨é…ç½®åŒ–çš„ LLM Profile
                # ğŸ†• V7.10: ä½¿ç”¨ create_llm_service æ”¯æŒå¤šæ¨¡å‹å®¹ç¾
                from config.llm_config import get_llm_profile
                from core.llm import create_llm_service

                profile = await get_llm_profile("semantic_inference")
                self._llm_service = create_llm_service(**profile)
            except Exception as e:
                logger.warning(f"âš ï¸ LLM æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                return None
        return self._llm_service

    def _get_cache_key(self, inference_type: InferenceType, context: str) -> str:
        """ç”Ÿæˆç¼“å­˜ key"""
        return f"{inference_type.value}:{hash(context)}"

    async def infer(
        self, inference_type: InferenceType, context: Dict[str, Any]
    ) -> InferenceResult:
        """
        æ‰§è¡Œè¯­ä¹‰æ¨ç†

        Args:
            inference_type: æ¨ç†ç±»å‹
            context: æ¨ç†ä¸Šä¸‹æ–‡ï¼ˆå¦‚ {"query": "..."} æˆ– {"tool_info": {...}}ï¼‰

        Returns:
            InferenceResult æ¨ç†ç»“æœ
        """
        # 1. æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        context_str = self._build_context_string(inference_type, context)

        # 2. æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(inference_type, context_str)
        if cache_key in self._cache:
            logger.debug(f"ğŸ“¦ ç¼“å­˜å‘½ä¸­: {inference_type.value}")
            return self._cache[cache_key]

        # 3. è·å– LLM æœåŠ¡
        llm = await self._get_llm_service()
        if llm is None:
            logger.warning(f"âš ï¸ LLM æœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¿å®ˆé»˜è®¤å€¼")
            return self._get_fallback_result(inference_type)

        # 4. æ„å»º Few-Shot æç¤ºè¯
        prompt = self._build_prompt(inference_type, context)

        # 5. è°ƒç”¨ LLM
        try:
            # ğŸ†• ä½¿ç”¨é…ç½®åŒ–çš„ LLM Profile
            from config.llm_config import get_llm_profile
            from core.llm import Message

            profile = await get_llm_profile("semantic_inference")

            response = await llm.create_message_async(
                messages=[Message(role="user", content=prompt)], **profile
            )

            # 6. è§£æå“åº”
            result = self._parse_response(inference_type, response)

            # 7. ç¼“å­˜ç»“æœ
            self._cache[cache_key] = result

            logger.info(f"ğŸ§  è¯­ä¹‰æ¨ç†å®Œæˆ: {inference_type.value} â†’ {result.result}")
            return result

        except Exception as e:
            logger.warning(f"âš ï¸ LLM æ¨ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨ä¿å®ˆé»˜è®¤å€¼")
            return self._get_fallback_result(inference_type)

    def _build_context_string(self, inference_type: InferenceType, context: Dict[str, Any]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼ˆç”¨äºç¼“å­˜ keyï¼‰"""
        if inference_type == InferenceType.COMPLEXITY:
            return context.get("query", "")
        elif inference_type == InferenceType.INTENT:
            return context.get("query", "")
        elif inference_type == InferenceType.CAPABILITY:
            tool_info = context.get("tool_info", {})
            return f"{tool_info.get('name', '')}:{tool_info.get('description', '')}"
        return str(context)

    def _build_prompt(self, inference_type: InferenceType, context: Dict[str, Any]) -> str:
        """æ„å»º Few-Shot æç¤ºè¯"""
        template = self.FEW_SHOT_PROMPTS.get(inference_type, "")

        # ä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢è€Œä¸æ˜¯ .format()ï¼ˆé¿å… JSON ä¸­çš„èŠ±æ‹¬å·è¢«è¯¯è§£æï¼‰
        if inference_type == InferenceType.COMPLEXITY:
            return template.replace("{query}", context.get("query", ""))
        elif inference_type == InferenceType.INTENT:
            return template.replace("{query}", context.get("query", ""))
        elif inference_type == InferenceType.CAPABILITY:
            tool_info = context.get("tool_info", {})
            return template.replace("{tool_info}", json.dumps(tool_info, ensure_ascii=False))

        return template

    def _parse_response(self, inference_type: InferenceType, response) -> InferenceResult:
        """è§£æ LLM å“åº”"""
        try:
            # æå–æ–‡æœ¬ï¼ˆLLMResponse.content æ˜¯ str ç±»å‹ï¼‰
            text = response.content.strip() if response.content else ""

            # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—
            if text.startswith("```"):
                text = text.split("```")[1]
                if text.startswith("json"):
                    text = text[4:]
            if text.endswith("```"):
                text = text[:-3]

            # è§£æ JSON
            result_data = json.loads(text.strip())

            return InferenceResult(
                inference_type=inference_type,
                result=result_data,
                confidence=0.9,  # LLM æ¨ç†ç½®ä¿¡åº¦é«˜
                reasoning=result_data.get("reasoning", ""),
                is_fallback=False,
            )

        except (json.JSONDecodeError, IndexError, KeyError, AttributeError) as e:
            logger.warning(f"âš ï¸ è§£æ LLM å“åº”å¤±è´¥: {e}")
            return self._get_fallback_result(inference_type)

    def _get_fallback_result(self, inference_type: InferenceType) -> InferenceResult:
        """è·å–ä¿å®ˆçš„ fallback ç»“æœï¼ˆä¸åšå…³é”®è¯çŒœæµ‹ï¼‰"""
        default = self.CONSERVATIVE_DEFAULTS.get(inference_type, {})
        return InferenceResult(
            inference_type=inference_type,
            result=default,
            confidence=0.3,  # ä½ç½®ä¿¡åº¦ï¼Œæ ‡è®°è¿™æ˜¯çŒœæµ‹
            reasoning="LLM æœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¿å®ˆé»˜è®¤å€¼",
            is_fallback=True,
        )

    # ============================================================
    # ä¾¿æ·æ–¹æ³•
    # ============================================================

    async def infer_complexity(self, query: str) -> InferenceResult:
        """æ¨æ–­ä»»åŠ¡å¤æ‚åº¦"""
        return await self.infer(InferenceType.COMPLEXITY, {"query": query})

    async def infer_intent(self, query: str) -> InferenceResult:
        """æ¨æ–­ç”¨æˆ·æ„å›¾"""
        return await self.infer(InferenceType.INTENT, {"query": query})

    async def infer_capability(self, tool_name: str, tool_description: str) -> InferenceResult:
        """æ¨æ–­å·¥å…·èƒ½åŠ›"""
        return await self.infer(
            InferenceType.CAPABILITY,
            {"tool_info": {"name": tool_name, "description": tool_description}},
        )


# ============================================================
# ä¾¿æ·å‡½æ•°
# ============================================================

# å…¨å±€å•ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_global_inference: Optional[SemanticInference] = None


def get_semantic_inference() -> SemanticInference:
    """è·å–å…¨å±€è¯­ä¹‰æ¨ç†å®ä¾‹"""
    global _global_inference
    if _global_inference is None:
        _global_inference = SemanticInference()
    return _global_inference


async def infer_complexity(query: str) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ¨æ–­ä»»åŠ¡å¤æ‚åº¦

    Args:
        query: ç”¨æˆ·è¾“å…¥

    Returns:
        "simple" | "medium" | "complex"
    """
    inference = get_semantic_inference()
    result = await inference.infer_complexity(query)
    return result.result.get("complexity", "medium")


async def infer_intent(query: str) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ¨æ–­ç”¨æˆ·æ„å›¾

    Args:
        query: ç”¨æˆ·è¾“å…¥

    Returns:
        ä»»åŠ¡ç±»å‹å­—ç¬¦ä¸²
    """
    inference = get_semantic_inference()
    result = await inference.infer_intent(query)
    return result.result.get("task_type", "other")


async def infer_capability(tool_name: str, tool_description: str) -> List[str]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ¨æ–­å·¥å…·èƒ½åŠ›

    Args:
        tool_name: å·¥å…·åç§°
        tool_description: å·¥å…·æè¿°

    Returns:
        èƒ½åŠ›åˆ—è¡¨
    """
    inference = get_semantic_inference()
    result = await inference.infer_capability(tool_name, tool_description)
    return result.result.get("capabilities", [])
