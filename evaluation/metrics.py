"""
è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€å®Œæ•´åº¦ã€æ•ˆç‡ã€ç”¨æˆ·æ»¡æ„åº¦ç­‰
"""
from typing import List, Dict, Optional
from dataclasses import dataclass
from statistics import mean, stdev
from .models import EvaluationReport, TaskResult, GraderType


@dataclass
class MetricResult:
    """å•ä¸ªæŒ‡æ ‡ç»“æœ"""
    name: str
    value: float
    threshold: float
    passed: bool
    unit: str = ""
    description: str = ""


@dataclass
class MetricSummary:
    """æŒ‡æ ‡æ±‡æ€»"""
    overall_score: float
    metrics: List[MetricResult]
    quality_tier: str  # EXCELLENT, GOOD, ACCEPTABLE, POOR
    regression_detected: bool
    recommendations: List[str]


class MetricsCalculator:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
    
    # è´¨é‡åˆ†å±‚é˜ˆå€¼
    QUALITY_THRESHOLDS = {
        "EXCELLENT": 0.90,
        "GOOD": 0.75,
        "ACCEPTABLE": 0.60,
        "POOR": 0.0
    }
    
    def __init__(self, baseline_report: Optional[EvaluationReport] = None):
        """
        åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
        
        Args:
            baseline_report: åŸºçº¿è¯„æµ‹æŠ¥å‘Šï¼Œç”¨äºå›å½’æ£€æµ‹
        """
        self.baseline_report = baseline_report
    
    def calculate(self, report: EvaluationReport) -> MetricSummary:
        """
        è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        
        Args:
            report: è¯„æµ‹æŠ¥å‘Š
            
        Returns:
            æŒ‡æ ‡æ±‡æ€»
        """
        metrics = []
        
        # 1. å‡†ç¡®ç‡æŒ‡æ ‡
        accuracy_metrics = self._calculate_accuracy(report)
        metrics.extend(accuracy_metrics)
        
        # 2. å®Œæ•´åº¦æŒ‡æ ‡
        completeness_metrics = self._calculate_completeness(report)
        metrics.extend(completeness_metrics)
        
        # 3. æ•ˆç‡æŒ‡æ ‡
        efficiency_metrics = self._calculate_efficiency(report)
        metrics.extend(efficiency_metrics)
        
        # 4. è´¨é‡æŒ‡æ ‡
        quality_metrics = self._calculate_quality(report)
        metrics.extend(quality_metrics)
        
        # 5. ç¨³å®šæ€§æŒ‡æ ‡
        stability_metrics = self._calculate_stability(report)
        metrics.extend(stability_metrics)
        
        # è®¡ç®—æ€»ä½“å¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        overall_score = self._calculate_overall_score(metrics)
        
        # åˆ¤æ–­è´¨é‡åˆ†å±‚
        quality_tier = self._determine_quality_tier(overall_score)
        
        # å›å½’æ£€æµ‹
        regression_detected = self._detect_regression(report)
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        recommendations = self._generate_recommendations(metrics, report)
        
        return MetricSummary(
            overall_score=overall_score,
            metrics=metrics,
            quality_tier=quality_tier,
            regression_detected=regression_detected,
            recommendations=recommendations
        )
    
    def _calculate_accuracy(self, report: EvaluationReport) -> List[MetricResult]:
        """è®¡ç®—å‡†ç¡®ç‡ç›¸å…³æŒ‡æ ‡"""
        metrics = []
        
        # Code-based grader é€šè¿‡ç‡
        code_passed = sum(
            1 for r in report.results
            for t in r.trials
            for g in t.grades
            if g.grader_type == GraderType.CODE and g.passed
        )
        code_total = sum(
            1 for r in report.results
            for t in r.trials
            for g in t.grades
            if g.grader_type == GraderType.CODE
        )
        if code_total > 0:
            code_pass_rate = code_passed / code_total
            metrics.append(MetricResult(
                name="code_pass_rate",
                value=code_pass_rate,
                threshold=0.95,
                passed=code_pass_rate >= 0.95,
                unit="%",
                description="ä»£ç çº§æ£€æŸ¥é€šè¿‡ç‡ï¼ˆå·¥å…·è°ƒç”¨ã€æ ¼å¼ã€è¯­æ³•ç­‰ï¼‰"
            ))
        
        # Model-based grader é€šè¿‡ç‡
        model_passed = sum(
            1 for r in report.results
            for t in r.trials
            for g in t.grades
            if g.grader_type == GraderType.MODEL and g.passed
        )
        model_total = sum(
            1 for r in report.results
            for t in r.trials
            for g in t.grades
            if g.grader_type == GraderType.MODEL
        )
        if model_total > 0:
            model_pass_rate = model_passed / model_total
            metrics.append(MetricResult(
                name="model_pass_rate",
                value=model_pass_rate,
                threshold=0.80,
                passed=model_pass_rate >= 0.80,
                unit="%",
                description="æ¨¡å‹çº§æ£€æŸ¥é€šè¿‡ç‡ï¼ˆæ„å›¾ç†è§£ã€è´¨é‡ã€é€»è¾‘ç­‰ï¼‰"
            ))
        
        # æ•´ä½“ä»»åŠ¡æˆåŠŸç‡
        task_passed = sum(1 for r in report.results if r.passed)
        task_total = len(report.results)
        if task_total > 0:
            task_success_rate = task_passed / task_total
            metrics.append(MetricResult(
                name="task_success_rate",
                value=task_success_rate,
                threshold=0.85,
                passed=task_success_rate >= 0.85,
                unit="%",
                description="æ•´ä½“ä»»åŠ¡æˆåŠŸç‡ï¼ˆæ‰€æœ‰ grader å…¨éƒ¨é€šè¿‡ï¼‰"
            ))
        
        return metrics
    
    def _calculate_completeness(self, report: EvaluationReport) -> List[MetricResult]:
        """è®¡ç®—å®Œæ•´åº¦ç›¸å…³æŒ‡æ ‡"""
        metrics = []
        
        # å“åº”å®Œæ•´æ€§ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰ç©ºå“åº”ã€æˆªæ–­ç­‰ï¼‰
        complete_responses = sum(
            1 for r in report.results
            for t in r.trials
            if t.outcome and t.outcome.transcript and t.outcome.transcript.messages
        )
        total_trials = sum(len(r.trials) for r in report.results)
        if total_trials > 0:
            response_completeness = complete_responses / total_trials
            metrics.append(MetricResult(
                name="response_completeness",
                value=response_completeness,
                threshold=0.98,
                passed=response_completeness >= 0.98,
                unit="%",
                description="å“åº”å®Œæ•´æ€§ï¼ˆæ— ç©ºå“åº”ã€æ— æˆªæ–­ï¼‰"
            ))
        
        # å·¥å…·è°ƒç”¨æˆåŠŸç‡
        successful_tool_calls = sum(
            1 for r in report.results
            for t in r.trials
            if t.outcome and t.outcome.transcript
            for tool_call in t.outcome.transcript.tool_calls
            if tool_call.get("status") == "success"
        )
        total_tool_calls = sum(
            1 for r in report.results
            for t in r.trials
            if t.outcome and t.outcome.transcript
            for _ in t.outcome.transcript.tool_calls
        )
        if total_tool_calls > 0:
            tool_success_rate = successful_tool_calls / total_tool_calls
            metrics.append(MetricResult(
                name="tool_success_rate",
                value=tool_success_rate,
                threshold=0.90,
                passed=tool_success_rate >= 0.90,
                unit="%",
                description="å·¥å…·è°ƒç”¨æˆåŠŸç‡"
            ))
        
        return metrics
    
    def _calculate_efficiency(self, report: EvaluationReport) -> List[MetricResult]:
        """è®¡ç®—æ•ˆç‡ç›¸å…³æŒ‡æ ‡"""
        metrics = []
        
        # å¹³å‡æ‰§è¡Œæ—¶é—´
        execution_times = [
            t.outcome.execution_time
            for r in report.results
            for t in r.trials
            if t.outcome and t.outcome.execution_time
        ]
        if execution_times:
            avg_execution_time = mean(execution_times)
            metrics.append(MetricResult(
                name="avg_execution_time",
                value=avg_execution_time,
                threshold=30.0,
                passed=avg_execution_time <= 30.0,
                unit="s",
                description="å¹³å‡æ‰§è¡Œæ—¶é—´"
            ))
        
        # Token ä½¿ç”¨æ•ˆç‡ï¼ˆè¾“å‡º/è¾“å…¥æ¯”ï¼‰
        token_ratios = [
            t.outcome.token_usage.completion_tokens / max(t.outcome.token_usage.prompt_tokens, 1)
            for r in report.results
            for t in r.trials
            if t.outcome and t.outcome.token_usage
        ]
        if token_ratios:
            avg_token_ratio = mean(token_ratios)
            metrics.append(MetricResult(
                name="token_efficiency",
                value=avg_token_ratio,
                threshold=0.5,
                passed=avg_token_ratio <= 0.5,
                unit="",
                description="Token ä½¿ç”¨æ•ˆç‡ï¼ˆè¾“å‡º/è¾“å…¥æ¯”ï¼Œè¶Šä½è¶Šå¥½ï¼‰"
            ))
        
        # å·¥å…·è°ƒç”¨æ•ˆç‡ï¼ˆå¹³å‡è°ƒç”¨æ¬¡æ•°ï¼‰
        tool_call_counts = [
            len(t.outcome.transcript.tool_calls)
            for r in report.results
            for t in r.trials
            if t.outcome and t.outcome.transcript
        ]
        if tool_call_counts:
            avg_tool_calls = mean(tool_call_counts)
            metrics.append(MetricResult(
                name="avg_tool_calls",
                value=avg_tool_calls,
                threshold=5.0,
                passed=avg_tool_calls <= 5.0,
                unit="",
                description="å¹³å‡å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼ˆåˆç†èŒƒå›´å†…è¶Šå°‘è¶Šå¥½ï¼‰"
            ))
        
        return metrics
    
    def _calculate_quality(self, report: EvaluationReport) -> List[MetricResult]:
        """è®¡ç®—è´¨é‡ç›¸å…³æŒ‡æ ‡"""
        metrics = []
        
        # LLM-as-Judge å¹³å‡å¾—åˆ†
        model_scores = [
            g.score
            for r in report.results
            for t in r.trials
            for g in t.grades
            if g.grader_type == GraderType.MODEL and g.score is not None
        ]
        if model_scores:
            avg_quality_score = mean(model_scores)
            metrics.append(MetricResult(
                name="avg_quality_score",
                value=avg_quality_score,
                threshold=7.0,
                passed=avg_quality_score >= 7.0,
                unit="/10",
                description="LLM-as-Judge å¹³å‡è´¨é‡å¾—åˆ†"
            ))
        
        # é«˜ç½®ä¿¡åº¦è¯„åˆ†å æ¯”
        high_confidence_grades = sum(
            1 for r in report.results
            for t in r.trials
            for g in t.grades
            if g.grader_type == GraderType.MODEL and g.confidence and g.confidence >= 0.8
        )
        total_model_grades = sum(
            1 for r in report.results
            for t in r.trials
            for g in t.grades
            if g.grader_type == GraderType.MODEL and g.confidence is not None
        )
        if total_model_grades > 0:
            high_confidence_rate = high_confidence_grades / total_model_grades
            metrics.append(MetricResult(
                name="high_confidence_rate",
                value=high_confidence_rate,
                threshold=0.70,
                passed=high_confidence_rate >= 0.70,
                unit="%",
                description="é«˜ç½®ä¿¡åº¦è¯„åˆ†å æ¯”ï¼ˆ>=0.8ï¼‰"
            ))
        
        # éœ€è¦äººå·¥å¤å®¡çš„æ¯”ä¾‹ï¼ˆåº”è¯¥è¾ƒä½ï¼‰
        needs_review = sum(
            1 for r in report.results
            for t in r.trials
            for g in t.grades
            if g.needs_human_review
        )
        total_grades = sum(
            1 for r in report.results
            for t in r.trials
            for _ in t.grades
        )
        if total_grades > 0:
            review_rate = needs_review / total_grades
            metrics.append(MetricResult(
                name="human_review_rate",
                value=review_rate,
                threshold=0.15,
                passed=review_rate <= 0.15,
                unit="%",
                description="éœ€è¦äººå·¥å¤å®¡çš„æ¯”ä¾‹ï¼ˆåº” â‰¤15%ï¼‰"
            ))
        
        return metrics
    
    def _calculate_stability(self, report: EvaluationReport) -> List[MetricResult]:
        """è®¡ç®—ç¨³å®šæ€§ç›¸å…³æŒ‡æ ‡"""
        metrics = []
        
        # å¤šæ¬¡è¯•éªŒä¸€è‡´æ€§ï¼ˆåŒä¸€ä»»åŠ¡å¤šæ¬¡è¯•éªŒçš„ç»“æœåº”è¯¥ä¸€è‡´ï¼‰
        consistency_scores = []
        for result in report.results:
            if len(result.trials) > 1:
                trial_results = [all(g.passed for g in t.grades) for t in result.trials]
                if trial_results:
                    consistency = sum(trial_results) / len(trial_results)
                    consistency_scores.append(consistency)
        
        if consistency_scores:
            avg_consistency = mean(consistency_scores)
            metrics.append(MetricResult(
                name="trial_consistency",
                value=avg_consistency,
                threshold=0.80,
                passed=avg_consistency >= 0.80,
                unit="%",
                description="å¤šæ¬¡è¯•éªŒä¸€è‡´æ€§ï¼ˆåŒä¸€ä»»åŠ¡é‡å¤æ‰§è¡Œçš„ç¨³å®šæ€§ï¼‰"
            ))
        
        # é”™è¯¯ç‡
        error_count = sum(
            1 for r in report.results
            for t in r.trials
            if t.outcome and t.outcome.error
        )
        total_trials = sum(len(r.trials) for r in report.results)
        if total_trials > 0:
            error_rate = error_count / total_trials
            metrics.append(MetricResult(
                name="error_rate",
                value=error_rate,
                threshold=0.05,
                passed=error_rate <= 0.05,
                unit="%",
                description="æ‰§è¡Œé”™è¯¯ç‡ï¼ˆåº” â‰¤5%ï¼‰"
            ))
        
        return metrics
    
    def _calculate_overall_score(self, metrics: List[MetricResult]) -> float:
        """
        è®¡ç®—æ€»ä½“å¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        
        æƒé‡åˆ†é…ï¼š
        - å‡†ç¡®ç‡: 40%
        - è´¨é‡: 30%
        - å®Œæ•´åº¦: 15%
        - ç¨³å®šæ€§: 10%
        - æ•ˆç‡: 5%
        """
        weights = {
            "code_pass_rate": 0.15,
            "model_pass_rate": 0.15,
            "task_success_rate": 0.10,
            "avg_quality_score": 0.20,
            "high_confidence_rate": 0.10,
            "response_completeness": 0.10,
            "tool_success_rate": 0.05,
            "trial_consistency": 0.08,
            "error_rate": 0.02,
            "avg_execution_time": 0.02,
            "token_efficiency": 0.02,
            "avg_tool_calls": 0.01,
        }
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for metric in metrics:
            if metric.name in weights:
                weight = weights[metric.name]
                # å½’ä¸€åŒ–åˆ° 0-1
                if metric.name == "avg_quality_score":
                    normalized_value = metric.value / 10.0
                elif metric.name in ["avg_execution_time", "token_efficiency", "avg_tool_calls", "error_rate", "human_review_rate"]:
                    # è¿™äº›æ˜¯"è¶Šä½è¶Šå¥½"çš„æŒ‡æ ‡ï¼Œéœ€è¦åè½¬
                    normalized_value = max(0, 1 - (metric.value / metric.threshold))
                else:
                    normalized_value = metric.value
                
                weighted_sum += normalized_value * weight
                total_weight += weight
        
        if total_weight > 0:
            return weighted_sum / total_weight
        return 0.0
    
    def _determine_quality_tier(self, overall_score: float) -> str:
        """åˆ¤æ–­è´¨é‡åˆ†å±‚"""
        for tier, threshold in self.QUALITY_THRESHOLDS.items():
            if overall_score >= threshold:
                return tier
        return "POOR"
    
    def _detect_regression(self, report: EvaluationReport) -> bool:
        """æ£€æµ‹å›å½’ï¼ˆä¸åŸºçº¿å¯¹æ¯”ï¼‰"""
        if not self.baseline_report:
            return False
        
        # å¯¹æ¯”å…³é”®æŒ‡æ ‡
        current_metrics = self.calculate(report)
        baseline_metrics = self.calculate(self.baseline_report)
        
        # å¦‚æœæ•´ä½“å¾—åˆ†ä¸‹é™è¶…è¿‡ 5%ï¼Œè§†ä¸ºå›å½’
        score_drop = baseline_metrics.overall_score - current_metrics.overall_score
        if score_drop > 0.05:
            return True
        
        # æ£€æŸ¥å…³é”®æŒ‡æ ‡æ˜¯å¦æœ‰æ˜¾è‘—ä¸‹é™
        critical_metrics = ["task_success_rate", "code_pass_rate", "model_pass_rate"]
        for metric_name in critical_metrics:
            current = next((m for m in current_metrics.metrics if m.name == metric_name), None)
            baseline = next((m for m in baseline_metrics.metrics if m.name == metric_name), None)
            if current and baseline:
                if baseline.value - current.value > 0.10:  # ä¸‹é™è¶…è¿‡ 10%
                    return True
        
        return False
    
    def _generate_recommendations(
        self, 
        metrics: List[MetricResult], 
        report: EvaluationReport
    ) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # æ£€æŸ¥æœªé€šè¿‡çš„å…³é”®æŒ‡æ ‡
        for metric in metrics:
            if not metric.passed:
                if metric.name == "task_success_rate":
                    recommendations.append(
                        f"ä»»åŠ¡æˆåŠŸç‡ ({metric.value:.1%}) ä½äºé˜ˆå€¼ ({metric.threshold:.1%})ï¼Œ"
                        "å»ºè®®æ£€æŸ¥å¤±è´¥æ¡ˆä¾‹å¹¶ä¼˜åŒ– prompt æˆ–å·¥å…·å®ç°"
                    )
                elif metric.name == "code_pass_rate":
                    recommendations.append(
                        f"ä»£ç çº§æ£€æŸ¥é€šè¿‡ç‡ ({metric.value:.1%}) è¾ƒä½ï¼Œ"
                        "å»ºè®®æ£€æŸ¥å·¥å…·è°ƒç”¨æ ¼å¼ã€å“åº”ç»“æ„ç­‰åŸºç¡€é—®é¢˜"
                    )
                elif metric.name == "model_pass_rate":
                    recommendations.append(
                        f"æ¨¡å‹çº§æ£€æŸ¥é€šè¿‡ç‡ ({metric.value:.1%}) è¾ƒä½ï¼Œ"
                        "å»ºè®®ä¼˜åŒ–æ„å›¾ç†è§£ã€å“åº”è´¨é‡æˆ–é€»è¾‘è¿è´¯æ€§"
                    )
                elif metric.name == "avg_quality_score":
                    recommendations.append(
                        f"å¹³å‡è´¨é‡å¾—åˆ† ({metric.value:.1f}/10) è¾ƒä½ï¼Œ"
                        "å»ºè®®äººå·¥å¤å®¡ä½åˆ†æ¡ˆä¾‹å¹¶è°ƒæ•´ grader rubric"
                    )
                elif metric.name == "avg_execution_time":
                    recommendations.append(
                        f"å¹³å‡æ‰§è¡Œæ—¶é—´ ({metric.value:.1f}s) è¿‡é•¿ï¼Œ"
                        "å»ºè®®ä¼˜åŒ– Agent è§„åˆ’ç­–ç•¥æˆ–å·¥å…·æ‰§è¡Œæ•ˆç‡"
                    )
                elif metric.name == "trial_consistency":
                    recommendations.append(
                        f"å¤šæ¬¡è¯•éªŒä¸€è‡´æ€§ ({metric.value:.1%}) è¾ƒä½ï¼Œ"
                        "è¯´æ˜ Agent è¡Œä¸ºä¸ç¨³å®šï¼Œå»ºè®®æ£€æŸ¥éšæœºæ€§æˆ–å·¥å…·å¯é æ€§"
                    )
                elif metric.name == "error_rate":
                    recommendations.append(
                        f"é”™è¯¯ç‡ ({metric.value:.1%}) è¾ƒé«˜ï¼Œ"
                        "å»ºè®®æ£€æŸ¥å¼‚å¸¸å¤„ç†å’Œå®¹é”™æœºåˆ¶"
                    )
        
        # æ£€æŸ¥éœ€è¦äººå·¥å¤å®¡çš„é«˜æ¯”ä¾‹
        review_metric = next((m for m in metrics if m.name == "human_review_rate"), None)
        if review_metric and review_metric.value > 0.20:
            recommendations.append(
                f"éœ€è¦äººå·¥å¤å®¡çš„æ¯”ä¾‹ ({review_metric.value:.1%}) è¾ƒé«˜ï¼Œ"
                "å»ºè®®ä¼˜åŒ– LLM-as-Judge çš„ prompt æˆ–å¢åŠ è®­ç»ƒæ•°æ®ä»¥æé«˜ç½®ä¿¡åº¦"
            )
        
        # å¦‚æœæ²¡æœ‰å»ºè®®ï¼Œè¯´æ˜è¡¨ç°è‰¯å¥½
        if not recommendations:
            recommendations.append("è¯„æµ‹ç»“æœæ•´ä½“è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
        
        return recommendations


def format_metric_summary(summary: MetricSummary) -> str:
    """æ ¼å¼åŒ–æŒ‡æ ‡æ±‡æ€»ä¸ºå¯è¯»æ–‡æœ¬"""
    lines = []
    lines.append("=" * 80)
    lines.append("è¯„ä¼°æŒ‡æ ‡æ±‡æ€»")
    lines.append("=" * 80)
    lines.append(f"æ€»ä½“å¾—åˆ†: {summary.overall_score:.2%}")
    lines.append(f"è´¨é‡åˆ†å±‚: {summary.quality_tier}")
    lines.append(f"å›å½’æ£€æµ‹: {'âš ï¸  å‘ç°å›å½’' if summary.regression_detected else 'âœ… æ— å›å½’'}")
    lines.append("")
    
    # æŒ‰ç±»åˆ«åˆ†ç»„
    accuracy_metrics = [m for m in summary.metrics if m.name in ["code_pass_rate", "model_pass_rate", "task_success_rate"]]
    quality_metrics = [m for m in summary.metrics if m.name in ["avg_quality_score", "high_confidence_rate", "human_review_rate"]]
    completeness_metrics = [m for m in summary.metrics if m.name in ["response_completeness", "tool_success_rate"]]
    efficiency_metrics = [m for m in summary.metrics if m.name in ["avg_execution_time", "token_efficiency", "avg_tool_calls"]]
    stability_metrics = [m for m in summary.metrics if m.name in ["trial_consistency", "error_rate"]]
    
    def format_metrics_group(title: str, metrics: List[MetricResult]):
        lines.append(f"\n{title}")
        lines.append("-" * 80)
        for metric in metrics:
            status = "âœ…" if metric.passed else "âŒ"
            if metric.unit == "%":
                value_str = f"{metric.value:.1%}"
                threshold_str = f"{metric.threshold:.1%}"
            elif metric.unit == "/10":
                value_str = f"{metric.value:.1f}/10"
                threshold_str = f">={metric.threshold:.1f}"
            else:
                value_str = f"{metric.value:.2f}{metric.unit}"
                threshold_str = f"<={metric.threshold:.2f}{metric.unit}" if metric.name in ["avg_execution_time", "error_rate"] else f">={metric.threshold:.2f}{metric.unit}"
            
            lines.append(f"  {status} {metric.description}")
            lines.append(f"     å½“å‰å€¼: {value_str} | é˜ˆå€¼: {threshold_str}")
    
    format_metrics_group("ğŸ“Š å‡†ç¡®ç‡æŒ‡æ ‡", accuracy_metrics)
    format_metrics_group("â­ è´¨é‡æŒ‡æ ‡", quality_metrics)
    format_metrics_group("âœ“ å®Œæ•´åº¦æŒ‡æ ‡", completeness_metrics)
    format_metrics_group("âš¡ æ•ˆç‡æŒ‡æ ‡", efficiency_metrics)
    format_metrics_group("ğŸ”’ ç¨³å®šæ€§æŒ‡æ ‡", stability_metrics)
    
    lines.append("\n" + "=" * 80)
    lines.append("æ”¹è¿›å»ºè®®")
    lines.append("=" * 80)
    for i, rec in enumerate(summary.recommendations, 1):
        lines.append(f"{i}. {rec}")
    
    lines.append("=" * 80)
    
    return "\n".join(lines)
