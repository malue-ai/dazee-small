# ZenFlux Agent è¯„ä¼°ç³»ç»Ÿ

åŸºäº Anthropic æ–¹æ³•è®ºçš„ä¸‰å±‚è¯„åˆ†å™¨ï¼ˆCode + Model + Humanï¼‰è¯„ä¼°ç³»ç»Ÿï¼Œç”¨äºå…¨é¢è¯„ä¼° Agent æ€§èƒ½ã€‚

## ğŸ“‹ ç›®å½•ç»“æ„

```
evaluation/
â”œâ”€â”€ models.py              # æ•°æ®æ¨¡å‹å®šä¹‰
â”œâ”€â”€ harness.py            # è¯„ä¼°æ‰§è¡Œå¼•æ“
â”œâ”€â”€ graders/              # è¯„åˆ†å™¨å®ç°
â”‚   â”œâ”€â”€ code_based.py     # ä»£ç çº§è¯„åˆ†å™¨ï¼ˆå®¢è§‚ã€å¿«é€Ÿï¼‰
â”‚   â”œâ”€â”€ model_based.py    # æ¨¡å‹çº§è¯„åˆ†å™¨ï¼ˆLLM-as-Judgeï¼‰
â”‚   â””â”€â”€ human.py          # äººå·¥è¯„åˆ†å™¨æ¥å£
â”œâ”€â”€ metrics.py            # æŒ‡æ ‡è®¡ç®—æ¨¡å—
â”œâ”€â”€ calibration.py        # äººå·¥æ ¡å‡†å·¥ä½œæµ
â”œâ”€â”€ dashboard.py          # æŒ‡æ ‡çœ‹æ¿
â”œâ”€â”€ alerts.py             # å‘Šè­¦æœºåˆ¶
â”œâ”€â”€ ci_integration.py     # CI/CD é›†æˆ
â”œâ”€â”€ qos_config.py         # QoS è¯„ä¼°é…ç½®
â”œâ”€â”€ case_converter.py     # å¤±è´¥æ¡ˆä¾‹è½¬æ¢å™¨
â”œâ”€â”€ case_reviewer.py      # å¤±è´¥æ¡ˆä¾‹å¤å®¡
â”œâ”€â”€ promptfoo_adapter.py  # Promptfoo ç»“æœè½¬æ¢å™¨
â”œâ”€â”€ promptfoo_mapping.md  # Promptfoo èƒ½åŠ›æ˜ å°„æ–‡æ¡£
â”œâ”€â”€ suites/               # è¯„æµ‹å¥—ä»¶
â”‚   â”œâ”€â”€ promptfoo/       # Promptfoo å¥—ä»¶ï¼ˆPrompt çº§åˆ«å›å½’ï¼‰
â”‚   â”œâ”€â”€ conversation/     # å¯¹è¯ç±»ä»»åŠ¡
â”‚   â”œâ”€â”€ coding/           # ç¼–ç ç±»ä»»åŠ¡
â”‚   â”œâ”€â”€ multi_agent/      # å¤šæ™ºèƒ½ä½“ä»»åŠ¡
â”‚   â””â”€â”€ regression/       # å›å½’æµ‹è¯•ï¼ˆä»ç”Ÿäº§å¤±è´¥æ¡ˆä¾‹ç”Ÿæˆï¼‰
â””â”€â”€ reports/              # è¯„ä¼°æŠ¥å‘Šè¾“å‡º
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. è¿è¡Œå•ä¸ªè¯„æµ‹å¥—ä»¶

```python
from evaluation.harness import EvaluationHarness

harness = EvaluationHarness()
report = await harness.run_suite(
    suite_name="conversation/basic",
    num_trials=3
)
```

### 3. æŸ¥çœ‹è¯„ä¼°æŒ‡æ ‡

```python
from evaluation.metrics import MetricsCalculator, format_metric_summary

calculator = MetricsCalculator()
summary = calculator.calculate(report)
print(format_metric_summary(summary))
```

### 4. åœ¨ CI/CD ä¸­è¿è¡Œ

```bash
# è¿è¡Œ BASIC çº§åˆ«è¯„ä¼°
python -m evaluation.ci_integration --qos-level BASIC

# è¿è¡Œæ‰€æœ‰çº§åˆ«è¯„ä¼°
python -m evaluation.ci_integration --qos-level ALL

# ä½¿ç”¨åŸºçº¿æŠ¥å‘Šè¿›è¡Œå›å½’æ£€æµ‹
python -m evaluation.ci_integration \
    --qos-level PRO \
    --baseline evaluation/baselines/baseline.json
```

## ğŸ“Š è¯„æµ‹å¥—ä»¶

### Promptfoo å¥—ä»¶ (promptfoo/)

**é€‚ç”¨åœºæ™¯ï¼šPrompt çº§åˆ«çš„å›å½’æµ‹è¯•**

- **prompt_regression**: Prompt å›å½’æµ‹è¯•
- **format_validation**: æ ¼å¼éªŒè¯
- **cost_check**: æˆæœ¬æ£€æŸ¥

**ç‰¹ç‚¹ï¼š**
- ä½¿ç”¨ Promptfoo æ¡†æ¶è¿è¡Œï¼ˆå¿«é€Ÿã€è½»é‡ï¼‰
- é€‚åˆå•è½®å¯¹è¯ã€æ ¼å¼éªŒè¯ã€æˆæœ¬æ£€æŸ¥
- ç»“æœè‡ªåŠ¨è½¬æ¢ä¸º ZenFlux æŠ¥å‘Šæ ¼å¼

**ä¸é€‚ç”¨ï¼š** å¤šè½®å¯¹è¯ã€å·¥å…·è°ƒç”¨ã€ä¸­é—´æ£€æŸ¥ç‚¹ â†’ ä½¿ç”¨ Agent å¥—ä»¶

è¯¦è§ï¼š[promptfoo/README.md](suites/promptfoo/README.md)

### Agent å¥—ä»¶ï¼ˆå®Œæ•´æµç¨‹è¯„ä¼°ï¼‰

#### å¯¹è¯ç±» (conversation/)

- **basic**: åŸºç¡€å¯¹è¯ç†è§£
- **multi_turn**: å¤šè½®å¯¹è¯
- **context_tracking**: ä¸Šä¸‹æ–‡è·Ÿè¸ª

#### ç¼–ç ç±» (coding/)

- **basic_code_generation**: åŸºç¡€ä»£ç ç”Ÿæˆ
- **file_operations**: æ–‡ä»¶æ“ä½œ
- **code_execution**: ä»£ç æ‰§è¡Œ

#### å¤šæ™ºèƒ½ä½“ (multi_agent/)

- **task_decomposition**: ä»»åŠ¡åˆ†è§£
- **sub_agent_coordination**: å­æ™ºèƒ½ä½“åä½œ
- **checkpoint_recovery**: æ£€æŸ¥ç‚¹æ¢å¤

#### å›å½’æµ‹è¯• (regression/)

ä»ç”Ÿäº§å¤±è´¥æ¡ˆä¾‹åº“è‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…æ‹¬ï¼š
- æŠ€æœ¯æŠ¥é”™ï¼ˆcontext overflow, tool call failure ç­‰ï¼‰
- ç”¨æˆ·åé¦ˆä¸æ»¡æ„ï¼ˆthumbs down, negative commentsï¼‰
- å“åº”è´¨é‡é—®é¢˜ï¼ˆincomplete, logical inconsistency ç­‰ï¼‰

### å¥—ä»¶é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èå¥—ä»¶ | åŸå›  |
|------|---------|------|
| Prompt å˜æ›´å›å½’ | `promptfoo/` | å¿«é€Ÿã€è½»é‡ã€é€‚åˆå•è½®æµ‹è¯• |
| å¤šè½®å¯¹è¯è¯„ä¼° | `conversation/` | éœ€è¦ä¸Šä¸‹æ–‡è·Ÿè¸ª |
| å·¥å…·è°ƒç”¨éªŒè¯ | `coding/` | éœ€è¦ Transcript å’Œå·¥å…·è°ƒç”¨è®°å½• |
| å¤šæ™ºèƒ½ä½“åä½œ | `multi_agent/` | éœ€è¦ä»»åŠ¡åˆ†è§£å’Œæ£€æŸ¥ç‚¹ |
| ç”Ÿäº§å¤±è´¥å›å½’ | `regression/` | ä»çœŸå®å¤±è´¥æ¡ˆä¾‹ç”Ÿæˆ |

## ğŸ¯ è¯„åˆ†å™¨

### Code-based Gradersï¼ˆä»£ç çº§è¯„åˆ†å™¨ï¼‰

å®¢è§‚ã€å¿«é€Ÿã€å¯å¤ç°ï¼š

```python
from evaluation.graders.code_based import CodeBasedGraders

graders = CodeBasedGraders()

# æ£€æŸ¥å·¥å…·è°ƒç”¨
result = graders.check_tool_calls(transcript, expected_tools=["write_file"])

# æ£€æŸ¥ Token é™åˆ¶
result = graders.check_token_limit(transcript, max_tokens=10000)

# æ£€æŸ¥ä¸­é—´ç»“æœ
result = graders.check_checkpoint(transcript, checkpoint_step=2, expression="result > 0")
```

### Model-based Gradersï¼ˆæ¨¡å‹çº§è¯„åˆ†å™¨ï¼‰

çµæ´»ã€ä¸»è§‚ã€å¸¦ç½®ä¿¡åº¦ï¼š

```python
from evaluation.graders.model_based import ModelBasedGraders

graders = ModelBasedGraders(llm=llm_instance)

# æ„å›¾ç†è§£
result = await graders.grade_intent_understanding(query, transcript)

# å“åº”è´¨é‡
result = await graders.grade_response_quality(query, transcript, expected_outcome)

# è¿‡åº¦å·¥ç¨‹åŒ–æ£€æµ‹
result = await graders.grade_over_engineering(query, transcript)

# ä½¿ç”¨ ensembleï¼ˆå¤š Judge æŠ•ç¥¨ï¼‰
result = await graders.grade_with_ensemble(
    query, transcript, grading_func="grade_response_quality", num_judges=3
)
```

### Human Gradersï¼ˆäººå·¥è¯„åˆ†å™¨ï¼‰

ç”¨äºæ ¡å‡†å’Œå¤„ç†å¤æ‚æ¡ˆä¾‹ï¼š

```python
from evaluation.graders.human import HumanGraderInterface

interface = HumanGraderInterface()

# æäº¤äººå·¥å¤å®¡è¯·æ±‚
review_id = interface.submit_for_review(task, trial, context="Low confidence from LLM")

# è·å–å¾…å¤å®¡ä»»åŠ¡
pending = interface.get_pending_reviews()

# æäº¤äººå·¥è¯„åˆ†
interface.submit_review(review_id, grades=[...], notes="...")
```

## ğŸ“ˆ æŒ‡æ ‡ä½“ç³»

### å‡†ç¡®ç‡æŒ‡æ ‡ï¼ˆ40%æƒé‡ï¼‰

- **code_pass_rate**: ä»£ç çº§æ£€æŸ¥é€šè¿‡ç‡ï¼ˆâ‰¥95%ï¼‰
- **model_pass_rate**: æ¨¡å‹çº§æ£€æŸ¥é€šè¿‡ç‡ï¼ˆâ‰¥80%ï¼‰
- **task_success_rate**: æ•´ä½“ä»»åŠ¡æˆåŠŸç‡ï¼ˆâ‰¥85%ï¼‰

### è´¨é‡æŒ‡æ ‡ï¼ˆ30%æƒé‡ï¼‰

- **avg_quality_score**: LLM-as-Judge å¹³å‡å¾—åˆ†ï¼ˆâ‰¥7.0/10ï¼‰
- **high_confidence_rate**: é«˜ç½®ä¿¡åº¦è¯„åˆ†å æ¯”ï¼ˆâ‰¥70%ï¼‰
- **human_review_rate**: éœ€äººå·¥å¤å®¡çš„æ¯”ä¾‹ï¼ˆâ‰¤15%ï¼‰

### å®Œæ•´åº¦æŒ‡æ ‡ï¼ˆ15%æƒé‡ï¼‰

- **response_completeness**: å“åº”å®Œæ•´æ€§ï¼ˆâ‰¥98%ï¼‰
- **tool_success_rate**: å·¥å…·è°ƒç”¨æˆåŠŸç‡ï¼ˆâ‰¥90%ï¼‰

### ç¨³å®šæ€§æŒ‡æ ‡ï¼ˆ10%æƒé‡ï¼‰

- **trial_consistency**: å¤šæ¬¡è¯•éªŒä¸€è‡´æ€§ï¼ˆâ‰¥80%ï¼‰
- **error_rate**: æ‰§è¡Œé”™è¯¯ç‡ï¼ˆâ‰¤5%ï¼‰

### æ•ˆç‡æŒ‡æ ‡ï¼ˆ5%æƒé‡ï¼‰

- **avg_execution_time**: å¹³å‡æ‰§è¡Œæ—¶é—´ï¼ˆâ‰¤30sï¼‰
- **token_efficiency**: Token ä½¿ç”¨æ•ˆç‡
- **avg_tool_calls**: å¹³å‡å·¥å…·è°ƒç”¨æ¬¡æ•°

## ğŸ”” å‘Šè­¦æœºåˆ¶

ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹ä»¥ä¸‹æƒ…å†µå¹¶å‘é€å‘Šè­¦ï¼š

### ğŸ”´ ä¸¥é‡å‘Šè­¦ï¼ˆé˜»å¡å‘å¸ƒï¼‰

- æ€§èƒ½å›å½’æ£€æµ‹
- è´¨é‡åˆ†å±‚ä¸º POOR
- ä»»åŠ¡æˆåŠŸç‡ < 70%
- é”™è¯¯ç‡ > 10%

### ğŸŸ¡ è­¦å‘Šå‘Šè­¦ï¼ˆå»ºè®®ä¿®å¤ï¼‰

- è´¨é‡åˆ†å±‚ä¸º ACCEPTABLE
- ä»£ç çº§æ£€æŸ¥é€šè¿‡ç‡ < 90%
- æ¨¡å‹çº§æ£€æŸ¥é€šè¿‡ç‡ < 70%
- å¹³å‡è´¨é‡å¾—åˆ† < 6.0/10
- å¹³å‡æ‰§è¡Œæ—¶é—´ > 60s

### ğŸ”µ ä¿¡æ¯å‘Šè­¦ï¼ˆæç¤ºå…³æ³¨ï¼‰

- äººå·¥å¤å®¡æ¯”ä¾‹ > 25%

### é…ç½®é€šçŸ¥æ¸ é“

```python
from evaluation.alerts import AlertManager, slack_notifier, email_notifier

alert_manager = AlertManager()

# æ·»åŠ  Slack é€šçŸ¥
alert_manager.add_notifier(slack_notifier(webhook_url="https://..."))

# æ·»åŠ é‚®ä»¶é€šçŸ¥
alert_manager.add_notifier(email_notifier(
    smtp_server="smtp.gmail.com",
    smtp_port=587,
    sender="alerts@example.com",
    password="***",
    recipients=["team@example.com"]
))
```

## ğŸ”„ äººå·¥æ ¡å‡†å·¥ä½œæµ

LLM-as-Judge éœ€è¦å®šæœŸæ ¡å‡†ä»¥ä¿è¯å¯é æ€§ï¼š

```python
from evaluation.calibration import CalibrationWorkflow

workflow = CalibrationWorkflow(model_graders=graders)

# 1. åˆ›å»ºæ ¡å‡†é›†ï¼ˆé‡‡æ · 50 ä¸ªæ¡ˆä¾‹ï¼‰
calibration_cases = await workflow.create_calibration_set(
    tasks=all_tasks,
    sample_size=50,
    strategy="diverse"  # diverse/edge_cases/random
)

# 2. äººå·¥æ ‡æ³¨
for case in calibration_cases:
    # äººå·¥å¤å®¡å¹¶æäº¤è¯„åˆ†
    human_grades = [...]  # äººå·¥ç»™å‡ºçš„è¯„åˆ†
    workflow.submit_human_grades(case.task_id, human_grades)

# 3. ç”Ÿæˆæ ¡å‡†æŠ¥å‘Š
report = workflow.generate_calibration_report()
print(format_calibration_report(report))

# 4. æ›´æ–° grader promptsï¼ˆæ ¹æ®ä¸ä¸€è‡´æ¡ˆä¾‹ï¼‰
workflow.update_grader_prompts(report)
```

## ğŸ“Š è¯„ä¼°çœ‹æ¿

å®æ—¶ç›‘æ§è¯„ä¼°æŒ‡æ ‡ï¼š

```python
from evaluation.dashboard import EvaluationDashboard

dashboard = EvaluationDashboard()

# ç”Ÿæˆæ–‡æœ¬çœ‹æ¿
text = dashboard.generate_dashboard(days=7)
print(text)

# å¯¼å‡º JSONï¼ˆç”¨äºå¤–éƒ¨å¯è§†åŒ–ï¼‰
dashboard.export_to_json("dashboard.json")
```

çœ‹æ¿å†…å®¹åŒ…æ‹¬ï¼š
- ğŸ“ˆ è¶‹åŠ¿åˆ†æï¼ˆæ€»ä½“å¾—åˆ†ã€æˆåŠŸç‡ã€è´¨é‡å¾—åˆ†ï¼‰
- ğŸ“Š æœ€æ–°è¯„ä¼°æ‘˜è¦
- ğŸ“‰ å…³é”®æŒ‡æ ‡å†å²
- âŒ å¤±è´¥æ¡ˆä¾‹ Top 5
- âš ï¸  å‘Šè­¦ä¿¡æ¯

## ğŸ”§ å¤±è´¥æ¡ˆä¾‹é‡‡é›†

ä»ç”Ÿäº§ç¯å¢ƒè‡ªåŠ¨é‡‡é›†å¤±è´¥æ¡ˆä¾‹å¹¶è½¬æ¢ä¸ºå›å½’æµ‹è¯•ï¼š

### 1. å¤±è´¥æ£€æµ‹å™¨

```python
from core.monitoring.failure_detector import FailureDetector

detector = FailureDetector()

# è‡ªåŠ¨æ£€æµ‹å„ç±»å¤±è´¥
# - æŠ€æœ¯æŠ¥é”™: context_overflow, tool_call_failure, timeout
# - ç”¨æˆ·åé¦ˆ: user_negative_feedback, user_retry
# - è´¨é‡é—®é¢˜: over_engineering, logical_incoherence

# åœ¨ç”Ÿäº§ä»£ç ä¸­é›†æˆ
await detector.detect_and_record(
    conversation_id=conv_id,
    failure_type=FailureType.TOOL_CALL_FAILURE,
    context={"tool_name": "write_file", "error": str(e)}
)
```

### 2. è´¨é‡æ‰«æå™¨

å®šæœŸæ‰«æå¯¹è¯è´¨é‡ï¼š

```python
from core.monitoring.quality_scanner import QualityScanner

scanner = QualityScanner(
    model_graders=graders,
    failure_detector=detector
)

# åå°è¿è¡Œï¼ˆå»ºè®®æ¯å°æ—¶è¿è¡Œä¸€æ¬¡ï¼‰
await scanner.scan_recent_conversations(hours=1)
```

### 3. æ¡ˆä¾‹è½¬æ¢å™¨

å°†å¤±è´¥æ¡ˆä¾‹è½¬æ¢ä¸ºè¯„æµ‹ä»»åŠ¡ï¼š

```python
from evaluation.case_converter import FailureCaseConverter

converter = FailureCaseConverter(failure_detector=detector)

# æ‰¹é‡è½¬æ¢
tasks = converter.convert_batch(limit=50, min_severity="medium")

# å¯¼å‡ºä¸ºè¯„æµ‹å¥—ä»¶
converter.export_to_suite(
    tasks=tasks,
    output_file="evaluation/suites/regression/production_failures.yaml"
)
```

### 4. æ¡ˆä¾‹å¤å®¡

äººå·¥å¤å®¡å¤±è´¥æ¡ˆä¾‹ï¼š

```python
from evaluation.case_reviewer import CaseReviewer

reviewer = CaseReviewer(failure_detector=detector)

# è·å–å¾…å¤å®¡æ¡ˆä¾‹
pending_cases = reviewer.get_pending_cases(category="user_feedback")

# æäº¤å¤å®¡
reviewer.submit_review(
    case_id=case.id,
    is_valid_failure=True,
    root_cause="intent_misunderstanding",
    suggested_fix="Update intent detection prompt",
    priority="high"
)
```

## ğŸšï¸ QoS å·®å¼‚åŒ–è¯„ä¼°

ä¸åŒæœåŠ¡ç­‰çº§æœ‰ä¸åŒçš„è¯„ä¼°é…ç½®ï¼š

```python
from evaluation.qos_config import QOS_EVAL_CONFIGS, QoSLevel

config = QOS_EVAL_CONFIGS[QoSLevel.PRO]

print(f"è¯„æµ‹å¥—ä»¶: {config.suites}")
print(f"è¯•éªŒæ¬¡æ•°: {config.trials}")
print(f"æœ€ä½æ€»åˆ†: {config.min_overall_score}")
print(f"æœ€ä½æˆåŠŸç‡: {config.min_task_success_rate}")
```

| QoS Level | å¥—ä»¶æ•° | è¯•éªŒæ¬¡æ•° | æœ€ä½æ€»åˆ† | æœ€ä½æˆåŠŸç‡ |
|-----------|--------|----------|----------|------------|
| FREE      | 1      | 1        | 60%      | 70%        |
| BASIC     | 2      | 2        | 75%      | 80%        |
| PRO       | 4      | 3        | 85%      | 90%        |
| ENTERPRISE| 5      | 5        | 90%      | 95%        |

## ğŸ”„ CI/CD é›†æˆ

### GitHub Actions å·¥ä½œæµ

å‚è§ `.github/workflows/evaluation.yml`

è§¦å‘æ—¶æœºï¼š
- **PR æäº¤**: è¿è¡Œ BASIC çº§åˆ«è¯„ä¼°
- **åˆå¹¶åˆ° main**: è¿è¡Œ PRO çº§åˆ«è¯„ä¼°
- **æ¯æ—¥å®šæ—¶**: è¿è¡Œ ALL çº§åˆ«å…¨é¢è¯„ä¼°
- **æ‰‹åŠ¨è§¦å‘**: å¯é€‰æ‹©ä»»æ„çº§åˆ«

### æœ¬åœ°è¿è¡Œ

```bash
# å¼€å‘é˜¶æ®µï¼šå¿«é€ŸéªŒè¯
python -m evaluation.ci_integration --qos-level FREE

# PR å‰ï¼šå®Œæ•´æµ‹è¯•
python -m evaluation.ci_integration --qos-level BASIC --strict

# å‘å¸ƒå‰ï¼šå…¨é¢å›å½’
python -m evaluation.ci_integration --qos-level ALL --baseline evaluation/baselines/baseline.json
```

## ğŸ“ ç¼–å†™è¯„æµ‹å¥—ä»¶

YAML æ ¼å¼ç¤ºä¾‹ï¼š

```yaml
name: "åŸºç¡€ä»£ç ç”Ÿæˆ"
description: "æµ‹è¯• Agent çš„åŸºç¡€ä»£ç ç”Ÿæˆèƒ½åŠ›"
version: "1.0"

tasks:
  - task_id: "fibonacci"
    input:
      query: "å†™ä¸€ä¸ª Python å‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ç¬¬ n é¡¹"
      agent_type: "simple"
      qos_level: "BASIC"
    
    expected_outcome:
      status: "success"
      tool_calls: ["write_file", "run_python"]
    
    reference_answer: |
      def fibonacci(n):
          if n <= 1:
              return n
          return fibonacci(n-1) + fibonacci(n-2)
    
    graders:
      - type: "code"
        name: "check_tool_calls"
        config:
          expected_tools: ["write_file"]
          
      - type: "code"
        name: "check_code_syntax"
        config:
          language: "python"
      
      - type: "model"
        name: "grade_response_quality"
        config:
          min_score: 7.0
          confidence_threshold: 0.7
```

## ğŸ› ï¸ æœ€ä½³å®è·µ

### 1. è¯„æµ‹å¥—ä»¶è®¾è®¡

- âœ… è¦†ç›–æ ¸å¿ƒç”¨æˆ·åœºæ™¯
- âœ… åŒ…å«è¾¹ç•Œæƒ…å†µå’Œå¤±è´¥æ¡ˆä¾‹
- âœ… ä½¿ç”¨å¤šè½®å¯¹è¯æµ‹è¯•ä¸Šä¸‹æ–‡ç†è§£
- âœ… è®¾ç½®åˆç†çš„ checkpoint éªŒè¯ä¸­é—´æ­¥éª¤
- âœ… ä»ç”Ÿäº§å¤±è´¥æ¡ˆä¾‹ç”Ÿæˆå›å½’æµ‹è¯•

### 2. Grader é€‰æ‹©

- ğŸ¯ èƒ½ç”¨ Code-based å°±ä¸ç”¨ Model-basedï¼ˆå¿«é€Ÿã€ç¨³å®šï¼‰
- ğŸ¯ Model-based grader éœ€è¦å®šæœŸæ ¡å‡†
- ğŸ¯ ä½ç½®ä¿¡åº¦çš„è¯„åˆ†åº”æäº¤äººå·¥å¤å®¡
- ğŸ¯ ä½¿ç”¨ ensemble æé«˜å¯é æ€§ï¼ˆé‡è¦åœºæ™¯ï¼‰

### 3. æŒ‡æ ‡ç›‘æ§

- ğŸ“Š æ¯æ—¥ç”Ÿæˆçœ‹æ¿ï¼Œå…³æ³¨è¶‹åŠ¿
- ğŸ“Š è®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼
- ğŸ“Š å›å½’æ£€æµ‹åº”ä½¿ç”¨ç¨³å®šçš„åŸºçº¿
- ğŸ“Š ä¸åŒ QoS çº§åˆ«é‡‡ç”¨ä¸åŒæ ‡å‡†

### 4. CI/CD é›†æˆ

- ğŸ”„ PR é˜¶æ®µè¿è¡Œå¿«é€Ÿè¯„ä¼°ï¼ˆFREE/BASICï¼‰
- ğŸ”„ å‘å¸ƒå‰è¿è¡Œå…¨é¢è¯„ä¼°ï¼ˆPRO/ENTERPRISEï¼‰
- ğŸ”„ ä¸¥æ ¼æ¨¡å¼ä¸‹ä»»ä½• CRITICAL å‘Šè­¦éƒ½é˜»å¡å‘å¸ƒ
- ğŸ”„ å®šæœŸæ›´æ–°åŸºçº¿æŠ¥å‘Šï¼ˆæ¯å‘¨/æ¯æ¬¡å‘å¸ƒï¼‰

## ğŸ“ é™„å½•ï¼šè¯„ä¼°æ¡†æ¶

ä¸ºé¿å…é‡å¤é€ è½®å­ï¼Œå»ºè®®æ ¹æ®åœºæ™¯ç»„åˆå¼€æº/å•†ä¸šæ¡†æ¶ä¸æœ¬è¯„ä¼°ç³»ç»Ÿï¼š

### 1. Promptfooï¼ˆæ¨èä½œä¸ºæç¤ºè¯çº§åˆ«è¯„ä¼°ï¼‰

- **é€‚ç”¨**ï¼šæç¤ºè¯å›å½’ã€YAML é…ç½®åŒ–æ–­è¨€ã€LLM-as-Judge rubric
- **ä¼˜åŠ¿**ï¼šè½»é‡ã€æ˜“ä¸Šæ‰‹ã€ç¤¾åŒºæ´»è·ƒ
- **æ¨èç»„åˆæ–¹å¼**ï¼š
  - ä½¿ç”¨ Promptfoo åš prompt çº§åˆ«çš„å¿«é€Ÿå›å½’
  - æœ¬ç³»ç»Ÿè´Ÿè´£æ™ºèƒ½ä½“çº§åˆ«ï¼ˆå¤šè½®ã€å·¥å…·è°ƒç”¨ã€æ£€æŸ¥ç‚¹ã€QoSï¼‰è¯„ä¼°

#### Promptfoo é›†æˆä½¿ç”¨

**æ­¥éª¤ 1ï¼šè¿è¡Œ Promptfoo è¯„ä¼°**

```bash
# å®‰è£… Promptfoo
npm install -g promptfoo

# è¿è¡Œè¯„ä¼°
npx promptfoo eval -c evaluation/suites/promptfoo/prompt_regression.yaml -o promptfoo_results.json
```

**æ­¥éª¤ 2ï¼šè½¬æ¢ä¸º ZenFlux æŠ¥å‘Š**

```python
from evaluation.promptfoo_adapter import convert_promptfoo_result
from pathlib import Path

# è½¬æ¢ç»“æœ
report = convert_promptfoo_result(
    promptfoo_result_path=Path("promptfoo_results.json"),
    suite_name="prompt_regression"
)

# è®¡ç®—æŒ‡æ ‡
from evaluation.metrics import MetricsCalculator, format_metric_summary

calculator = MetricsCalculator()
summary = calculator.calculate(report)
print(format_metric_summary(summary))
```

**æ­¥éª¤ 3ï¼šåœ¨ CI ä¸­é›†æˆ**

å‚è§ [CI/CD é›†æˆ](#cicd-é›†æˆ) éƒ¨åˆ†ï¼ŒPromptfoo ç»“æœä¼šè‡ªåŠ¨è½¬æ¢ä¸º ZenFlux æŠ¥å‘Šå¹¶ç»Ÿä¸€å¤„ç†ã€‚

**æ–­è¨€ç±»å‹æ˜ å°„**

è¯¦è§ï¼š[promptfoo_mapping.md](promptfoo_mapping.md)

### 2. Harborï¼ˆå®¹å™¨åŒ–ä»»åŠ¡åŸºå‡†ï¼‰

- **é€‚ç”¨**ï¼šåœ¨å®¹å™¨åŒ–ç¯å¢ƒä¸­è¿è¡Œæ ‡å‡†åŸºå‡†æˆ–å¤§è§„æ¨¡è¯•éªŒ
- **ä¼˜åŠ¿**ï¼šç»Ÿä¸€ä»»åŠ¡/è¯„åˆ†æ ¼å¼ï¼Œæ”¯æŒäº‘ç«¯è§„æ¨¡åŒ–
- **æ¨èç»„åˆæ–¹å¼**ï¼š
  - ä½¿ç”¨ Harbor è·‘æ ‡å‡†åŸºå‡†ï¼ˆå¦‚ Terminal-Benchï¼‰
  - å°†ç»“æœå¯¼å…¥æœ¬ç³»ç»Ÿè¿›è¡Œç»Ÿä¸€æŒ‡æ ‡æ±‡æ€»

### 3. Braintrustï¼ˆç¦»çº¿è¯„ä¼° + ç”Ÿäº§å¯è§‚æµ‹ï¼‰

- **é€‚ç”¨**ï¼šéœ€è¦åŒæ—¶åšç¦»çº¿è¯„ä¼°ä¸çº¿ä¸Šè´¨é‡ç›‘æ§
- **ä¼˜åŠ¿**ï¼š`autoevals` æä¾›äº‹å®æ€§ã€ç›¸å…³æ€§ç­‰é¢„ç½®è¯„åˆ†å™¨
- **æ¨èç»„åˆæ–¹å¼**ï¼š
  - Braintrust è´Ÿè´£çº¿ä¸Šè§‚æµ‹å’Œå®éªŒè·Ÿè¸ª
  - æœ¬ç³»ç»Ÿè´Ÿè´£é’ˆå¯¹ agent å†…éƒ¨æµç¨‹çš„ç»†ç²’åº¦è¯„æµ‹

### 4. LangSmith / Langfuseï¼ˆç”Ÿæ€é›†æˆï¼‰

- **é€‚ç”¨**ï¼šLangChain ç”Ÿæ€ç”¨æˆ·ï¼Œæˆ–æœ‰æ•°æ®é©»ç•™éœ€æ±‚
- **ä¼˜åŠ¿**ï¼šè¿½è¸ªã€æ•°æ®é›†ç®¡ç†ã€åœ¨çº¿/ç¦»çº¿è¯„ä¼°
- **æ¨èç»„åˆæ–¹å¼**ï¼š
  - LangSmith/Langfuse è´Ÿè´£é“¾è·¯è¿½è¸ªä¸æ•°æ®é›†ç®¡ç†
  - æœ¬ç³»ç»Ÿè´Ÿè´£æŒ‡æ ‡è®¡ç®—ã€æ ¡å‡†ä¸å‘å¸ƒé—¨ç¦

### é€‰æ‹©å»ºè®®

- **ä¼˜å…ˆé€‰æ¡†æ¶ï¼Œå†æŠ•å…¥è¯„æµ‹**ï¼šæ¡†æ¶åªæ˜¯åŠ é€Ÿå™¨ï¼Œè¯„æµ‹è´¨é‡å–å†³äºä»»åŠ¡å’Œè¯„åˆ†å™¨
- **Promptfoo + æœ¬ç³»ç»Ÿ**ï¼šæœ€è½»é‡ã€è¦†ç›– prompt ä¸ agent ä¸¤å±‚
- **å·²æœ‰è§‚æµ‹å¹³å°**ï¼šæ¥å…¥æœ¬ç³»ç»Ÿå³å¯è¡¥é½ agent å†…éƒ¨æµç¨‹è¯„ä¼°èƒ½åŠ›

## ğŸ“š å‚è€ƒèµ„æ–™

- [Anthropic: Demystifying evals for AI agents](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents)
- [ZenFlux Agent æ¶æ„æ–‡æ¡£](../docs/architecture/00-ARCHITECTURE-OVERVIEW.md)
- [è¯„ä¼°æ–¹æ³•è®ºè¯¦ç»†è¯´æ˜](./docs/METHODOLOGY.md)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®æ–°çš„è¯„æµ‹å¥—ä»¶ã€grader æˆ–æ”¹è¿›å»ºè®®ï¼

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/new-grader`)
3. æäº¤æ›´æ”¹ (`git commit -am 'Add new grader'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/new-grader`)
5. åˆ›å»º Pull Request

## ğŸ“„ è®¸å¯è¯

MIT License
