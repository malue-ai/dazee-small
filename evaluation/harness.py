"""
Evaluation Harnessï¼ˆè¯„ä¼°å·¥å…·ï¼‰

è¯„ä¼°ç³»ç»Ÿçš„æ ¸å¿ƒæ‰§è¡Œå¼•æ“ï¼Œè´Ÿè´£ï¼š
1. åŠ è½½è¯„ä¼°å¥—ä»¶ï¼ˆYAMLé…ç½®ï¼‰
2. æ‰§è¡Œè¯„ä¼°ä»»åŠ¡ï¼ˆæ”¯æŒå¤šæ¬¡Trialï¼‰
3. è°ƒç”¨è¯„åˆ†å™¨ï¼ˆCode/Model/Humanï¼‰
4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

è®¾è®¡å‚è€ƒï¼šAnthropic è¯„ä¼°æ–¹æ³•è®º + Promptfoo
"""

import asyncio
import uuid
import yaml
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

from evaluation.models import (
    Checkpoint,
    EvaluationReport,
    EvaluationSuite,
    GradeResult,
    GraderConfig,
    GraderType,
    Outcome,
    Task,
    TaskInput,
    TaskResult,
    TokenUsage,
    Transcript,
    Trial,
    TrialStatus,
)
from evaluation.graders.code_based import CodeBasedGraders
from evaluation.graders.model_based import ModelBasedGraders


class EvaluationHarness:
    """
    è¯„ä¼°å·¥å…·ï¼ˆEvaluation Harnessï¼‰
    
    ä½¿ç”¨æ–¹å¼ï¼š
        # åˆå§‹åŒ–
        harness = EvaluationHarness(
            agent_factory=my_agent_factory,
            llm_service=claude_service
        )
        
        # åŠ è½½è¯„ä¼°å¥—ä»¶
        suite = harness.load_suite("evaluation/suites/conversation/intent_understanding.yaml")
        
        # è¿è¡Œè¯„ä¼°
        report = await harness.run_suite(suite)
        
        # è¾“å‡ºæŠ¥å‘Š
        print(report.to_summary())
    """
    
    def __init__(
        self,
        agent_factory: Optional[Callable] = None,
        llm_service: Optional[Any] = None,
        suites_dir: str = "evaluation/suites",
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å·¥å…·
        
        Args:
            agent_factory: Agentå·¥å‚å‡½æ•°ï¼ˆç”¨äºåˆ›å»ºå¾…æµ‹è¯•çš„Agentï¼‰
            llm_service: LLMæœåŠ¡ï¼ˆç”¨äºModel-based Gradersï¼‰
            suites_dir: è¯„ä¼°å¥—ä»¶ç›®å½•
        """
        self.agent_factory = agent_factory
        self._llm_service = llm_service
        self.suites_dir = Path(suites_dir)
        
        # åˆå§‹åŒ–è¯„åˆ†å™¨
        self.code_graders = CodeBasedGraders()
        self.model_graders = ModelBasedGraders(llm_service=llm_service)

    @property
    def llm_service(self):
        return self._llm_service

    @llm_service.setter
    def llm_service(self, value):
        """Sync LLM service to model_graders when updated (e.g. defer-grading)."""
        self._llm_service = value
        self.model_graders.llm = value
        
    # ===================
    # å¥—ä»¶åŠ è½½
    # ===================
    
    def load_suite(self, path: str) -> EvaluationSuite:
        """
        ä»YAMLæ–‡ä»¶åŠ è½½è¯„ä¼°å¥—ä»¶
        
        Args:
            path: YAMLæ–‡ä»¶è·¯å¾„
            
        Returns:
            EvaluationSuite: è¯„ä¼°å¥—ä»¶
        """
        file_path = Path(path)
        
        if not file_path.is_absolute():
            file_path = self.suites_dir / path
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        
        # è§£æä»»åŠ¡
        tasks = []
        for task_data in data.get("tasks", []):
            task = self._parse_task(task_data)
            tasks.append(task)
        
        return EvaluationSuite(
            id=data.get("id", file_path.stem),
            name=data.get("name", file_path.stem),
            description=data.get("description", ""),
            category=data.get("category", "general"),
            tasks=tasks,
            default_trials=data.get("default_trials", 3),
            metadata=data.get("metadata", {}),
        )
    
    def _parse_task(self, data: Dict[str, Any]) -> Task:
        """
        è§£æä»»åŠ¡é…ç½®
        
        Args:
            data: ä»»åŠ¡é…ç½®å­—å…¸
            
        Returns:
            Task: ä»»åŠ¡å¯¹è±¡
        """
        # è§£æè¾“å…¥
        input_data = data.get("input", {})
        task_input = TaskInput(
            user_query=input_data.get("user_query", ""),
            conversation_history=input_data.get("conversation_history", []),
            context=input_data.get("context", {}),
            files=input_data.get("files", []),
        )
        
        # è§£æè¯„åˆ†å™¨é…ç½®
        graders = []
        for grader_data in data.get("graders", []):
            grader = GraderConfig(
                type=GraderType(grader_data.get("type", "code")),
                name=grader_data.get("name", grader_data.get("check", "unknown")),
                check=grader_data.get("check"),
                rubric=grader_data.get("rubric"),
                min_score=grader_data.get("min_score"),
                weight=grader_data.get("weight", 1.0),
            )
            graders.append(grader)
        
        # è§£æä¸­é—´æ£€æŸ¥ç‚¹
        checkpoints = []
        for cp_data in data.get("checkpoints", []):
            checkpoint = Checkpoint(
                name=cp_data.get("name", ""),
                check=cp_data.get("check", ""),
                description=cp_data.get("description"),
            )
            checkpoints.append(checkpoint)
        
        # è§£ææ¨èç­”æ¡ˆ
        reference_answer = data.get("reference_answer")
        
        return Task(
            id=data.get("id", str(uuid.uuid4())[:8]),
            description=data.get("description", ""),
            category=data.get("category", "general"),
            input=task_input,
            graders=graders,
            trials=data.get("trials", 3),
            timeout_seconds=data.get("timeout_seconds", 60),
            tags=data.get("tags", []),
            metadata=data.get("metadata", {}),
            checkpoints=checkpoints,
            reference_answer=reference_answer,
        )
    
    def load_all_suites(self) -> List[EvaluationSuite]:
        """
        åŠ è½½æ‰€æœ‰è¯„ä¼°å¥—ä»¶
        
        Returns:
            List[EvaluationSuite]: è¯„ä¼°å¥—ä»¶åˆ—è¡¨
        """
        suites = []
        
        for yaml_file in self.suites_dir.rglob("*.yaml"):
            try:
                suite = self.load_suite(str(yaml_file))
                suites.append(suite)
            except Exception as e:
                print(f"Warning: Failed to load suite {yaml_file}: {e}")
        
        return suites
    
    # ===================
    # è¯„ä¼°æ‰§è¡Œ
    # ===================
    
    async def run_suite(
        self,
        suite: EvaluationSuite,
        concurrency: int = 5,
        verbose: bool = True
    ) -> EvaluationReport:
        """
        è¿è¡Œè¯„ä¼°å¥—ä»¶
        
        Args:
            suite: è¯„ä¼°å¥—ä»¶
            concurrency: å¹¶å‘æ•°
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            
        Returns:
            EvaluationReport: è¯„ä¼°æŠ¥å‘Š
        """
        if verbose:
            print(f"ğŸ“Š å¼€å§‹è¯„ä¼°å¥—ä»¶: {suite.name}")
            print(f"   ä»»åŠ¡æ•°: {len(suite.tasks)}")
            print(f"   é»˜è®¤è¯•éªŒæ¬¡æ•°: {suite.default_trials}")
        
        start_time = datetime.now()
        task_results = []
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(concurrency)
        
        async def run_task_with_semaphore(task: Task) -> TaskResult:
            async with semaphore:
                return await self.run_task(task, verbose=verbose)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        tasks_coroutines = [
            run_task_with_semaphore(task) 
            for task in suite.tasks
        ]
        task_results = await asyncio.gather(*tasks_coroutines)
        
        end_time = datetime.now()
        
        # ç»Ÿè®¡ç»“æœ
        passed_tasks = sum(1 for tr in task_results if tr.pass_rate >= 0.5)
        failed_tasks = len(task_results) - passed_tasks
        unstable_tasks = sum(1 for tr in task_results if not tr.is_stable)
        
        # æ±‡æ€»Tokenä½¿ç”¨
        total_token_usage = TokenUsage()
        for tr in task_results:
            for trial in tr.trials:
                if trial.transcript:
                    usage = trial.transcript.token_usage
                    total_token_usage.input_tokens += usage.input_tokens
                    total_token_usage.output_tokens += usage.output_tokens
                    total_token_usage.thinking_tokens += usage.thinking_tokens
        
        report = EvaluationReport(
            report_id=f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            suite_id=suite.id,
            suite_name=suite.name,
            task_results=task_results,
            total_tasks=len(suite.tasks),
            passed_tasks=passed_tasks,
            failed_tasks=failed_tasks,
            unstable_tasks=unstable_tasks,
            total_token_usage=total_token_usage,
            total_duration_seconds=(end_time - start_time).total_seconds(),
        )
        
        if verbose:
            print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
            print(f"   é€šè¿‡: {passed_tasks}/{len(suite.tasks)}")
            print(f"   é€šè¿‡ç‡: {report.pass_rate:.1%}")
            print(f"   ä¸ç¨³å®šä»»åŠ¡: {unstable_tasks}")
            print(f"   æ€»Token: {total_token_usage.total_tokens}")
            print(f"   è€—æ—¶: {report.total_duration_seconds:.1f}s")
        
        return report
    
    async def run_task(
        self,
        task: Task,
        verbose: bool = False
    ) -> TaskResult:
        """
        è¿è¡Œå•ä¸ªè¯„ä¼°ä»»åŠ¡ï¼ˆå¤šæ¬¡Trialï¼‰
        
        Args:
            task: è¯„ä¼°ä»»åŠ¡
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            
        Returns:
            TaskResult: ä»»åŠ¡ç»“æœ
        """
        if verbose:
            print(f"  ğŸ“ ä»»åŠ¡: {task.id} - {task.description[:50]}...")
        
        trials = []
        
        for i in range(task.trials):
            trial = await self.run_trial(task, trial_number=i + 1)
            trials.append(trial)
            
            if verbose:
                status = "âœ“" if trial.passed else "âœ—"
                score_str = f"{trial.average_score:.2f}" if trial.average_score else "N/A"
                print(f"     Trial {i+1}: {status} (score: {score_str})")
        
        return TaskResult(
            task_id=task.id,
            task_description=task.description,
            trials=trials,
        )
    
    async def run_trial(
        self,
        task: Task,
        trial_number: int
    ) -> Trial:
        """
        è¿è¡Œå•æ¬¡è¯•éªŒ
        
        Args:
            task: è¯„ä¼°ä»»åŠ¡
            trial_number: è¯•éªŒåºå·
            
        Returns:
            Trial: è¯•éªŒç»“æœ
        """
        trial = Trial(
            trial_id=f"{task.id}_trial_{trial_number}",
            task_id=task.id,
            trial_number=trial_number,
            status=TrialStatus.RUNNING,
            started_at=datetime.now(),
        )
        
        try:
            # 1. æ‰§è¡ŒAgentï¼ˆè·å–Transcriptå’ŒOutcomeï¼‰
            transcript, outcome = await self._execute_agent(task)
            trial.transcript = transcript
            trial.outcome = outcome
            
            # 2. æ£€æŸ¥ä¸­é—´æ£€æŸ¥ç‚¹ï¼ˆæ–°å¢ï¼‰
            if task.checkpoints:
                checkpoint_results = await self._check_checkpoints(task, transcript)
                # å°†æ£€æŸ¥ç‚¹ç»“æœæ·»åŠ åˆ° grade_results
                trial.grade_results.extend(checkpoint_results)
            
            # 3. è¿è¡Œè¯„åˆ†å™¨
            grade_results = await self._run_graders(
                task=task,
                transcript=transcript,
                outcome=outcome,
            )
            trial.grade_results.extend(grade_results)
            
            # 4. æ›´æ–°çŠ¶æ€
            trial.status = TrialStatus.COMPLETED
            trial.completed_at = datetime.now()
            
        except asyncio.TimeoutError:
            trial.status = TrialStatus.TIMEOUT
            trial.error = f"æ‰§è¡Œè¶…æ—¶ï¼ˆ{task.timeout_seconds}sï¼‰"
            trial.completed_at = datetime.now()
            
        except Exception as e:
            trial.status = TrialStatus.FAILED
            trial.error = str(e)
            trial.completed_at = datetime.now()
        
        return trial
    
    async def _execute_agent(
        self,
        task: Task
    ) -> tuple[Transcript, Outcome]:
        """
        æ‰§è¡ŒAgentå¹¶æ”¶é›†Transcriptå’ŒOutcome
        
        Args:
            task: è¯„ä¼°ä»»åŠ¡
            
        Returns:
            Tuple[Transcript, Outcome]: è½¬å½•è®°å½•å’Œç»“æœè®°å½•
        """
        if self.agent_factory is None:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            return self._mock_execution(task)
        
        # åˆ›å»ºAgentå®ä¾‹
        agent = await self.agent_factory()
        
        # æ‰§è¡ŒAgent
        start_time = datetime.now()
        
        result = await asyncio.wait_for(
            agent.chat(
                user_query=task.input.user_query,
                conversation_history=task.input.conversation_history,
            ),
            timeout=task.timeout_seconds,
        )
        
        end_time = datetime.now()
        
        # æ„å»ºTranscript
        transcript = Transcript(
            messages=result.get("messages", []),
            tool_calls=result.get("tool_calls", []),
            token_usage=result.get("token_usage", TokenUsage()),
            duration_ms=int((end_time - start_time).total_seconds() * 1000),
            metadata=result.get("metadata", {}),
        )
        
        # æ„å»ºOutcome
        outcome = Outcome(
            database_changes=result.get("database_changes", []),
            file_changes=result.get("file_changes", []),
            external_api_calls=result.get("external_api_calls", []),
            custom_outcomes=result.get("custom_outcomes", {}),
        )
        
        return transcript, outcome
    
    def _mock_execution(self, task: Task) -> tuple[Transcript, Outcome]:
        """
        æ¨¡æ‹Ÿæ‰§è¡Œï¼ˆç”¨äºæµ‹è¯•è¯„ä¼°ç³»ç»Ÿæœ¬èº«ï¼‰
        
        Args:
            task: è¯„ä¼°ä»»åŠ¡
            
        Returns:
            Tuple[Transcript, Outcome]: æ¨¡æ‹Ÿçš„è½¬å½•è®°å½•å’Œç»“æœè®°å½•
        """
        from evaluation.models import Message, ToolCall
        
        # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
        mock_tool_calls = [
            ToolCall(
                name="search",
                arguments={"query": task.input.user_query},
                result="æ¨¡æ‹Ÿæœç´¢ç»“æœ",
            ),
        ]
        
        # æ¨¡æ‹Ÿæ¶ˆæ¯
        mock_messages = [
            Message(role="user", content=task.input.user_query),
            Message(
                role="assistant", 
                content=f"è¿™æ˜¯é’ˆå¯¹'{task.input.user_query}'çš„æ¨¡æ‹Ÿå›ç­”ã€‚",
                tool_calls=mock_tool_calls,
            ),
        ]
        
        transcript = Transcript(
            messages=mock_messages,
            tool_calls=mock_tool_calls,
            token_usage=TokenUsage(
                input_tokens=100,
                output_tokens=50,
                thinking_tokens=0,
            ),
            duration_ms=500,
        )
        
        outcome = Outcome()
        
        return transcript, outcome
    
    async def _check_checkpoints(
        self,
        task: Task,
        transcript: Transcript
    ) -> List[GradeResult]:
        """
        æ£€æŸ¥ä¸­é—´ç»“æœæ£€æŸ¥ç‚¹
        
        Args:
            task: è¯„ä¼°ä»»åŠ¡
            transcript: è½¬å½•è®°å½•
            
        Returns:
            List[GradeResult]: æ£€æŸ¥ç‚¹ç»“æœ
        """
        results = []
        
        for checkpoint in task.checkpoints:
            try:
                # ä½¿ç”¨ CodeBasedGraders çš„ check_checkpoint æ–¹æ³•
                result = self.code_graders.check_checkpoint(
                    transcript=transcript,
                    checkpoint_name=checkpoint.name,
                    check_expression=checkpoint.check,
                )
                results.append(result)
            except Exception as e:
                results.append(GradeResult(
                    grader_type=GraderType.CODE,
                    grader_name=f"checkpoint_{checkpoint.name}",
                    passed=False,
                    score=0.0,
                    explanation=f"æ£€æŸ¥ç‚¹éªŒè¯å¤±è´¥: {str(e)}",
                ))
        
        return results
    
    async def _run_graders(
        self,
        task: Task,
        transcript: Transcript,
        outcome: Outcome
    ) -> List[GradeResult]:
        """
        è¿è¡Œæ‰€æœ‰é…ç½®çš„è¯„åˆ†å™¨
        
        Args:
            task: è¯„ä¼°ä»»åŠ¡
            transcript: è½¬å½•è®°å½•
            outcome: ç»“æœè®°å½•
            
        Returns:
            List[GradeResult]: è¯„åˆ†ç»“æœåˆ—è¡¨
        """
        results = []
        
        for grader_config in task.graders:
            try:
                if grader_config.type == GraderType.CODE:
                    result = self._run_code_grader(grader_config, transcript, outcome)
                elif grader_config.type == GraderType.MODEL:
                    result = await self._run_model_grader(grader_config, task, transcript)
                else:
                    # Human grader ä¸åœ¨è‡ªåŠ¨è¯„ä¼°ä¸­è¿è¡Œ
                    continue
                
                results.append(result)
                
            except Exception as e:
                results.append(GradeResult(
                    grader_type=grader_config.type,
                    grader_name=grader_config.name,
                    passed=False,
                    score=0.0,
                    explanation=f"è¯„åˆ†å™¨æ‰§è¡Œå¤±è´¥: {str(e)}",
                ))
        
        return results
    
    def _run_code_grader(
        self,
        config: GraderConfig,
        transcript: Transcript,
        outcome: Outcome
    ) -> GradeResult:
        """
        è¿è¡ŒCode-basedè¯„åˆ†å™¨
        
        Args:
            config: è¯„åˆ†å™¨é…ç½®
            transcript: è½¬å½•è®°å½•
            outcome: ç»“æœè®°å½•
            
        Returns:
            GradeResult: è¯„åˆ†ç»“æœ
        """
        check = config.check or config.name
        
        # è§£ææ£€æŸ¥è¡¨è¾¾å¼
        # æ”¯æŒçš„æ ¼å¼ï¼š
        # - "tool_calls_contain('search')"
        # - "check_token_limit(50000)"
        # - "check_response_contains(['å…³é”®è¯1', 'å…³é”®è¯2'])"
        
        if check.startswith("tool_calls_contain"):
            # æå–å·¥å…·åç§°
            import re
            match = re.search(r"tool_calls_contain\(['\"](.+?)['\"]\)", check)
            if match:
                tool_name = match.group(1)
                return self.code_graders.check_tool_calls(
                    transcript, [tool_name]
                )
        
        elif check.startswith("check_token_limit"):
            import re
            match = re.search(r"check_token_limit\((\d+)\)", check)
            if match:
                max_tokens = int(match.group(1))
                return self.code_graders.check_token_limit(
                    transcript, max_tokens
                )
        
        elif check.startswith("check_response_contains"):
            import re
            import ast
            match = re.search(r"check_response_contains\((\[.+?\])\)", check)
            if match:
                keywords = ast.literal_eval(match.group(1))
                return self.code_graders.check_response_contains(
                    transcript, keywords
                )
        
        elif check.startswith("check_no_tool_errors"):
            return self.code_graders.check_no_tool_errors(transcript)
        
        elif check.startswith("check_execution_time"):
            import re
            match = re.search(r"check_execution_time\((\d+)\)", check)
            if match:
                max_duration = int(match.group(1))
                return self.code_graders.check_execution_time(
                    transcript, max_duration
                )

        elif check.startswith("check_step_count"):
            import re
            match = re.search(r"check_step_count\((\d+),\s*(\d+)\)", check)
            if match:
                optimal = int(match.group(1))
                max_accept = int(match.group(2))
                return self.code_graders.check_step_count(
                    transcript, optimal, max_accept
                )

        elif check.startswith("check_backtrack_occurred"):
            import re
            match = re.search(r"check_backtrack_occurred\((\d+)(?:,\s*(\d+))?\)", check)
            if match:
                min_count = int(match.group(1))
                max_count = int(match.group(2)) if match.group(2) else 5
                return self.code_graders.check_backtrack_occurred(
                    transcript, min_count, max_count
                )

        # é»˜è®¤ï¼šæœªçŸ¥çš„æ£€æŸ¥è¡¨è¾¾å¼
        return GradeResult(
            grader_type=GraderType.CODE,
            grader_name=check,
            passed=False,
            score=0.0,
            explanation=f"æœªçŸ¥çš„æ£€æŸ¥è¡¨è¾¾å¼: {check}",
        )
    
    async def _run_model_grader(
        self,
        config: GraderConfig,
        task: Task,
        transcript: Transcript
    ) -> GradeResult:
        """
        è¿è¡ŒModel-basedè¯„åˆ†å™¨
        
        Args:
            config: è¯„åˆ†å™¨é…ç½®
            task: è¯„ä¼°ä»»åŠ¡
            transcript: è½¬å½•è®°å½•
            
        Returns:
            GradeResult: è¯„åˆ†ç»“æœ
        """
        rubric = config.rubric or config.name

        # Multi-turn tests: extract user_query from transcript instead of empty task.input.user_query
        effective_user_query = task.input.user_query
        if not effective_user_query and task.metadata and task.metadata.get("multi_turn_sequence"):
            # Extract user queries from transcript messages
            user_messages = [
                msg.content for msg in transcript.messages
                if hasattr(msg, 'role') and msg.role == "user" and msg.content
            ]
            if user_messages:
                effective_user_query = "\n---\n".join(user_messages)
            else:
                # Fallback: extract from multi_turn_sequence definition
                sequence = task.metadata["multi_turn_sequence"]
                queries = [step.get("user_query", "") for step in sequence if step.get("user_query")]
                effective_user_query = "\n---\n".join(queries) if queries else ""

        if rubric == "grade_intent_understanding":
            result = await self.model_graders.grade_intent_understanding(
                user_query=effective_user_query,
                agent_response=transcript.get_final_response() or "",
            )
        
        elif rubric == "grade_over_engineering":
            result = await self.model_graders.grade_over_engineering(
                user_query=effective_user_query,
                transcript=transcript,
            )
        
        elif rubric == "grade_response_quality":
            # Build task context for Judge (expected behavior, verification points)
            task_context_parts = [f"ç”¨ä¾‹ ID: {task.id}", f"ç”¨ä¾‹æè¿°: {task.description}"]
            if task.metadata:
                if task.metadata.get("expected_behavior"):
                    task_context_parts.append(f"é¢„æœŸè¡Œä¸º: {task.metadata['expected_behavior']}")
                # Pass all metadata for Judge reference
                for k, v in task.metadata.items():
                    if k != "expected_behavior" and k != "multi_turn_sequence":
                        task_context_parts.append(f"{k}: {v}")
            # Include tool call summary from transcript
            tool_calls = transcript.tool_calls if hasattr(transcript, 'tool_calls') else []
            if tool_calls:
                tool_names = [tc.name for tc in tool_calls if hasattr(tc, 'name')]
                task_context_parts.append(f"å·¥å…·è°ƒç”¨é“¾: {' â†’ '.join(tool_names)} (å…± {len(tool_names)} æ¬¡)")
            # Include token usage
            if hasattr(transcript, 'token_usage') and transcript.token_usage:
                tu = transcript.token_usage
                task_context_parts.append(
                    f"Token æ¶ˆè€—: input={getattr(tu, 'input_tokens', 0):,}, "
                    f"output={getattr(tu, 'output_tokens', 0):,}, "
                    f"thinking={getattr(tu, 'thinking_tokens', 0):,}"
                )
            task_context = "\n".join(task_context_parts)

            result = await self.model_graders.grade_response_quality(
                user_query=effective_user_query,
                agent_response=transcript.get_final_response() or "",
                context=task_context,
            )
        
        elif rubric == "grade_logical_coherence":
            result = await self.model_graders.grade_logical_coherence(
                transcript=transcript,
            )
        
        elif rubric == "grade_safety_compliance":
            result = await self.model_graders.grade_safety_compliance(
                user_query=effective_user_query,
                agent_response=transcript.get_final_response() or "",
            )
        
        elif rubric == "grade_intermediate_output":
            # éœ€è¦ä» task æˆ– transcript ä¸­æå–ä¸­é—´è¾“å‡º
            intermediate_output = transcript.metadata.get("intermediate_output", "")
            step_description = task.description
            success_criteria = task.metadata.get("success_criteria", [])
            result = await self.model_graders.grade_intermediate_output(
                intermediate_output=intermediate_output,
                step_description=step_description,
                success_criteria=success_criteria,
            )
        
        # V11.0: ç§»é™¤ grade_multi_agent_coordinationï¼ˆä¸å†æ”¯æŒå¤šæ™ºèƒ½ä½“ï¼‰
        
        elif rubric == "grade_against_reference":
            reference_answer = task.reference_answer or ""
            result = await self.model_graders.grade_against_reference(
                agent_response=transcript.get_final_response() or "",
                reference_answer=reference_answer,
            )

        elif rubric == "grade_skill_selection":
            meta = task.metadata or {}
            result = await self.model_graders.grade_skill_selection(
                user_query=effective_user_query,
                transcript=transcript,
                optimal_tools=meta.get("optimal_tools"),
                suboptimal_tools=meta.get("suboptimal_tools"),
            )

        elif rubric == "grade_planning_depth":
            meta = task.metadata or {}
            result = await self.model_graders.grade_planning_depth(
                user_query=effective_user_query,
                transcript=transcript,
                expected_planning=meta.get("expected_planning"),
            )

        elif rubric == "grade_rollback_safety":
            # B9/B10 å›æ»šå®‰å…¨ä¸“é¡¹è¯„ä¼° â€” éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡æ‰èƒ½å‡†ç¡®è¯Šæ–­
            task_context_parts = [f"ç”¨ä¾‹ ID: {task.id}", f"ç”¨ä¾‹æè¿°: {task.description}"]
            if task.metadata:
                if task.metadata.get("expected_behavior"):
                    task_context_parts.append(f"é¢„æœŸè¡Œä¸º: {task.metadata['expected_behavior']}")
                for k, v in task.metadata.items():
                    if k not in ("expected_behavior", "multi_turn_sequence"):
                        task_context_parts.append(f"{k}: {v}")
            # å·¥å…·è°ƒç”¨é“¾ï¼ˆå…³é”®ï¼šå›æ»šè¯Šæ–­éœ€è¦çœ‹åˆ° snapshot/rollback äº‹ä»¶ï¼‰
            tool_calls = transcript.tool_calls if hasattr(transcript, 'tool_calls') else []
            if tool_calls:
                tool_names = [tc.name for tc in tool_calls if hasattr(tc, 'name')]
                task_context_parts.append(f"å·¥å…·è°ƒç”¨é“¾: {' â†’ '.join(tool_names)} (å…± {len(tool_names)} æ¬¡)")
            if hasattr(transcript, 'token_usage') and transcript.token_usage:
                tu = transcript.token_usage
                task_context_parts.append(
                    f"Token æ¶ˆè€—: input={getattr(tu, 'input_tokens', 0):,}, "
                    f"output={getattr(tu, 'output_tokens', 0):,}"
                )
            task_context = "\n".join(task_context_parts)

            # ä½¿ç”¨ judge_prompts.yaml ä¸­çš„ä¸“é¡¹æç¤ºè¯ + å®Œæ•´ä¸Šä¸‹æ–‡
            result = await self.model_graders.grade_response_quality(
                user_query=effective_user_query,
                agent_response=transcript.get_final_response() or "",
                context=task_context,
                rubric_override="grade_rollback_safety",
            )
            result.grader_name = "grade_rollback_safety"

        else:
            # Check if rubric exists in judge_prompts.yaml (strict evaluation prompts)
            yaml_prompt = self.model_graders._get_judge_prompt(rubric)
            if yaml_prompt:
                # Route through grade_response_quality with rubric_override
                # This loads the full prompt from judge_prompts.yaml with test_cases.md context
                task_context_parts = [
                    f"ç”¨ä¾‹ ID: {task.id}",
                    f"ç”¨ä¾‹æè¿°: {task.description}",
                ]
                if task.metadata:
                    if task.metadata.get("expected_behavior"):
                        task_context_parts.append(f"é¢„æœŸè¡Œä¸º: {task.metadata['expected_behavior']}")
                    for k, v in task.metadata.items():
                        if k not in ("expected_behavior", "multi_turn_sequence"):
                            task_context_parts.append(f"{k}: {v}")
                tool_calls = transcript.tool_calls if hasattr(transcript, 'tool_calls') else []
                if tool_calls:
                    tool_names = [tc.name for tc in tool_calls if hasattr(tc, 'name')]
                    task_context_parts.append(
                        f"å·¥å…·è°ƒç”¨é“¾: {' â†’ '.join(tool_names)} (å…± {len(tool_names)} æ¬¡)"
                    )
                if hasattr(transcript, 'token_usage') and transcript.token_usage:
                    tu = transcript.token_usage
                    task_context_parts.append(
                        f"Token æ¶ˆè€—: input={getattr(tu, 'input_tokens', 0):,}, "
                        f"output={getattr(tu, 'output_tokens', 0):,}"
                    )
                task_context = "\n".join(task_context_parts)

                result = await self.model_graders.grade_response_quality(
                    user_query=effective_user_query,
                    agent_response=transcript.get_final_response() or "",
                    context=task_context,
                    rubric_override=rubric,
                )
                result.grader_name = rubric
            else:
                # Fallback: unknown rubric â€” use generic custom rubric
                context_parts = [f"ç”¨ä¾‹ ID: {task.id}", f"ç”¨ä¾‹æè¿°: {task.description}"]
                if task.metadata and task.metadata.get("expected_behavior"):
                    context_parts.append(f"é¢„æœŸè¡Œä¸º: {task.metadata['expected_behavior']}")
                tool_calls = transcript.tool_calls if hasattr(transcript, 'tool_calls') else []
                if tool_calls:
                    tool_names = [tc.name for tc in tool_calls if hasattr(tc, 'name')]
                    context_parts.append(f"å·¥å…·è°ƒç”¨é“¾: {' â†’ '.join(tool_names)} (å…± {len(tool_names)} æ¬¡)")
                custom_context = "\n".join(context_parts) if context_parts else None

                result = await self.model_graders.grade_with_custom_rubric(
                    content=transcript.get_final_response() or "",
                    rubric=rubric,
                    context={"task_context": custom_context} if custom_context else None,
                )
        
        # Model grader quality gate:
        # - task_completed=false AND score < 0.4 (2.0/5.0) â†’ passed=False
        # - Otherwise â†’ passed=True (advisory, scores for human review)
        task_completed = (result.details or {}).get("task_completed", True)
        if task_completed is False and result.score is not None and result.score < 0.4:
            result.passed = False
        else:
            result.passed = True
        result.needs_human_review = True
        
        return result
    
    # ===================
    # æŠ¥å‘Šç”Ÿæˆ
    # ===================
    
    def generate_markdown_report(
        self,
        report: EvaluationReport
    ) -> str:
        """
        ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š
        
        Args:
            report: è¯„ä¼°æŠ¥å‘Š
            
        Returns:
            str: MarkdownæŠ¥å‘Šå†…å®¹
        """
        lines = [
            f"# è¯„ä¼°æŠ¥å‘Š: {report.suite_name}",
            f"",
            f"**æŠ¥å‘ŠID**: {report.report_id}",
            f"**ç”Ÿæˆæ—¶é—´**: {report.created_at.strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
            f"## ğŸ“Š æ€»ä½“ç»Ÿè®¡",
            f"",
            f"| æŒ‡æ ‡ | å€¼ |",
            f"|------|-----|",
            f"| æ€»ä»»åŠ¡æ•° | {report.total_tasks} |",
            f"| é€šè¿‡ä»»åŠ¡ | {report.passed_tasks} |",
            f"| å¤±è´¥ä»»åŠ¡ | {report.failed_tasks} |",
            f"| é€šè¿‡ç‡ | {report.pass_rate:.1%} |",
            f"| ä¸ç¨³å®šä»»åŠ¡ | {report.unstable_tasks} |",
            f"| å¹³å‡åˆ† | {f'{report.average_score:.2f}' if report.average_score else 'N/A'} |",
            f"| æ€»Token | {report.total_token_usage.total_tokens} |",
            f"| æ€»è€—æ—¶ | {report.total_duration_seconds:.1f}s |",
            f"",
            f"## ğŸ“ ä»»åŠ¡è¯¦æƒ…",
            f"",
        ]
        
        for tr in report.task_results:
            status = "âœ…" if tr.pass_rate >= 0.5 else "âŒ"
            stability = "âš ï¸" if not tr.is_stable else ""
            
            lines.extend([
                f"### {status} {tr.task_id} {stability}",
                f"",
                f"**æè¿°**: {tr.task_description}",
                f"",
                f"| Trial | çŠ¶æ€ | åˆ†æ•° | è¯¦æƒ… |",
                f"|-------|------|------|------|",
            ])
            
            for trial in tr.trials:
                trial_status = "âœ“" if trial.passed else "âœ—"
                score = f"{trial.average_score:.2f}" if trial.average_score else "N/A"
                duration = f"{trial.duration_seconds:.1f}s" if trial.duration_seconds else "N/A"
                
                lines.append(
                    f"| Trial {trial.trial_number} | {trial_status} | {score} | {duration} |"
                )
            
            lines.extend([
                f"",
                f"**é€šè¿‡ç‡**: {tr.pass_rate:.1%}",
                f"**å¹³å‡åˆ†**: {f'{tr.average_score:.2f}' if tr.average_score else 'N/A'}",
                f"**ç¨³å®šæ€§**: {'ç¨³å®š' if tr.is_stable else 'ä¸ç¨³å®šï¼ˆæ ‡å‡†å·®: ' + (f'{tr.score_std:.2f}' if tr.score_std is not None else 'N/A') + 'ï¼‰'}",
                f"",
                f"---",
                f"",
            ])
        
        return "\n".join(lines)
    
    def save_report(
        self,
        report: EvaluationReport,
        output_dir: str = "evaluation/reports"
    ) -> str:
        """
        ä¿å­˜è¯„ä¼°æŠ¥å‘Š
        
        Args:
            report: è¯„ä¼°æŠ¥å‘Š
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_content = self.generate_markdown_report(report)
        md_file = output_path / f"{report.report_id}.md"
        
        with open(md_file, "w", encoding="utf-8") as f:
            f.write(md_content)
        
        # ä¿å­˜JSONæ•°æ®
        import json
        json_file = output_path / f"{report.report_id}.json"
        
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(report.model_dump(), f, ensure_ascii=False, indent=2, default=str)
        
        return str(md_file)
    
    @staticmethod
    def load_report(report_path: Path) -> EvaluationReport:
        """
        åŠ è½½è¯„ä¼°æŠ¥å‘Š
        
        Args:
            report_path: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰
            
        Returns:
            EvaluationReport: è¯„ä¼°æŠ¥å‘Šå¯¹è±¡
        """
        import json
        
        with open(report_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # å¤„ç† datetime å­—ç¬¦ä¸²
        def parse_datetime(obj):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key in ["created_at", "timestamp"] and isinstance(value, str):
                        try:
                            obj[key] = datetime.fromisoformat(value.replace("Z", "+00:00"))
                        except (ValueError, TypeError):
                            pass
                    elif isinstance(value, (dict, list)):
                        parse_datetime(value)
            elif isinstance(obj, list):
                for item in obj:
                    parse_datetime(item)
            return obj
        
        data = parse_datetime(data)
        
        return EvaluationReport(**data)
