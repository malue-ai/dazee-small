"""
è¯„ä¼°æŒ‡æ ‡çœ‹æ¿
æä¾›å®æ—¶è¯„ä¼°æŒ‡æ ‡ç›‘æ§å’Œå¯è§†åŒ–ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
"""
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from pathlib import Path
import json
from .models import EvaluationReport
from .metrics import MetricsCalculator, MetricSummary, format_metric_summary


class EvaluationDashboard:
    """è¯„ä¼°æŒ‡æ ‡çœ‹æ¿"""
    
    def __init__(self, reports_dir: Path = Path("evaluation/reports")):
        """
        åˆå§‹åŒ–çœ‹æ¿
        
        Args:
            reports_dir: è¯„ä¼°æŠ¥å‘Šå­˜å‚¨ç›®å½•
        """
        self.reports_dir = Path(reports_dir)
        self.reports_dir.mkdir(parents=True, exist_ok=True)
    
    def load_recent_reports(self, days: int = 7) -> List[EvaluationReport]:
        """
        åŠ è½½æœ€è¿‘çš„è¯„ä¼°æŠ¥å‘Š
        
        Args:
            days: åŠ è½½æœ€è¿‘ N å¤©çš„æŠ¥å‘Š
            
        Returns:
            è¯„ä¼°æŠ¥å‘Šåˆ—è¡¨
        """
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        reports = []
        
        for report_file in self.reports_dir.glob("*.json"):
            try:
                with open(report_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    report = EvaluationReport(**data)
                    
                    # æ£€æŸ¥æ—¶é—´æˆ³
                    report_date = datetime.fromisoformat(report.timestamp.replace("Z", "+00:00"))
                    if report_date >= cutoff_date:
                        reports.append(report)
            except Exception as e:
                print(f"âš ï¸  åŠ è½½æŠ¥å‘Šå¤±è´¥ {report_file}: {e}")
        
        # æŒ‰æ—¶é—´æ’åº
        reports.sort(key=lambda r: r.timestamp, reverse=True)
        
        return reports
    
    def generate_dashboard(self, days: int = 7) -> str:
        """
        ç”Ÿæˆçœ‹æ¿ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
        
        Args:
            days: æ˜¾ç¤ºæœ€è¿‘ N å¤©çš„æ•°æ®
            
        Returns:
            çœ‹æ¿æ–‡æœ¬
        """
        reports = self.load_recent_reports(days)
        
        if not reports:
            return "æš‚æ— è¯„ä¼°æŠ¥å‘Š"
        
        lines = []
        lines.append("=" * 100)
        lines.append("ZenFlux Agent è¯„ä¼°æŒ‡æ ‡çœ‹æ¿")
        lines.append("=" * 100)
        lines.append(f"æ—¶é—´èŒƒå›´: æœ€è¿‘ {days} å¤©")
        lines.append(f"æŠ¥å‘Šæ•°é‡: {len(reports)}")
        lines.append(f"æœ€åæ›´æ–°: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}")
        lines.append("")
        
        # 1. è¶‹åŠ¿åˆ†æ
        lines.append("ğŸ“ˆ è¶‹åŠ¿åˆ†æ")
        lines.append("-" * 100)
        trend = self._analyze_trend(reports)
        lines.append(f"  æ€»ä½“å¾—åˆ†è¶‹åŠ¿: {trend['overall_score_trend']}")
        lines.append(f"  ä»»åŠ¡æˆåŠŸç‡è¶‹åŠ¿: {trend['task_success_rate_trend']}")
        lines.append(f"  è´¨é‡å¾—åˆ†è¶‹åŠ¿: {trend['quality_score_trend']}")
        lines.append("")
        
        # 2. æœ€æ–°è¯„ä¼°æ‘˜è¦
        lines.append("ğŸ“Š æœ€æ–°è¯„ä¼°æ‘˜è¦")
        lines.append("-" * 100)
        latest_report = reports[0]
        calculator = MetricsCalculator()
        latest_summary = calculator.calculate(latest_report)
        
        lines.append(f"  è¯„ä¼°å¥—ä»¶: {latest_report.suite_name}")
        lines.append(f"  è¯„ä¼°æ—¶é—´: {latest_report.timestamp}")
        lines.append(f"  æ€»ä½“å¾—åˆ†: {latest_summary.overall_score:.1%}")
        lines.append(f"  è´¨é‡åˆ†å±‚: {latest_summary.quality_tier}")
        lines.append(f"  ä»»åŠ¡æˆåŠŸç‡: {self._get_metric_value(latest_summary, 'task_success_rate'):.1%}")
        lines.append(f"  å¹³å‡è´¨é‡å¾—åˆ†: {self._get_metric_value(latest_summary, 'avg_quality_score'):.1f}/10")
        lines.append("")
        
        # 3. å…³é”®æŒ‡æ ‡å†å²
        lines.append("ğŸ“‰ å…³é”®æŒ‡æ ‡å†å²ï¼ˆæœ€è¿‘ 5 æ¬¡è¯„ä¼°ï¼‰")
        lines.append("-" * 100)
        history_table = self._generate_history_table(reports[:5])
        lines.append(history_table)
        lines.append("")
        
        # 4. å¤±è´¥æ¡ˆä¾‹Top 5
        lines.append("âŒ å¤±è´¥æ¡ˆä¾‹ Top 5")
        lines.append("-" * 100)
        failed_tasks = self._get_failed_tasks(latest_report, limit=5)
        for i, task in enumerate(failed_tasks, 1):
            lines.append(f"  {i}. {task['task_id']}")
            lines.append(f"     å¤±è´¥åŸå› : {task['failure_reason']}")
        lines.append("")
        
        # 5. å‘Šè­¦ä¿¡æ¯
        lines.append("âš ï¸  å‘Šè­¦ä¿¡æ¯")
        lines.append("-" * 100)
        alerts = self._check_alerts(latest_summary, reports)
        if alerts:
            for alert in alerts:
                lines.append(f"  {alert['severity']} {alert['message']}")
        else:
            lines.append("  âœ… æ— å‘Šè­¦")
        lines.append("")
        
        lines.append("=" * 100)
        
        return "\n".join(lines)
    
    def _analyze_trend(self, reports: List[EvaluationReport]) -> Dict[str, str]:
        """åˆ†æè¶‹åŠ¿"""
        if len(reports) < 2:
            return {
                "overall_score_trend": "æ•°æ®ä¸è¶³",
                "task_success_rate_trend": "æ•°æ®ä¸è¶³",
                "quality_score_trend": "æ•°æ®ä¸è¶³"
            }
        
        calculator = MetricsCalculator()
        
        # è®¡ç®—æœ€è¿‘ä¸¤æ¬¡çš„æŒ‡æ ‡
        latest_summary = calculator.calculate(reports[0])
        previous_summary = calculator.calculate(reports[1])
        
        def trend_indicator(current: float, previous: float) -> str:
            diff = current - previous
            if abs(diff) < 0.01:
                return f"æŒå¹³ ({current:.1%})"
            elif diff > 0:
                return f"ä¸Šå‡ â†—ï¸ ({previous:.1%} â†’ {current:.1%}, +{diff:.1%})"
            else:
                return f"ä¸‹é™ â†˜ï¸ ({previous:.1%} â†’ {current:.1%}, {diff:.1%})"
        
        return {
            "overall_score_trend": trend_indicator(
                latest_summary.overall_score,
                previous_summary.overall_score
            ),
            "task_success_rate_trend": trend_indicator(
                self._get_metric_value(latest_summary, "task_success_rate"),
                self._get_metric_value(previous_summary, "task_success_rate")
            ),
            "quality_score_trend": trend_indicator(
                self._get_metric_value(latest_summary, "avg_quality_score") / 10.0,
                self._get_metric_value(previous_summary, "avg_quality_score") / 10.0
            )
        }
    
    def _generate_history_table(self, reports: List[EvaluationReport]) -> str:
        """ç”Ÿæˆå†å²è®°å½•è¡¨æ ¼"""
        calculator = MetricsCalculator()
        
        lines = []
        header = f"  {'æ—¶é—´':<20} | {'æ€»åˆ†':<8} | {'æˆåŠŸç‡':<8} | {'è´¨é‡':<8} | {'åˆ†å±‚':<12}"
        lines.append(header)
        lines.append("  " + "-" * 95)
        
        for report in reports:
            summary = calculator.calculate(report)
            timestamp = report.timestamp[:19]  # æˆªå–åˆ°ç§’
            overall = f"{summary.overall_score:.1%}"
            success = f"{self._get_metric_value(summary, 'task_success_rate'):.1%}"
            quality = f"{self._get_metric_value(summary, 'avg_quality_score'):.1f}/10"
            tier = summary.quality_tier
            
            line = f"  {timestamp:<20} | {overall:<8} | {success:<8} | {quality:<8} | {tier:<12}"
            lines.append(line)
        
        return "\n".join(lines)
    
    def _get_failed_tasks(
        self,
        report: EvaluationReport,
        limit: int = 5
    ) -> List[Dict[str, str]]:
        """è·å–å¤±è´¥çš„ä»»åŠ¡"""
        failed_tasks = []
        
        for result in report.results:
            if not result.passed:
                # æ‰¾å‡ºå¤±è´¥çš„ grader
                failed_graders = []
                for trial in result.trials:
                    for grade in trial.grades:
                        if not grade.passed:
                            failed_graders.append(f"{grade.grader_name} ({grade.reasoning})")
                
                failed_tasks.append({
                    "task_id": result.task_id,
                    "failure_reason": "; ".join(failed_graders[:2])  # åªæ˜¾ç¤ºå‰2ä¸ª
                })
        
        return failed_tasks[:limit]
    
    def _check_alerts(
        self,
        latest_summary: MetricSummary,
        reports: List[EvaluationReport]
    ) -> List[Dict[str, str]]:
        """æ£€æŸ¥å‘Šè­¦"""
        alerts = []
        
        # 1. å›å½’å‘Šè­¦
        if latest_summary.regression_detected:
            alerts.append({
                "severity": "ğŸ”´ ä¸¥é‡",
                "message": "æ£€æµ‹åˆ°æ€§èƒ½å›å½’ï¼Œè¯·ç«‹å³æ£€æŸ¥æœ€æ–°å˜æ›´"
            })
        
        # 2. è´¨é‡åˆ†å±‚å‘Šè­¦
        if latest_summary.quality_tier == "POOR":
            alerts.append({
                "severity": "ğŸ”´ ä¸¥é‡",
                "message": f"è´¨é‡åˆ†å±‚ä¸º POORï¼ˆæ€»åˆ† {latest_summary.overall_score:.1%}ï¼‰ï¼Œå»ºè®®æš‚åœå‘å¸ƒ"
            })
        elif latest_summary.quality_tier == "ACCEPTABLE":
            alerts.append({
                "severity": "ğŸŸ¡ è­¦å‘Š",
                "message": f"è´¨é‡åˆ†å±‚ä¸º ACCEPTABLEï¼ˆæ€»åˆ† {latest_summary.overall_score:.1%}ï¼‰ï¼Œå»ºè®®ä¼˜åŒ–åå‘å¸ƒ"
            })
        
        # 3. å…³é”®æŒ‡æ ‡å‘Šè­¦
        for metric in latest_summary.metrics:
            if not metric.passed and metric.name in ["task_success_rate", "code_pass_rate", "error_rate"]:
                alerts.append({
                    "severity": "ğŸŸ¡ è­¦å‘Š",
                    "message": f"{metric.description} æœªè¾¾æ ‡ï¼š{metric.value:.1%}ï¼ˆé˜ˆå€¼ï¼š{metric.threshold:.1%}ï¼‰"
                })
        
        # 4. è¿ç»­ä¸‹é™å‘Šè­¦
        if len(reports) >= 3:
            calculator = MetricsCalculator()
            recent_scores = [
                calculator.calculate(r).overall_score
                for r in reports[:3]
            ]
            if recent_scores[0] < recent_scores[1] < recent_scores[2]:
                alerts.append({
                    "severity": "ğŸŸ¡ è­¦å‘Š",
                    "message": "æ€»ä½“å¾—åˆ†è¿ç»­ 3 æ¬¡ä¸‹é™ï¼Œè¯·å…³æ³¨è´¨é‡è¶‹åŠ¿"
                })
        
        return alerts
    
    def _get_metric_value(self, summary: MetricSummary, metric_name: str) -> float:
        """è·å–æŒ‡æ ‡å€¼"""
        metric = next((m for m in summary.metrics if m.name == metric_name), None)
        return metric.value if metric else 0.0
    
    def export_to_json(self, output_file: Path) -> None:
        """å¯¼å‡ºçœ‹æ¿æ•°æ®ä¸º JSONï¼ˆç”¨äºå¤–éƒ¨å¯è§†åŒ–å·¥å…·ï¼‰"""
        reports = self.load_recent_reports(days=30)
        calculator = MetricsCalculator()
        
        dashboard_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "reports": []
        }
        
        for report in reports:
            summary = calculator.calculate(report)
            dashboard_data["reports"].append({
                "timestamp": report.timestamp,
                "suite_name": report.suite_name,
                "overall_score": summary.overall_score,
                "quality_tier": summary.quality_tier,
                "regression_detected": summary.regression_detected,
                "metrics": [
                    {
                        "name": m.name,
                        "value": m.value,
                        "threshold": m.threshold,
                        "passed": m.passed,
                        "description": m.description
                    }
                    for m in summary.metrics
                ]
            })
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(dashboard_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… çœ‹æ¿æ•°æ®å·²å¯¼å‡ºåˆ° {output_file}")
